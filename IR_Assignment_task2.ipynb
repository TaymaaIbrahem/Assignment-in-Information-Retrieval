{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Oron's\n",
      "[nltk_data]     computer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Oron's\n",
      "[nltk_data]     computer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression:\n",
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "\n",
      "Naive Bayes:\n",
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "\n",
      "SVM:\n",
      "Accuracy: 0.5455\n",
      "Precision: 0.7980\n",
      "Recall: 0.5455\n",
      "\n",
      "Random Forest:\n",
      "Accuracy: 0.9091\n",
      "Precision: 0.9273\n",
      "Recall: 0.9091\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load documents and labels\n",
    "def load_documents_and_labels():\n",
    "    texts, labels = [], []\n",
    "    # Load positive examples\n",
    "    for file in os.listdir('possitive'):\n",
    "        with open(f'possitive/{file}', 'r', encoding='utf-8') as f:\n",
    "            texts.append(f.read())\n",
    "            labels.append(1)  # 1 for positive class\n",
    "    # Load negative examples\n",
    "    for file in os.listdir('negative'):\n",
    "        with open(f'negative/{file}', 'r', encoding='utf-8') as f:\n",
    "            texts.append(f.read())\n",
    "            labels.append(0)  # 0 for negative class\n",
    "    return texts, labels\n",
    "\n",
    "# Tokenization and removing stop words\n",
    "def preprocess_texts(texts):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    processed_texts = []\n",
    "    for text in texts:\n",
    "        tokens = word_tokenize(text)\n",
    "        filtered_tokens = [w for w in tokens if not w.lower() in stop_words]\n",
    "        processed_texts.append(' '.join(filtered_tokens))\n",
    "    return processed_texts\n",
    "\n",
    "texts, labels = load_documents_and_labels()\n",
    "processed_texts = preprocess_texts(texts)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(processed_texts, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Initialize the vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Classifiers to evaluate\n",
    "classifiers = [\n",
    "    LogisticRegression(max_iter=1000),\n",
    "    MultinomialNB(),\n",
    "    SVC(),\n",
    "    RandomForestClassifier()\n",
    "]\n",
    "\n",
    "classifier_names = ['Logistic Regression', 'Naive Bayes', 'SVM', 'Random Forest']\n",
    "\n",
    "# Train and evaluate classifiers\n",
    "for classifier, name in zip(classifiers, classifier_names):\n",
    "    pipeline = make_pipeline(vectorizer, StandardScaler(with_mean=False), classifier)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression:\n",
    "\n",
    "# Despite achieving perfect accuracy, precision, and recall of 1.0000, Logistic Regression may be considered cautiously due to its performance being assessed solely on these metrics. While its simplicity and high performance are apparent, it's important to scrutinize its generalization to unseen data and its capability in handling more complex patterns.\n",
    "# Naive Bayes:\n",
    "\n",
    "# Similar to Logistic Regression, Naive Bayes also achieved perfect accuracy, precision, and recall of 1.0000, indicating consistent and reliable performance across all metrics. This makes Naive Bayes an attractive choice for tasks where robustness and simplicity are valued, and where the assumptions of independence among features hold reasonably well.\n",
    "# SVM:\n",
    "\n",
    "# While SVM falls behind in terms of accuracy and recall compared to Logistic Regression and Naive Bayes, it still demonstrates a respectable performance with accuracy and recall values of 0.5455. However, its precision of 0.7980 suggests that it's relatively better at minimizing false positives at the expense of some false negatives. Therefore, SVM might be a suitable option when false positives need to be minimized while maintaining a reasonable level of recall.\n",
    "# Random Forest:\n",
    "\n",
    "# Random Forest achieved an accuracy of 0.9091 and precision of 0.9273, with recall matching its accuracy. Although not perfect, its performance is commendable, especially considering its robustness and ability to handle complex relationships in the data. Random Forest could be a preferable choice when high accuracy and reliability in classifying positive cases are prioritized without the strict requirement of perfect performance on all metrics.\n",
    "# In summary, the choice of the best classifier depends on the specific needs of the task. Logistic Regression and Naive Bayes offer simplicity and perfect performance but may require further evaluation for generalization. SVM, with its focus on minimizing false positives, could be valuable in scenarios where this is crucial. Random Forest, with its robustness and strong performance, may be a suitable option for tasks requiring high accuracy and reliability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
