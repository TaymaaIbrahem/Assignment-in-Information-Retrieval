International Journal of Information Management 52 (2020) 102061

Contents lists available at ScienceDirect

International Journal of Information Management
journal homepage: www.elsevier.com/locate/ijinfomgt

Beyond user experience: What constitutes algorithmic experiences?
a,

b

Donghee Shin *, Bu Zhong , Frank A. Biocca
a
b
c

T

c

Zayed University, P.O. Box 144534, Abu Dhabi, United Arab Emirates
College of Communication, Penn State University, United States
Department of Informatics, Ying Wu College of Computing, New Jersey Institute of Technology, University Heights Newark, NJ 07102, United States

A R T I C LE I N FO

A B S T R A C T

Keywords:
Algorithm
Algorithmic experience
Algorithmic trust
Transparency
Fairness
User experience
Aﬀordance

Algorithms are progressively transforming human experience, especially, the interaction with businesses, governments, education, and entertainment. As a result, people are growingly seeing the outside world, in a sense,
through the lens of algorithms. Despite the importance of algorithmic experience (AX), few studies had been
devoted to investigating the nature and processes through which users perceive and actualize the potential for
algorithm aﬀordance. This study proposes the Algorithm Acceptance Model to conceptualize the notion of AX as
part of the analytic framework for human-algorithm interaction. It then tests how AX shapes the satisfaction with
and acceptance of algorithm services. The results show that AX is inherently related to human understanding of
fairness, transparency, and other conventional components of user-experience, indicating the heuristic roles of
transparency and fairness regarding their underlying relations of user experience and trust. AX can inﬂuence the
user perception of algorithmic systems in the context of algorithm ecology, oﬀering useful insights into the
design of human-centered algorithm systems. The ﬁndings provide initial and robust support for the proposed
Algorithm Acceptance Model.

1. Introduction
Algorithms are growingly integrated into people’s everyday lives as
part of their media routine on the internet (Dwivedi et al., in-press).
Algorithms provide opportunities to utilize user data to predict user
behaviors and react to how people behave in cyberspace. Most industries are taking advantage of data insights via algorithms in order to
provide a more engaging, tailored and accurate experience for individual users. Algorithms can be used to mediate and assist in human
decision-making through a wide variety of venues, such as recruiting
algorithms, tailored news aggregation services, online machine learning
recommender systems, and credit scoring methods (Wilson, 2017).
Despite their rising popularity and adoption, algorithms remain to
be seen whether the users enjoy or accept them, and in what way the
algorithm experience (AX) may be improved by automated processes
(Beer, 2017; Helberger, Karppinen, & D’Acunto, 2018). The algorithm
services are believed to improve user experience (UX), but how to improve AX remains an open question. So far, the understanding about
how people experience and perceive algorithms is limited, though it
attracts some initial eﬀort, for instance, how users perceive automated
news that is selected or produced by algorithms (Zheng, Zhong, & Yang,
2018) and how users evaluate algorithm-powered recommender systems (Knijnenburg, Willemsen, Gantner, Soncu, & Newell, 2012). Few
⁎

studies were devoted to investigating the nature of AX.
This study thus seeks to model AX in the context of algorithm systems by placing emphases on the issues surrounding the use of algorithms. Algorithms have demonstrated great potentials in delivering
signiﬁcantly improved services to users, but complex issues such as
fairness, transparency, and accuracy (FAT) are inextricably interwoven
into the operation of algorithm services (Shin & Park, 2019). Many issues in this stream remained unsolved and troublesome, such as whether algorithms are impartial or unbiased, who should be held accountable for possible harmful consequences of algorithms, and how to
warrant the goals, operations, and actions of algorithms (Crain, 2018;
Shin, 2019). These issues, along with data policy, privacy concerns, and
ethical considerations as to how we conﬁgure and implement algorithms, are important to their success and long-term sustainability
(Beer, 2017; Sloan & Warner, 2017). More scholarly attention is urgently needed to work out a clear and measurable deﬁnition of the
matters (Lee, 2018), which often cause confusion regarding how to
operationalize such issues for academia and industry. This study thus
examines FAT in the user adoption process of algorithms. It aims to
conceptualize AX by proposing the Algorithm Acceptance Model
(AAM), centering on what users do with algorithms rather than what
algorithms do to users. This study will also examine the ways to enhance AX and promote a positive UX of algorithm artifacts.

Corresponding author.
E-mail address: donghee.shin@zu.ac.ae (D. Shin).

https://doi.org/10.1016/j.ijinfomgt.2019.102061
Received 20 October 2019; Received in revised form 26 December 2019; Accepted 26 December 2019
Available online 18 January 2020
0268-4012/ © 2019 Elsevier Ltd. All rights reserved.

International Journal of Information Management 52 (2020) 102061

D. Shin, et al.

2. Literature review

algorithms in recent years. Indeed, some users realize the exposure, but
AX is not always accurate or pleasant to them. The technical problem
may be partially due to the ﬂawed design of algorithms, which makes
AX not desirable. A user-centered approach towards eliciting desirable
AX qualities is not straightforward, since the general awareness of algorithmic inﬂuence is low among users. Serious challenges exist concerning how to design and develop algorithmic interfaces (Shin,
Fotiadis, & Yu, 2019). AX can be used as an analytical framework for
making the interaction with and experience of algorithms explicit (Shin
& Park, 2019). Yet, algorithms can exhibit unexpected, negative behavior, such as biased results, unfair processes, and inaccurate recommendation (Rossiter & Zehile, 2015). Another challenge to designing algorithms is that humans, unlike robots, have emotions who
are not just the sum of their behaviors. Humans do not always behave
rationally, or even predictably irrationally. Thus, it is important to
consider algorithms not only as working tools inside a system, but also
as technical features deserving design attention from a user-based point
of view. From a system perspective, the term AX includes any possible
codes or eﬀects that could be related to the algorithmic experience or
service. This study proposes and analyzes the concept of AX, in the hope
of implementing it for algorithm services. Just like the UX’s main
principle, focusing on AX demonstrates the point that users can and do
inﬂuence algorithms in their inaction with them –even when they do
not fully understand the technicalities. This argument sheds light on the
active role of users against algorithmic power from a network-downtoward-user perspective (Wilson, 2017). Focusing on outcomes and the
methodological shift towards a user-based approach highlights the
importance of user perspective in algorithm research.

Algorithms, in many cases, have shown a stronger inﬂuence on
people’s choices than advice from other people (Montal & Reich, 2017).
As we are entering an algorithmic era, algorithms begin to shape numerous human experiences (Shin, 2019). This endorses the importance
of examining the concept of algorithmic experience as it contributes to
the understanding of user heuristics – how users perceive, experience
and feel in the process of using algorithms. In this study, the concept of
AX goes beyond usability to include cognitive, socio-cognitive, and
aﬀective dimensions of users’ experience in their interaction with algorithms, especially user trust in the services or providers, and user
perception of transparency and fairness (Rossiter & Zehile, 2015).
Studying AX as an analytic framing for making the interaction with and
experience of algorithms explicit, requires us to develop new and innovative methods to assess the media routine humans had never experienced before.
2.1. The functions of algorithms
The rise of advanced data analytics has led to an explosion in the use
of algorithms across a range of industries and public sectors (Duan,
Edwards, & Dwivedi, 2019). In the domain of commerce and marketing,
for instance, algorithms are developed based on customer data to predict consumer behaviors like how they shop, purchase, and use products or services. With the help of algorithms, services are increasingly
driven by data insights, and become far more tailored and accurate
based on speciﬁc UX. For example, Netﬂix uses a combination of personalization algorithms to rank the video content based on its engagement scores to speciﬁc viewers, which help determine what to be
recommended to the viewers in their accounts. Based on user data, the
Netﬂix algorithms help optimize the recommender system and increase
overall engagement with the Netﬂix services. YouTube also utilizes a
complex algorithm to decide the position of videos in its recommended
lists. In order to recommend relevant and personalized video content to
the users, YouTube algorithms analyze user data in terms of average
time spent, video clips watched, click-through rates, involvement
(comments), and hundreds of feedback data points. Amazon, LinkedIn,
and Spotify leverage recommender systems to help people ﬁnd new and
relevant information (products, videos, jobs, music), generating a
meaningful UX while increasing revenue. These recommender algorithm works by ﬁltering and forecasting user estimates and preferences
for items by using content-based ﬁltering and collaborative mechanism.
The algorithm ﬁlters information and identiﬁes groups with similar
tastes to a target user and combines the ratings of that group for making
recommendations to that user. It makes global product-based associations and gives personalized recommendations based on a user’s own
rating. If a user likes the TV series like the Flash, then the algorithm
would recommend shows of a similar genre to the user. Other examples
include algorithmic journalism where news articles are generated by
algorithm program. Through artiﬁcial intelligence software, articles are
produced automatically by algorithms, which interpret, organize, and
present data in people readable ways. Optimizing the trading strategy
for an options-trading market, predicting the driving behavior of selfdriving cars, and balancing the load of electricity grids in varying demand cycles can be recent examples. The combination of AI and algorithms brings the birth of algorithmic commerce: understanding buyer
behavior every second and deliver the products the individual is most
likely to buy.
As users have more deep interaction with algorithms, the research
of AX becomes an emerging ﬁeld in many academic disciplines, generating new insights and knowledge through using algorithms (Dwivedi
et al., in-press; Hughes et al., 2019). Alvarado and Waern (2018) consider AX as the venue in which people experience systems and interfaces that are aﬀected by algorithmic actions. To the general public,
however, there is a low awareness in spite of their growing exposure to

2.2. Algorithmic heuristics: Fairness, transparency, and accountability
Algorithms are becoming more sophisticated, convenient, and pervasive (Shin et al., 2019). With the rapid advance in algorithm technologies, algorithms have produced not only unprecedented opportunities, but also signiﬁcant issues concerning users, society, and
governance (Ziewitz, 2016). The social and ethical issues arising from
an algorithmic society have been progressively addressed by governments and practitioners. Still, questions remain and some of them are
controversial (Diakopoulos, 2016). First, how can accurate and fair
algorithms be designed and operated? Second, how can we make algorithms more accountable and transparent (Shin & Park, 2019)? Third,
can we trust the results of algorithms and ultimately algorithm overall?
These questions could be catalysts for increasing algorithmic visibility.
Together with trust and FAT, algorithmic visibility issues constitute
algorithmic credibility. The black-box nature of algorithms poses
questions concerning whether the services are credible, algorithmic
visibility can be evaluated, and how. Thus, it is urgent to develop algorithm services that are transparent, accurate, and accountable (Lee,
2018).
The topic of fairness in algorithms has begun to attract scholarly
attention, but there is no widely accepted deﬁnition of algorithmic
fairness due to the nature of the elusive concept (Ismagilova et al., inpress). Algorithmic fairness addresses the idea that algorithmic decisions should not produce biased, discriminatory, or unfair results
(Diakopoulos, 2016). Inherent in a question about fairness is the implication that algorithms do not always function fairly (Beer, 2017).
The fairness of an algorithm can be judged based on its precision (i.e.,
the ability to produce precise results), accuracy (i.e., the percentage of
correct results), and recall (i.e., the ability to ﬁnd related results). That
said, achieving algorithmic fairness is not easy as they are developed by
humans, who are particularly vulnerable to biases (Shin et al., 2019).
The notion of transparency in algorithm contexts requires that the
process of generating results via algorithms should be open and transparent to the viewers/users (Cramer et al., 2008; Shin & Park, 2019). In
general, algorithms’ inner maneuvers are hardly known to users due to
the proprietary nature of information as well as the fact that the general
2

International Journal of Information Management 52 (2020) 102061

D. Shin, et al.

public does not have suﬃcient technical expertise to understand them
(Courtois & Timmermans, 2018). Recently, the principles of explainability and understandability became related components of transparency (Meijer, 2014). Can stakeholders interpret and understand the
operations of a system and its results? If easily comprehensible explanations of the system are provided to users, they may forgo the need
for access to the underlying algorithm (Lee & Boynton, 2017). When
people comprehend how automated journalism works, for instance,
they are more likely to consume the content, and trust the embedded
algorithms and recommended results (Ananny & Crawford, 2018).
The concept of algorithmic accountability indicates that the ﬁrms
should be held accountable for the outcomes of their algorithms
(Diakopoulos, 2016), including the goals, structures, and all the functions. Ample cases show algorithms could go wrong in various ways,
such as Uber’s self-driving car being found to skip a stop sign, and
Google’s facial recognition program once labelling African people as
gorillas. Such blunders may be avoided by emphasizing algorithmic
accountability (Shin, 2019).

The trust in algorithmic media is relevant and timely especially
when fake news (e.g., the posts from Twitter bots) threatens the sustainability of algorithm-produced news content. People tend to trust
and use the content when they understand how the content is produced
and what data sources are used (Parizi, Kazemifard, & Asghari, 2016).
During a transparent process, users can easily revise their input in order
to improve results; users are allowed to know the process and the logic
of the algorithms involved. The perception of transparency, fairness,
and accuracy plays a signiﬁcant role by improving user trust in algorithms (Diakopoulos & Koliska, 2016). Transparent algorithms can oﬀer
users a sense of personalization, and concomitantly, responsible and
fair algorithms aﬀord users a sense of trust that fosters satisfaction and
continued use (Shin & Park, 2019). Great visibility and clear transparency in terms of pertinent feedback increase search performance and
satisfaction with the algorithm system (Sloan & Warner, 2017). Guided
by the above literature, the following hypotheses are proposed:

2.3. The algorithm acceptance model

H2. Users’ perception of fairness positively aﬀects their trust of
algorithm services.

H1. Users’ perception of transparency positively aﬀects their trust of
algorithm services.

As algorithms become more mainstream, there is a need to come up
with models explaining the factors associated with algorithm adoption
and algorithmic behavioral actions. When users face a new algorithm,
several factors aﬀect their decision on whether they will use it and how
(Konstan & Riedl, 2012). We see the need to develop the Algorithm
Acceptance Model as a guiding theory of user-centered algorithms on
the basis of the Technology Acceptance Model (TAM), a theory addressing how people adopt and utilize technology (Davis, 1989). The
TAM is limited in explaining the complex process of user heuristic and
systematic process of adoption for emerging technologies like algorithms (Dwivedi, Rana, Jeyaraj, Clement, & Williams, 2019; Rana,
Dwivedi, Williams, & Weerakkody, 2016; Shin, 2019; Tamilmani, Rana,
Prakasam, & Dwivedi, 2019). Our proposed AAM is expanded from
TAM, incorporating antecedent variables of perceived usefulness and
ease-of-use or convenience. Given the nature of algorithms, users perceive them as being useful when their design and services are fair,
transparent, and accurate. Users tend to perceive them as convenient
when the services are reliable and trustable (Shin & Park, 2019; Wolker
& Powell, 2020). In this model, transparency, fairness, and accountability are postulated as antecedents of trust and these relations are
posited as users’ heuristic evaluation of algorithmic credibility. The
AAM also includes the explanatory factors of personalization and accuracy as factors used for users’ systematic process of algorithm utilities. In light of AAM, the following research questions are examined in
this study:

H3. Users’ perception of accountability positively aﬀects their trust of
algorithm services.

3.1. Trust and utility
In the literature on technology acceptance, usefulness and ease of
use have been widely utilized as perceived values. Numerous studies
have shown that these factors are related to user acceptance and
adoption of recommendation systems (Knijnenburg et al., 2012; Pu,
Chen, & Hu, 2012; Zheng, Yang, & Li, 2014). For example, two factors
are found to signiﬁcantly inﬂuence user trust and intention to use
(Alexander, Blinder, & Zak, 2018; Dwivedi et al., 2017). As most algorithm services are easy to use, perceived ease of use can be replaced
with perceived convenience, which is a major goal of employing algorithms. In the TAM literature, extensive research has found the inﬂuence path from usefulness and convenience to attitude change (Hong &
Cha, 2013; Shin, 2010). It is conceivable that user trust in algorithms is
increased due to perceived usefulness and convenience. Thus, the following can be hypothesized:
H4. Trust positively inﬂuences the perceived usefulness of algorithm
services.
H5. Trust positively inﬂuences the perceived convenience of algorithm
services.

RQ1. How do people experience and perceive algorithm-driven
systems?

H6. Usefulness positively inﬂuences the user's attitude toward
algorithm services.

RQ2. How can algorithmic experiences be conceptualized and
modeled?

H7. Convenience positively inﬂuences the user's attitude toward
algorithm services.

RQ3. What impact does user trust have on algorithmic processes and
credibility in algorithm use process? How can an algorithm practitioner
eﬀectively meet the needs of user facing varying degrees of doubts and
uncertainty?

Trust has long been found to be a factor inﬂuencing the actual usage
of system. Previous studies have shown the relation between trust and
behavior (Shin, 2010). In general, when people have trust in certain
technologies, they are more likely to use them. This relation applies to
algorithm services. Shin (2010) argued that trust plays a key role in
technology adoption, particularly the adoption of complicated systems.
Prior studies have repeatedly identiﬁed the key role trust plays in the
process of acceptance, continuance intention, and diﬀusion of technology (Zhang, Wang, & Jin, 2014). User trust obviously aﬀects user
assessment, and such assurance inﬂuences user willingness to provide
more data to the systems or services (Bedi & Vashisth, 2014; Rana,
Dwivedi, Lal, Williams, & Clement, 2017). In the context of algorithms,
trust in algorithms may be deﬁned as the reliability of believing in the
accuracy of the recommender services and adopting the

3. Hypothesis development
As the number of data misusing cases rise, the FAT of algorithms
draws more scrutiny (Ananny & Crawford, 2018; Shin & Park, 2019).
Algorithms are programmed to produce precise and accurate recommendation systems (Shin, 2019), but the outcomes may not mirror
users’ customization, and even fail to serve anyone (Kitchin, 2017). As a
result, FAT emerge as fundamental factors in algorithm services
(Diakopoulos & Koliska, 2016; Shin & Park, 2019).
3

International Journal of Information Management 52 (2020) 102061

D. Shin, et al.

4.1. Heuristic-systematic process

recommendation system (Shin & Park, 2019). Since trust signiﬁes how
reliable and credible the system is, the following hypothesis is formulated:

The primary objective of this study is to clarify the eﬀects of users’
assessment of FAT on user’s assessment and use of FAT when making
adoption decisions. The HSM of information processing serves as the
theoretical foundation of this study. This model is used to explain how
users perceive and process persuasive messages (Sundar, Bellur, Oh, Xu,
& Jia, 2014). This study uses the HSM to provide a theoretical explanation of how a user selects and processes FAT features in arriving at
an adoption decision. The model encompasses two modes of information processing: heuristic processing and systematic processing. According to the HSM, motivation and ability play key roles in the choice
between the two modes of processing.
Heuristic processing uses judgmental rules known as knowledge
structures, which are learned and stored in memory (Shin, 2019). This
process involves the use of heuristics or the simpliﬁcation of decision
rules to quickly assess the message’s validity, thus requiring least cognitive eﬀort on the part of the message recipient. People with limited
time and ability to think carefully tend to engage in heuristic processing
in order to form an attitude toward a message. In doing so, they tend to
rely on more accessible information—such as the source’s identity—in
judging the validity of the message. Heuristic processing is best suited
to situations in which the message recipient lacks the motivation or
ability to systematically process the information or when economic
concerns like time and cost matter (Shin & Biocca, 2018).
Meanwhile, systematic processing involves eﬀorts to understand
available information through careful attention, deep thinking, and
intensive reasoning. In addition to attempting to actively comprehend
and evaluate the arguments of a message, individuals employing systematic processing will assess the validity of an argument in relation to
the message’s conclusion (Chen & Chaiken, 1999). Accordingly, this
process eventually results in an informed evaluative judgment and/or
decision (Shin & Park, 2019) and greater conﬁdence regarding the
adoption decision (Shin, 2019).
Per the heuristic-systematic process, methods were designed dualstage: qualitative methods were used for understanding users’ heuristic
assessment of FAT and quantitative methods were used for analyzing
the users’ systematic process of adoption (Fig. 2).

H8. Trust positively inﬂuences the actual use of algorithm services.

3.2. Personalization and accuracy
Algorithm systems represent a set of personalization features that
help people search through massive amount of information.
Personalized content needs to be accurate as users expect personalized
recommendations match their preferences. Personalization and accuracy are the two important criteria determining a user's perceived utility of the system. Accuracy is about whether the recommender system
predicts those items that people have already evaluated or interacted
with; recommender systems with optimized precision will prioritize
more related items for its users.
Overall, personalization can be a key attribute of an algorithm
system (Soﬀer, 2019). A personalized news recommendation service
acts as an information ﬁlter with the capability of learning a user's interests and preferences, according to their proﬁle or past history. When
users get the sense that news recommendations are personalized to
their needs, they rate the service useful, and feel more satisﬁed with the
content (Kim & Lee, 2019). Users view the algorithm as easy to use and
convenient as long as they perceive the recommended items or contents
accurate. Empirical evidence has conﬁrmed these relationships in various algorithm services (Li, 2016), in which personalization and accuracy are found to be determinants of trust and satisfaction (Shin & Park,
2019). Thus, two ﬁnal hypotheses are developed:
H9. Perceived personalization positively inﬂuences the utility of
algorithm system.
H10: Perceived accuracy positively inﬂuences the convenience of
algorithm system (Fig. 1).

4. Methods
The method applied the heuristic-systematic model (HSM) of information processing to perceptions of trust in the adoption process of
algorithm. With HSM in place, it examines transparency, fairness, and
accountability as factors that inﬂuenced trust perceptions and subsequent cognitive processes.

4.2. Heuristic process: qualitative methods
Qualitative methods, such as focus groups, interviews, cognitive
walkthroughs, and expert interviews, were used to obtain insights on
algorithms, algorithmic services and applications. First, focused interviews were conducted with selected algorithm users. They were

Fig. 1. Heuristic-Systematic Model of Algorithmic Process.
4

International Journal of Information Management 52 (2020) 102061

D. Shin, et al.

Fig. 2. Heuristic-Systematic Methods.

5. Findings

students taking university courses related to system design and digital
technologies, who were constantly working on algorithms and familiar
with their designs. They were encouraged to freely express their perceptions and views on algorithm and algorithm-powered digital media
contend.
Second, with three focus group sessions (utilizing diﬀerent participants, but the same questions), some patterns emerged. In these sessions, teams with ﬁve to seven participants discussed their current
usage of algorithm-based services and their feedback provided a list of
possible variables/patterns concerning algorithmic experience. The
objective of the in-depth interviews and focus group sessions was to
gain feedback on the pilot analyses, perform a needs analysis, identify
concepts that might be missed in the model, and obtain an overall
structure of the factors that enable interaction and trigger heuristics.
Third, a cognitive walkthrough method was used to identify usability issues in algorithmic services. To analyze the interaction between algorithms and humans, we examined the ways algorithm-powered media content being used. Respondents were given speciﬁc tasks
(browsing, viewing, reviewing, and content curation) using one of their
preferred media providers (search engines, online news websites, social
networking sites, etc.). After the tasks, the respondents openly discussed the experiences, problems, and usability issues.

5.1. Data analysis
5.1.1. Measurement instrument
The reliability and validity of the measurement instrument were
evaluated with AMOS 18 using the reliability and convergent validity
criteria. The reliability of the survey instrument was established by
calculating Cronbach’s alpha to measure internal consistency. All values
were above the recommended level of 0.7. The convergent and discriminant validity of the model were examined using the procedure
suggested by Fornell and Larcker, who recommend measuring the reliability of each measure and each construct, as well as the average
variance extracted (AVE) for each construct. The reliability of each item
was examined according to a principle component factor analysis. The
results of this analysis, with varimax rotation on the original 30 items
(seven items were eliminated due to low loading). According to Hair
et al., measurement of items loads highly if the loading coeﬃcient is
above 0.6. This analysis showed that most items had factor loadings
higher than 0.7, which Fornell and Larcker considered to be very signiﬁcant. Each item loaded signiﬁcantly on its underlying construct
(p < 0.01 in all cases). Therefore, all constructs in the model had
adequate reliability and convergent validity (Table 3).
To examine the discriminant validity, this study compared the
shared variance between constructs with the average variance extracted
from the individual constructs. The shared variance between constructs
was lower than the average variance extracted from the individual
constructs, conﬁrming discriminant validity (Table 2). In short, the
measurement model demonstrated adequate reliability, convergent and
discriminant validity. Content validity is a characteristic of items that
are representative and drawn from established literature. The variables
in this study that were derived from the existing literature exhibited
strong content validity. A list of the construct items used in the questionnaire is presented, along with their sources in the literature. The
wording used in the questionnaire appears similar, so that the responses
can be highly inter-correlated. In addition, a correlation analysis of
Pearson’s R (correlation coeﬃcient) shows an acceptable level of correlation among variables (Table 4).

4.3. Systematic process: the survey method
Based on the qualitative analyses, a survey was constructed using a
7-point Likert scale from 1 = Strongly Disagree to 7 = Strongly Agree.
The participants were recruited through the college program related to
emerging technologies and systems, including algorithms. The drafted
survey was ﬁeld-tested in Seoul before being rolled out in several areas
of South Korea. A total of 19 students from three Korean colleges participated in the pretest with tests given at three-week intervals. The
ﬁnalized survey was ﬁelded in late 2018. During the data collection,
follow-up calls and emails were made to encourage non-respondents to
take part in the survey (Table 1).
Table 1
The sample demographics.

5.1.2. Structural model
A test of the structural model was performed using the AMOS procedure, and a maximum likelihood-based SEM software was used.
Table 4 shows the estimates from the structural modeling. The overall
ﬁt of the model is satisfactory, with all of the relevant goodness of ﬁt
indices greater than 0.90. The GFI is 0.95, the AGFI 0.91, and the TLI
0.87. Similarly, there is no evidence of misﬁt, with the RMSEA showing
a very satisfactory level of 0.067, which favorably compares to the
benchmarks by Joreskog and Sorbom, who suggest that values of 0.06
or more reﬂect a close ﬁt. The standardized RMR was also very good, at
0.027, well below the threshold for a good overall ﬁt. Another positive
test statistic was the normed chi-square value (a chi-square divided by
degrees of freedom) of 1.98, a value that is appropriately below the
benchmark of three, to indicate good overall model performance. Given
a satisfactory measurement of the model’s ﬁt to the data, the path

Number
Age (year)
Under 20
21–35
36–45

99
201
45

Prior Experience (month)
1–5
6–9
10–12
Over 12 months

56
59
101
129

Gender
Female
Male
Total Participants

166
179
345

5

International Journal of Information Management 52 (2020) 102061

D. Shin, et al.

Table 2
Discriminant validity.
Construct

1

2

3

4

5

6

7

8

9

10

Usage
Accountability
Personalization
Accuracy
Convenience
Transparency
Fairness
Trust
Attitude
Usefulness

0.71
0.21
0.33
0.13
0.12
0.23
0.42
0.52
0.23
0.31

0.83
0.52
0.23
0.11
0.53
0.22
0.11
0.31
0.11

0.83
0.52
0.23
0.13
0.53
0.22
0.14
0.52

0.93
0.42
0.52
0.21
0.22
0.52
0.13

0.72
0.11
0.53
0.22
0.11
0.22

0.82
0.13
0.53
0.22
0.22

0.82
0.23
0.42
0.52

0.73
0.22
0.19

0.73
0.22

0.81

Notes: Diagonals represent the AVE. Other entries represent the shared variance.
Table 3
Reliability checks for constructs.

Table 5
Results of Hypothesis Testing.

Construct

Initial items

Final items

M (SD)

Cronbach’s alpha

Paths

Coeﬃcient

S.E.

C.R.

Usage

4

3

0.795

Accountability

4

3

Personalization

5

3

Accuracy

4

3

H1: Transparency→ Trust
H2: Fairness → Trust
H3: Accountability → Trust
H4: Trust → Usefulness
H5: Trust → Convenience
H6: Usefulness → Attitude
H7: Convenience → Attitude
H8: Accuracy → Convenience
H9: Personalization →Usefulness
H10: Attitude → Use behavior

0.480
0.443
0.149
0.647
0.839
0.402
0.518
0.864
0.048
0.913

0.072
0.077
0.064
0.051
0.075
0.049
0.044
0.041
0.037
0.046

6.647***
5.763***
2.318**
12.713 ***
3.570***
4.060***
11.723**
21.249***
1.287*
19.692***

Convenience

4

3

Transparency

4

3

Fairness

3

3

Trust

3

3

Attitude

3

3

Usefulness

3

3

4.17 (1.011)
3.84 (1.055)
4.11 (1.047)
4.17 (1.011)
3.84 (1.055)
4.11 (1.047)
3.39 (1.054)
3.98 (1.128)
3.30 (1.282)
3.74 (1.346)
3.87 (1.385)
3.40 (1.411)
4.37 (1.182)
4.30 (1.141)
4.63 (1.202)
4.37 (1.182)
4.30 (1.141)
4.63 (1.202)
4.34 (1.210)
4.44 (1.295)
4.05 (0.984)
4.42 (1.121)
4.10 (1.264)
4.18 (1.268)
4.37 (1.114)
4.25 (1.051)
4.12 (1.033)
4.38 (1.107)
4.56 (1.163)
4.42 (1.276)

0.619

0.875

0.812

0.842

Note: Signiﬁcance: *1.56: 95 % (0.05), **2.58: 99 % (0.01), ***3.29: 99.9 %
(0.001).

0.853

coeﬃcients of the structural model were assessed.

0.856

5.2. Algorithmic experiences
The path analysis showed that the relationships outlined in the 10
hypotheses were supported, as all the coeﬃcients and critical ratios
were signiﬁcant (Table 5). User behavior was signiﬁcantly inﬂuenced
by attitude (coeﬃcient 0.913; C.R. 19.5) and trust. User behavior accounted for approximately 47 % by attitude and trust. Attitude is
greatly inﬂuenced by usefulness and convenience. The eﬀects of trust
on usefulness and convenience were signiﬁcant (C.R. = 12.713***;
C.R. = 3.570***). Trust was inﬂuenced by transparency, fairness, and
accountability. The strong paths implied an underlying link between
the trust factor and perceived value through a trust-perceived valueattitude-intention process. Convenience and usefulness were associated

0.900

0.890

0.900

Table 4
Correlation and principal component analysis with varimax rotation.

Note: Selected image capture. Due to the page template, the whole table cannot be shown.
6

International Journal of Information Management 52 (2020) 102061

D. Shin, et al.

systems (Kizilcec, 2016).
Despite their prevalence, humans often show feelings of aversion to
autonomous algorithms. Such aversion occurs when the operation of
algorithm leads to irresponsible or biased use of algorithms. Thus, if
they are to become a mainstream and essential tool in people’s daily
lives, algorithms need to be used along with FAT mechanisms. The
established FAT mechanisms surely help promote trust within society.
Since the results in the model show that user trust in algorithm services
has a signiﬁcant inﬂuence on user attitudes on adoption, the concept of
user trust will inﬂuence the design and development of future algorithm
systems.

Table 6
Experimental Design.

High trust tendency
Low trust tendency

Algorithm services

Human services

Group 1
Group 3

Group 2
Group 4

with user expectations, which then signiﬁcantly inﬂuenced user attitude and intention in using algorithms. The highly signiﬁcant eﬀect of
trust indicated possible relations among other factors, for instance, the
interaction eﬀect of trust in algorithm FAT features upon attitude and
value.

6. Discussion
When people buy a recommended product online or read a news
article on Google or Apple news app, they often rely on suggestions
from algorithm-powered recommender systems. While the use of algorithm systems is on the rise, however, they are still far from being our
primary means of communication and interaction, and numerous issues
have arisen. To make algorithm systems more suitable for everyday use,
it is necessary to gain an understanding as to how users come to accept
and adopt them. To explain user cognitive processes of recommender
systems, the AAM was proposed based on AX. The research results show
that the model is the suitable acceptance model to predict user acceptance, as it provides a suﬃcient level of detail and ﬁts the existing body
of work in algorithms and technology adoptions. The results suggest the
relationships of FAT on trust are signiﬁcant, which expand the HSM to
trust in algorithm.
This study proposed the idea of AX to examine the manner in which
users experience algorithms and interfaces that are mutually shaped by
user action. The AAM in this study suggests the constituents and process
of algorithmic experience when users accept, use, and interact with
algorithm system. Expanding from TAM, algorithm experience includes
perceived value, attitude, intention to use, and trust. AAM reﬂects the
unique nature of algorithms in terms of trust, personalization, and accuracy.
People would like to conﬁrm the validity and reliability of algorithm systems. Users using the FTA heuristics showed a better acceptance of the algorithm services because they are able to understand the
system better and able to increase their algorithm literacy. FAT serves
the cues that people use to judge and determine the qualities of algorithm (Shin & Park, 2019). People resort to FAT when they face the
credibility of algorithms, that is FAT oﬀers people the heuristic mechanism how to evaluate algorithmic credibility. In other words, FAT
issues work as a nudge by guiding users to understand algorithmic
qualities and help users change their behaviors. FAT issues are the
antecedents to perceived value, and at the same time they aﬀect trust.
Users do not simply trust algorithms, they would like to conﬁrm algorithmic credibility, in terms of whether or not they are transparent, fair,
and accountable. Users attempt to estimate how trustable algorithm
system through heuristic process using FAT. When these issues are
conﬁrmed through users’ cognitive process, users start to trust, and
acceptance processes step in. FAT inﬂuences the perceived usefulness
and convenience, which then aﬀect attitude and intention. Yet all the
processes are not discretely processed but represent one continuous
process with interactions.
The results of this study contribute to a developing understanding of
algorithms, users, and experience. First, our ﬁndings contribute to the
scholarly literature and practical knowledge regarding the interfaces
between humans and algorithms. Algorithm technologies are increasingly characterized as an ecosystem of complex, sociotechnical matters
(Shin, 2019). There has been a need to develop analytical models for
describing the interaction with, and experience of algorithms. By conceptualizing and developing a scale to measure AX components, this
work contributes to the research body on how to warrant illusive and
equivocal algorithmic concerns, and how to design algorithm systems

5.3. Interaction eﬀects of algorithm process
An interaction eﬀect refers to the concurrent eﬀect of two or more
independent factors on at least one dependent factor, in which the
combined eﬀect is signiﬁcantly stronger than the sum of their individual eﬀects (Shin & Biocca, 2018). An interaction eﬀect was identiﬁed through ANOVA analyses between trust and FAT, as to value and
attitude (F[1, 30] = 6.69, p < 0.01). The existence of the interaction
eﬀects implies that the higher trust tendency with the high FAT perception yielded a more positive value and attitude, consequentially
leading to a higher intention than did the low trust tendency with the
low FAT perception (Table 6). People with high trust tendency found
the algorithm more transparent, accountable and fairer than non-algorithm services, whereas people with low trust tendency found nonalgorithm services generated higher accountability, transparency, and
fairness than algorithm services. Trust tendencies and the algorithm’s
properties have combined eﬀects on attitudes and experiences. The
high trust tendency users perceive more positive value and feel more
acceptable to FAT, whereas the low trust tendency users feel more
gratiﬁed with non-algorithm services.
The issues of FAT are interrelated, and they are interacting each
other creating interaction eﬀects on perceived utility in algorithm
system. The interaction eﬀects are understandable because it is hard for
normal users to discern what transparent is and what fair is. Normal
users rely on their existing trust in algorithms and technologies when
they face issues such as fairness, transparency, and accountability. They
usually do not clearly understand what FAT is and how it aﬀects the
quality of algorithm services. In reality, the concepts of FAT are interrelated and overlap somewhat. Users utilize existing trust as a heuristic
when evaluating FAT in algorithms. Trust is a cue to assess the FAT of
algorithms, and processed perceptions of FAT trigger attitude formation
and behavior.
Many users are averse to using algorithms, preferring instead to rely
on their instincts when it comes to algorithm decisions. Users develop
their own heuristic understanding of algorithmic trust based on the
cognitive process of FAT. User perception of personalization and accuracy is not automatically granted or formed; rather, it is dependent
upon how users recognize FAT. This inference has heuristic implications for algorithm development and UX. Issues of transparency and
accuracy are hot topics in algorithm research; users are concerned with
these issues. It can be inferred, based on previous research, that trust is
closely interrelated with these issues as it plays a signiﬁcant role in
establishing trust and heuristics (Shin, 2017; Shin & Park, 2019). When
users are assured with FAT issues, their trust levels can be increased,
and they are willing to allow more of their data to be collected and
processed. With the increased trust between users and algorithms, more
transparent processes are warranted, and more data will enable algorithms to produce accurate results tailored and individualized to users’
preferences and personal history. Thus, while the relationship between
trust and perceived algorithm features is not examined in this study, it
would be worthwhile to empirically test this relationship. Trust will be
a key factor in positive feedback loops between users and algorithm
7

International Journal of Information Management 52 (2020) 102061

D. Shin, et al.

on the model, this study made strides forward and showed that there is
signiﬁcant potential for the model as a cue for user-centered algorithm
design in various sectors. Future studies may further examine the algorithm acceptance with a more solid model and with a more representative population. For example, it may be useful to understand
varying FAT notions held by users considering people, contexts, and
interfaces for algorithms to work fairly in the real world.

that are user-centered and socially responsive.
Second, the AAM advances the current technology adoption model
by recognizing contextual issues and the core relationships among them
(Alvarado & Waern, 2018; Thurman, Moeller, Helberger, & Trilling,
2019). In the same manner, the discussion of AX in this study could
advance the current literature of UX by clarifying the algorithm motivation and behavior. Although the TAM and UX are still useful, the
TAM is designed for general technology and the UX concept pertains to
wide-ranging system experience. While AAM and AX may be conceptually similar to TAM and UX, AAM and AX address speciﬁc to algorithms (internal properties and inﬂuence of such properties in user
heuristics); instead of having them deal with front-end of technological
properties. In many ways, algorithms are diﬀerent from conventional
technologies, and algorithm-speciﬁc factors are needed to model them.
As innovative algorithm services rapidly develop, the traditional technology-based framework and conventional user framework must be
modiﬁed to reﬂect the evolving computing paradigms. An understanding of how people recognize algorithm functionality, how their
attitudes are formed, how the behavioral intentions are performed,
what cognitive views are held, and what outcomes are derived from the
cognitive process is critical. The results, particularly in terms of a userbased approach and perception-based quality measurement, will enable
future studies to make signiﬁcant steps towards the development of a
human-centered algorithm framework.
Third, our ﬁndings provide practical guidelines for algorithm design
and other related services such as machine learning and broader artiﬁcial intelligence. The industry has struggled to ensure the accuracy
and eﬀectiveness of algorithm-driven services. The industry is being
requested to develop advanced algorithmic visibility and user satisfaction, upon which many critical decisions are based (Moller,
Trilling, Helberger, & van Es, 2018). As more creative content and
complex services are introduced through algorithms, FAT as algorithmic visibility becomes a critical diﬀerentiator among diverse algorithm services (Diakopoulos, 2016; Shin, 2019). In algorithms, how
users perceive FAT and actualize trust aﬀordances is considered more
urgent than technical qualities such as prediction and accuracy (Shin,
2019; Shin & Park, 2019). Per Ettlinger (2018), considering algorithmic
aﬀordances is useful as the key concept of a user interface for algorithms. This study’s outcomes will yield insight into users’ heuristic and
systematic processes of algorithms via the aﬀordance lens. The heuristic
identiﬁed here will be a starting point of conceptualizing nudge in algorithmic context—algorithmic nudge. Examining user aﬀordance can
be the essential step towards the development of a user-centered interface for algorithm services.

6.2. Implications
The contributions of this work are both theoretical and practical.
The implications help shed light on the "black box" of algorithmic and
cognitive processes. There is an increasing need to understand the
mediating aﬀordance of trust at both a practical and theoretical level.
The ﬁndings from this study support the AAM and help to conceptualize
and operationalize AX.
6.2.1. Theoretical implications
This research contributes to the literatures on trust and technology
aﬀordances in algorithmic contexts. It provides a more comprehensive
view of the determinants of user attitudes by combining trust of algorithm services with TAM and HSM. The two processes of heuristic and
systematic processing occur in how users assess and judge algorithm.
The current study expands the HSM to the algorithm acceptance literature, demonstrating heuristic and systematic processing are occurring when perceiving algorithm qualities. Our results veriﬁed that algorithmic use and interactions were positively related to perceived
values, which were positively related to user processing of transparency
and accuracy and to future intention.
In addition, the results of this study contribute to theoretical development in this ﬁeld by clarifying how trust is formed, and, from
there, how trust can be conceptualized as aﬀordances (or nudge). This
research is a focused attempt to broaden the current literature on trust
of algorithmic processes. The results show that algorithmic features and
interactions are positively connected to trust. By showing the relations
of perceived features and personalization and accuracy, this study sheds
light on the connections between algorithmic properties, algorithmic
experiences, and user interactions with algorithms. From these links,
cues can be inferred: how algorithmic properties provide users with
cues for trust, and how trust allows users to use algorithms with a
feeling of security and personalization. These results contribute to the
new area of human-algorithm interaction by showing the feedback
loop: how people’s own attitude and behavior aﬀect the output of algorithms. While people are aﬀected by the output of all types of machine learning algorithms, the continuous interaction between humans
and algorithms is rarely taken into account in human-algorithm interaction. The ﬁndings can be a possible vector for further development of
human-algorithm interaction. New concepts such as algorithmic nudge,
aﬀordance, and algorithm literacy are expected to trigger ample new
research areas.
The more people are exposed to and use algorithms, the more likely
they are to trust algorithm services. While obviously people’s increased
familiarity with algorithms may contribute to enhanced trust, the trust
mechanism can be viewed from a theoretical perspective of FAT. Trust
is conﬁgured by user feedback and input, which are inﬂuenced by the
perceived algorithm properties such as FAT. FAT does not stimulate
passive users perfunctorily but instead aﬀord users to play an active
function by recognizing algorithmic features and utilizing them in their
use and interaction processes. People’s attitudes toward algorithms are
formed during user actions, motivations are formed while they interact
with algorithms, and attitudes and motivations together embody trust,
and, subsequently, intention and behavior. Assurances and trust are
materialized and embodied through interaction with algorithm services
and processes. While this argument is in line with embodied interaction, trust in algorithms is formed through active involvement, or
"trusting by doing"; users determine the nature of trust (based on

6.1. Limitations and future studies
This study contributes to the literature by formulating and validating the algorithm model to investigate the acceptance model of algorithm and provides heuristic information for both academia and industry. Whereas the ﬁndings of this study are legitimate, the results
must be interpreted with caution for the following reasons. Firstly, the
responding users might not represent the whole population because
predominant users of algorithm remain young people. The subjects of
the study were recruited as representatives of young users. Thus, this
study does not provide a comprehensive picture of entire algorithm
populations; rather, it just provides a snapshot of a subset of user
proﬁles. Future studies may examine user motivations from diﬀerent
groups of users with a longitudinal investigation.
Secondly, the current study did not consider external factors (different platform infrastructure, service provisions, and providers of algorithms) as algorithms are in still early stage of development. Wide
diﬀerences in services across diﬀerent algorithms may exist, and user
expectation and experiences may diﬀer accordingly. Given a signiﬁcant
increase variance of usage in user research, future studies can consider
various variables as covariates. Although more work needs to be done
8

International Journal of Information Management 52 (2020) 102061

D. Shin, et al.

are also procedurally correct, objective, and transparent. Understanding AX will be critical for predicting users’ future behaviors. This
task will be challenging, because users may have ever-changing interests, and because there is fast-moving innovation and ever-increasing
competition among providers. The algorithm model in this work will
provide insights on how to deal with transparency and fairness issues
with usability features and concept of behavioral intention. How to
reﬂect the vague issues of FAT into the design and how to strike balance
between FAT and utility will be critical challenges as well as opportunities for industry. The ultimate goal of algorithmic and automated
process systems is to progress towards user-centered and human-based
algorithm services. Integrating a user cognitive process into AX design
presents users with pertinent and meaningful information that they can
use to make decisions. Algorithms that are human-centered and include
trust-based feedback loops will be important for developing such
human-centered artifacts. The model identiﬁed in this study serves as a
ﬁrst step in the achievement of such long-term goals. The identiﬁed
algorithmic trust processes open new venues for research. Future research can examine trust and aﬀordances in diverse emerging algorithmic domain in greater detail. Also, it is worthwhile to develop
concrete methods to measure AX.

algorithmic features and individual intrinsic traits) by interacting with
algorithms. The FAT features of algorithms respond to tactile input
from the user, while user input hinges on the technological qualities of
the system (Shin & Park, 2019). Users can shape and reshape algorithmic outcomes.
FAT may inﬂuence users’ informed trust that is related to the use of
information to judge whether trust is aﬃrmed. In this way, users may
display a more rational response rather than an emotional response to
information. In other words, informed trust may involve the users
carefully weighing up a situation, whereas embodied trust may involve
the users basing their decision on the reputation of the providers or
services. Embodied trust implies a provider service is unquestioned and
users are well intentioned. This may be contrasted with the case of
informed trust, in which users may express greater doubt and skepticism about algorithmic qualities and features.
6.2.2. Practical implications
The immediate practical contributions can be the ﬁndings provide
useful insights for new service developments. For the developers of
algorithm-based recommendations or other similar services of machine
learning, the implications of this study can help advance system performance and the UX of their products. With the identiﬁed roles of
accuracy and transparency in the UX of algorithms, the industry may
develop a new framework that applies a user heuristic-systematic approach to algorithm and machine learning systems.
The ﬁrst practical suggestion for the algorithm industry is a strategic
focus on the UX in algorithm design. User heuristics and psychological
processes are vital in analyzing how people feel about the challenging
issues surrounding algorithms, and how they experience algorithmbased services. The main goal of an algorithm is to help people ﬁnd
content (for online shopping, reading, or entertainment) that is interesting to them. Identifying how users search, understand, and consume
algorithm content is key to the optimization process of algorithm to
perform more eﬀectively and sustainably. There have been numerous
challenges to successfully oﬀering recommended results in the algorithmic context. The incorporation of AX is critical in producing accurate and precise results. In order to promote algorithm services, the
industry should design AX-based algorithm systems while promoting
trust on algorithm services.
Design of algorithmic interfaces is as an important area of development and AX should be focus of such design. The results of this study
provide the algorithm industry with practical guidelines on how to
integrate FAT issues into operational interface principles or general
usability policy (for instance, how to collect user data and/or implicit
feedback eﬀectively, while enhancing user satisfaction and trust).
Second, users’ perception and trust on algorithm system is a major
factor in determining their perceived value and behavior, which aﬀect
their decision on whether to adopt using algorithm system.
Accordingly, it can be suggested that users are willing to accept algorithm system, but still has concern about the FAT of the system. Thus,
algorithm system can be promoted by building user trust as a reliable,
credible, and trustworthy system. More broadly, understanding algorithms provides implications for algorithmic governance. As algorithmic governance, i.e., the automated collection, aggregation, and
analysis of data, using algorithms to model, predict, and pre-emptively
aﬀect and govern potential behaviors (Shin, 2019), it is critical to ensure legality and legitimacy. The AAM and AX can help ensure that
algorithms are a valid tool for satisfying a justiﬁable policy goal, and

7. Conclusion
This study proposes and tests a conceptual model that analyzes the
relations between perceptions of FAT, trust and intention, while also
going deeper into their relations. It applied a mixed approach for the
analysis: ﬁrst, a qualitative method to explore the people’s heuristics
towards the algorithm system; second, a cognitive walkthrough to elicit
AX-based experience; and third, a survey was constructed and conducted to model the relations among the factors identiﬁed from the
qualitative data. Our ﬁndings validate the AAM conceptualizing the
notion of AX as part of the analytic framework for human-algorithm
interaction. Our tested model suggests a dual route of inﬂuence of FAT
on users’ intention in algorithm system: one through heuristic user
cognitive process and the other through the systematic process evoked
by the accuracy and personalization of the system. From the identiﬁed
heuristic-systematic process, we further argue that AX is inherently
related to human understanding of fairness, transparency, and other
conventional components of user-experience, indicating the heuristic
roles of transparency and fairness regarding their underlying relations
of user experience and trust. AX can inﬂuence the user perception of
algorithmic systems in the context of algorithm ecology, oﬀering useful
insights into the design of human-centered algorithm systems. This new
AX framework for algorithm system contributes to a focused approach
for designing human-centered algorithm systems. Future research can
further examine how human-centered algorithm systems can be concretely applied by focusing on questions like how we can design algorithmic services to make the services that are not only accurate and
relevant, but also reliable and fair.
CRediT authorship contribution statement
Donghee Shin: Conceptualization, Data curation, Formal analysis,
Funding acquisition, Investigation, Methodology, Project administration, Software, Supervision, Validation, Visualization, Writing - original
draft, Writing - review & editing. Bu Zhong: Investigation. Frank A.
Biocca: Resources.

Appendix A
Table A1

9

International Journal of Information Management 52 (2020) 102061

D. Shin, et al.

Table A1
Survey Measurement Items.
Variables
Fairness

Accountability

Transparency

Personalization

Accuracy

Trust

Use behavior

Attitude

Measures
1 The system has no favoritism and does not discriminate against people (Nondiscrimination)
2 The source of data throughout an algorithm and its data sources should be identiﬁed, logged, and benchmarked (Accuracy)
3 3. I believe the system follows due process of impartiality with no prejudice (Due process).
1 I think that the system requires a person in charge who should be accountable for its adverse individual or societal eﬀects in a timely fashion
(Responsibility)
2 Algorithms should be designed to enable third parties to examine and review the behavior of an algorithm (Auditability)
3 3. Algorithms should have the option to modify a system in its whole conﬁguration using only certain manipulations (Controllability)
1 I think that the evaluation and the criteria of algorithms used should be publicly released and understandable to people (Understandability).
2 Any outputs produced by an algorithmic system should be explainable to the people aﬀected by those outputs (Explainability)
3 3. Algorithms should let people know how well internal states of algorithms can be understood from knowledge of its external outputs (Observability)
1 I think that the recommended items reﬂect my personalized preferences.
2 I found the recommended items are a great match to my needs.
3 3. It seems that the algorithm-based service is customized to me.
1 I think the contents produced by algorithms are accurate.
2 Recommended items by algorithm systems are in general precise.
3 3. Algorithm-enabled recommendations are exact and correct.
1 I trust the recommendations by algorithms-driven services.
2 Recommended items through algorithmic processes are trustworthy.
3 3. I believe that the algorithm service results are reliable.
1 I would like to intend to use algorithm services.
2 I will continue to use algorithm services.
3 3. It is my intention to use and will further adopt algorithm services.
1 Largely, I am fairly pleased with algorithm services.
2 Overall, the algorithm services fulﬁll my initial expectations.
3 3. Generally, I am happy with the contents of algorithm services.

References

191–207. https://doi.org/10.1080/1369118X.2016.1271900.
Hong, I., & Cha, H. (2013). The mediating role of consumer trust in an online merchant in
predicting purchase intention. International Journal of Information Management, 33(6),
927–939. https://doi.org/10.1080/10447318.2018.1437864.
Hughes, L., Dwivedi, Y., Misra, S., Rana, N., Raghavan, V., & Akella, V. (2019).
Blockchain research, practice and policy. International Journal of Information
Management, 49, 114–129. https://doi.org/10.1016/j.ijinfomgt.2019.02.005.
Ismagilova, E., Dwivedi, Y., & Slade, E. (2020). Perceived helpfulness of eWOM:
Emotions, fairness and rationality. in-press Journal of Retailing and Consumer Services.
https://doi.org/10.1016/j.jretconser.2019.02.002.
Kim, D., & Lee, J. (2019). Designing an algorithm-driven text generation system for
personalized and interactive news reading. International Journal of Human-computer
Interaction, 35(2), 109–121. https://doi.org/10.1080/10447318.2018.1437864.
Kitchin, R. (2017). Thinking critically about and researching algorithms. Information,
Communication and Society, 20(1), 14–29. https://doi.org/10.1080/1369118X.2016.
1154087.
Kizilcec, R. (2016). How much information? Chi 2016https://doi.org/10.1145/2858036.
2858402 May 7–12, 2016, San Jose, CA.
Knijnenburg, B., Willemsen, M., Gantner, Z., Soncu, H., & Newell, C. (2012). Explaining
the user experience of recommender systems. User Modeling and User-adapted
Interaction, 22, 441–504. https://doi.org/10.1007/s11257-011-9118-4.
Konstan, J. A., & Riedl, J. (2012). Recommender systems. User Modeling and User-adapted
Interaction, 22(2), 101–123.
Lee, M. (2018). Understanding perception of algorithmic decisions: Fairness, trust, and
emotion in response to algorithmic management. Big Data & Society, 5(1), 1–16.
https://doi.org/10.1177/2053951718756684.
Lee, B., & Boynton, L. (2017). Conceptualizing transparency: Propositions for the integration of situational factors and stakeholders’ perspectives. Public Relations Inquiry,
6(3), 233–251. https://doi.org/10.1177/2046147X17694937.
Li, C. (2016). When does web-based personalization really work? Computers in Human
Behavior, 54, 25–33. https://doi.org/10.1016/j.chb.2015.07.049.
Meijer, A. (2014). In M. Bovens, R. Goodin, & T. Schillemans (Eds.). Transparency. In the
Oxford handbook of public accountabilityOxford University Presshttps://doi.org/10.
1093/oxfordhb/9780199641253.013.0043.
Moller, J., Trilling, D., Helberger, N., & van Es, B. (2018). Do not blame it on the algorithm. Information, Communication and Society, 21(7), 959–977. https://doi.org/10.
1080/1369118X.2018.1444076.
Montal, T., & Reich, Z. (2017). I, robot. you, journalist. Who is the author? Digital
Journalism, 5(7), 829–849. https://doi.org/10.1080/21670811.2016.1209083.
Parizi, A., Kazemifard, M., & Asghari, M. (2016). EmoNews. Journal of Digital Information
Management, 14(6), 392–402.
Pu, P., Chen, L., & Hu, R. (2012). Evaluating recommender systems from the users perspective. User Modeling and User-adapted Interaction, 22(4), 317–355. https://doi.org/
10.1007/s11257-011-9115-7.
Rana, N. P., Dwivedi, Y. K., Williams, M. D., & Weerakkody, V. (2016). Adoption of online
public grievance redressal system in India: Toward developing a uniﬁed view.
Computers in Human Behavior, 59, 265–282.
Rana, N. P., Dwivedi, Y. K., Lal, B., Williams, M. D., & Clement, M. (2017). Citizens’
adoption of an electronic government system. Information Systems Frontiers, 19(3),
549–568.
Rossiter, N., & Zehile, S. (2015). The aesthetics of algorithmic experience. In R. Martin

Alexander, V., Blinder, C., & Zak, P. (2018). Why trust an algorithm? Performance,
cognition, and neurophysiology. Computers in Human Behavior, 89, 279–288. https://
doi.org/10.1016/j.chb.2018.07.026.
Alvarado, O., & Waern, A. (2018). Towards algorithmic experience. Proceedings of the
2018 CHI Conference on Human Factors in Computing Systems, 286. https://doi.org/10.
1145/3173574.3173860.
Ananny, M., & Crawford, K. (2018). Seeing without knowing: Limitations of the transparency ideal and its application. New Media & Society, 20(3), 973–989. https://doi.
org/10.1177/1461444816676645.
Bedi, P., & Vashisth, P. (2014). Empowering recommender systems using trust and argumentation. Information Sciences, 279, 569–586. https://doi.org/10.1016/j.ins.
2014.04.012.
Beer, D. (2017). The social power of algorithms. Information, Communication and Society,
20(1), 1–13. https://doi.org/10.1080/1369118X.2016.1216147.
Chen, S. C., & Chaiken, S. (1999). The heuristic-systematic model in its broader context.
In S. Chaiken, & Y. Trope (Eds.). Dual-process theories in social psychology (pp. 73–96).
New York, NY: The Guilford Press.
Courtois, C., & Timmermans, E. (2018). Cracking the tinder code: An experience sampling
approach to the dynamics and impact of platform governing algorithms. Journal of
Computer-Mediated Communication, 23(1), 1–16. https://doi.org/10.1093/jcmc/
zmx001.
Crain, M. (2018). The limits of transparency: Data brokers and commodiﬁcation. New
Media & Society. https://doi.org/10.1177/1461444816657096.
Cramer, H., Evers, V., Ramlal, S., van Someren, M., Rutledge, L., Stash, N., et al. (2008).
The eﬀects of transparency on trust in and acceptance of a content-based art recommender. User Modeling and User-adapted Interaction, 18(5), 455–496.
Diakopoulos, N. (2016). Accountability in algorithmic decision making. Communications
of the ACM, 59(2), 58–62.
Diakopoulos, N., & Koliska, M. (2016). Algorithmic transparency in the news media.
Digital Journalism, 5(7), 809–828. https://doi.org/10.1080/21670811.2016.
1208053.
Duan, Y., Edwards, J., & Dwivedi, Y. (2019). Artiﬁcial intelligence for decision making in
the era of Big Data–evolution, challenges and research agenda. International Journal of
Information Management, 48, 63–71. https://doi.org/10.1016/j.ijinfomgt.2019.01.
021.
Dwivedi, Y. K., Rana, N. P., Janssen, M., Lal, B., Williams, M. D., & Clement, R. M. (2017).
An empirical validation of a uniﬁed model of electronic government adoption.
Government Information Quarterly, 34(2), 211–230.
Dwivedi, Y. K., Rana, N. P., Jeyaraj, A., Clement, M., & Williams, M. D. (2019). Reexamining the uniﬁed theory of acceptance and use of technology: Towards a revised
theoretical model. Information Systems Frontiers, 21(3), 719–734.
Dwivedi, Y., Huges, L., Ismagilova, E., et al. (2020). Artiﬁcial intelligence:
Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for
research, practice and policy. in-press International Journal of Information
Management. https://doi.org/10.1016/j.ijinfomgt.2019.08.002.
Ettlinger, N. (2018). Algorithmic aﬀordances for productive resistance. Big Data & Society,
5(1), 1–13. https://doi.org/10.1177/2053951718771399.
Helberger, N., Karppinen, K., & D’Acunto, L. (2018). Exposure diversity as a design
principle for recommender systems. Information, Communication and Society, 21(2),

10

International Journal of Information Management 52 (2020) 102061

D. Shin, et al.

Sundar, S. S., Bellur, S., Oh, J., Xu, Q., & Jia, H. (2014). User experience of on-screen
interaction techniques: An experimental investigation of clicking, sliding, zooming,
hovering, dragging & ﬂipping. Human-Computer Interaction, 29(2), 109–152.
Tamilmani, K., Rana, N., Prakasam, N., & Dwivedi, Y. (2019). The battle of brain vs.
heart: A literature review and meta-analysis of hedonic motivation use in UTAUT2.
International Journal of Information Management, 46, 222–235. https://doi.org/10.
1016/j.ijinfomgt.2019.01.008.
Thurman, N., Moeller, J., Helberger, N., & Trilling, D. (2019). My friends, editors, algorithms, and I. Digital Journalism, 7(4), 447–460. https://doi.org/10.1080/21670811.
2018.1493936.
Wilson, M. (2017). Algorithms (and the) everyday. Information, Communication and
Society, 20(1), 137–150. https://doi.org/10.1080/1369118X.2016.1200645.
Wolker, A., & Powell, T. (2020). Algorithms in the newsroom? News readers’ perceived
credibility and selection of automated journalism. in-press Journalism. https://doi.
org/10.1177/1464884918757072.
Zhang, B., Wang, N., & Jin, H. (2014). Privacy concerns in online recommender systems.
Symposium on Usable Privacy and Security 2014.
Zheng, L., Yang, F., & Li, T. (2014). Modeling and broadening temporal user interest in
personalized news recommendation. Expert Systems with Applications, 47(7),
3168–3177. https://doi.org/10.1016/j.eswa.2013.11.020.
Zheng, Y., Zhong, B., & Yang, F. (2018). When algorithms meet journalism: The user
perception to automated news in a cross-cultural context. Computers in Human
Behavior, 86, 266–275. https://doi.org/10.1016/j.chb.2018.04.046.
Ziewitz, M. (2016). Governing algorithms myth, mess, and methods. Science, Technology &
Human Values, 41(1), 3–16.

(Ed.). The routledge companion to art and politicsNew York: Routledge. https://doi.org/
10.4324/9781315736693-26.
Shin, D. (2010). The eﬀects of trust, security and privacy in social networking: A securitybased approach to understand the pattern of adoption. Interacting With Computers,
22(5), 428–438. https://doi.org/10.1016/j.intcom.2010.05.001.
Shin, D. (2017). The role of aﬀordance in the experience of virtual reality learning:
Technological and aﬀective aﬀordances in virtual reality. Telematics and Informatics,
34(8), 1826–1836. https://doi.org/10.1016/j.tele.2017.05.013.
Shin, D. (2019). How do users experience the interaction with an immersive screen?
Computers in Human Behavior, 98, 302–310. https://doi.org/10.1016/j.chb.2018.11.
010.
Shin, D., & Biocca, F. (2018). Exploring immersive experience in journalism what makes
people empathize with and embody immersive journalism? New Media & Society,
20(8), 2800–2823. https://doi.org/10.1177/1461444817733133.
Shin, D., & Park, Y. (2019). Role of fairness, accountability, and transparency in algorithmic aﬀordance. Computers in Human Behavior, 98, 277–284. https://doi.org/10.
1016/j.chb.2019.04.019.
Shin, D., Fotiadis, A., & Yu, H. (2019). Prospectus and limitations of algorithmic governance: An ecological evaluation of algorithmic trends. Digital Policy Regulation and
Governance, 24(4), 369–383. https://doi.org/10.1108/DPRG-03-2019-0017.
Sloan, R. H., & Warner, R. (2017). When is an algorithm transparent? Predictive analytics,
privacy, and public policy. IEEE Security & Privacy(May/June 2018), https://doi.org/
10.2139/ssrn.3051588.
Soﬀer, O. (2019). Algorithmic personalization and the two-step ﬂow of communication.
Communication Theory. https://doi.org/10.1093/ct/qtz008.

11

