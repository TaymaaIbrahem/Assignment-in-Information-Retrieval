See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/346572791

Keep It Mystic? – The Effects of Algorithm Transparency on the Use of Advice
Conference Paper · December 2020

CITATIONS

READS

6

1,499

4 authors:
Cedric Lehmann

Christiane Haubitz

University of Cologne

University of Cologne

2 PUBLICATIONS 7 CITATIONS

5 PUBLICATIONS 15 CITATIONS

SEE PROFILE

SEE PROFILE

Andreas Fügener

Ulrich W. Thonemann

University of Cologne

University of Cologne

35 PUBLICATIONS 957 CITATIONS

133 PUBLICATIONS 2,690 CITATIONS

SEE PROFILE

Some of the authors of this publication are also working on these related projects:

Evidence-based Contingency Planning for Electronic Health Record Downtime View project

Supply Chain Segmentation View project

All content following this page was uploaded by Cedric Lehmann on 14 January 2021.
The user has requested enhancement of the downloaded file.

SEE PROFILE

Keep it mystic? – Effects of Algorithm Transparency

Keep It Mystic? – The Effects of Algorithm
Transparency on the Use of Advice
Short Paper

Cedric A. Lehmann, Christiane B. Haubitz,
Andreas Fügener, Ulrich W. Thonemann
Department of Supply Chain Management,
University of Cologne, Germany
cedric.lehmann@uni-koeln.de, christiane.haubitz@uni-koeln.de,
andreas.fuegener@uni-koeln.de, ulrich.thonemann@uni-koeln.de
Abstract
Algorithmic decision support is omnipresent for many managerial tasks where human
judgment makes the final call. However, the lack of transparency of algorithms is often
stated as a barrier of successful human-machine collaboration. In this paper, we analyze
the effects of algorithm transparency on the perceived value of algorithmic advice and its
resulting utilization for a simple, easy-to-understand algorithm.
In a laboratory experiment, participants received algorithmic advice for a forecasting
task. Only the treatment group was informed about the underlying principles of the
simple yet optimal advice-giving algorithm. While the explanation increased the
understanding of the algorithmic procedure, it reduced the perceived value of the
algorithmic advice, its utilization, and the participants’ performance.
Our results indicate that the effects of algorithm transparency on the use of algorithmic
advice are not straightforward, and that transparency might even be harmful. Going
forward, we plan to explore whether algorithm complexity moderates this effect.
Keywords: Algorithm Transparency, Decision Making, Use of Advice

Introduction
Over the last decades, advances in data availability and computational power have allowed for an increasing
use of algorithms in day-to-day decision-making. For meaningful managerial advice, we more and more
rely on complex black-box algorithms that reveal little about their underlying principles and often only
provide a final recommendation. In various decision-making domains, complex algorithms, such as
machine learning models, have been successfully employed to improve decision quality (Brynjolfsson and
McAfee 2014; Lecun et al. 2015). However, there is also the belief that the black-box nature of those
algorithms results in a lower acceptance of advice (Burton et al. 2020), which could be addressed by simpler
and easy-to-understand models (Gigerenzer and Gaissmaier 2011) that are made transparent.
These thoughts motivated a project with the service division of a large equipment manufacturer, where we
were asked to develop a spare parts optimization tool for trunk stock planning of service technicians. The
company preferred a transparent algorithm with easy-to-understand results over an alternative, more
complicated and thus non-transparent algorithm that maximizes performance. The implemented algorithm
is now used by planners in over 80 country organizations. The feedback indicates that the planners
appreciate the intuitive results of the algorithm. However, it remains open whether the transparency of the
easy-to-understand tool leads to a higher utilization of its advice compared to a less transparent option.
Forty-First International Conference on Information Systems, India 2020
1

Keep it mystic? – Effects of Algorithm Transparency

To explore this question, we asked the 450 undergraduates of an operations management class about their
attitude towards the use of algorithms for managerial decisions. The vast majority (94.2 %) indicated that
they would like to receive algorithmic advice when facing a managerial task. About equally as many (94.9
%) preferred a transparent algorithm and wanted to be informed about the algorithmic procedures.
Although the questions were of hypothetical nature, the favor of transparent algorithmic advice is evident.
The same group of undergraduates participated in an in-class experiment (preliminary study), which
mirrored the inventory management problem mentioned above. For 15 different spare parts sets they had
to decide which parts to stock, considering part demand rates, part weights, and the total weight limit – a
classical combinatorial optimization task, known as ‘knapsack-problem’ (Dantzig 1957). The problem can
be solved by a greedy algorithm, which sorts the spare parts by their value-to-weight ratio and selects the
part with the highest ratio, as long as the capacity is not exceeded. Although this procedure does not
necessarily solve the problem optimally, it usually produces good results and is perceived as intuitive
(Diubin and Korbut 2008). In our experiment, the greedy algorithm’s solutions were optimal. We divided
the students in two groups. Both groups received the same algorithmic advice, but one group received an
explanation of the greedy algorithm while the other did not. The group that received the explanation stated
significantly lower levels of perceived reliability and technical competence of the algorithm. Moreover, the
share of participants who followed the algorithmic advice was smaller compared to the group without
explanation (Figure 1).

Figure 1 Share of participants that followed the algorithmic advice by round (randomized
sequence of spare parts sets). Error bars indicate standard errors.
To increase explainability and understandability, it often seems natural to use simple, easy-to-understand
algorithms rather than more complex ones. Yet, we did not find evidence that making simple algorithms
more transparent indeed leads to a higher use of algorithmic advice, but our preliminary study shows
contrary effects. In this paper, we therefore focus on the effects of transparency of such simple algorithms
in managerial decision tasks. We hypothesize that even though providing transparency increases
understanding of the algorithmic principles, this higher understanding does not lead to a higher perceived
value of algorithmic advice and, as a result, neither to higher utilization of advice. To test our hypotheses,
we conduct a laboratory experiment (main study), where we manipulate the level of transparency by only
informing the treatment group about the underlying principles of a simple yet optimal advice-giving
algorithm. The results indicate that the effects of transparency on utilization of algorithmic advice are
anything but trivial. We believe that in this context, the level of algorithm complexity and the resulting
satisfaction of the user’s expectations play an important role. In follow-up studies, we therefore plan to vary
the level of algorithm complexity. To contribute to the research streams of human-machine interaction and
advice taking, we aim to gain a better understanding of when algorithm transparency is beneficial, and
when it is best to “keep it mystic”.

Literature Review
People who face decisions like to receive advice to improve the decision quality and to share the
responsibility for possible negative consequences (Harvey and Fischer 1997; Yaniv 2004a, 2004b).
Research on advice-taking and decision-making is broad and we refer to Bonaccio and Dalal (2006) for a
comprehensive review. In the following, we focus on drivers of advice utilization, particularly on the role of
transparency of advice.
Decision makers tend not to follow advice as much as would be in their own interest and put too much
weight on their own judgment (Dietvorst et al. 2015; Harvey and Fischer 1997; Logg et al. 2019; Yaniv
2004a, 2004b). For example, Harvey and Fischer (1997) observe a weight of only 20% to 30% on human
Forty-First International Conference on Information Systems, India 2020
2

Keep it mystic? – Effects of Algorithm Transparency

advice, even if the advisor has more expertise than the decision maker. Logg et al. (2019) observe a weight
of 34% to 52% on algorithmic advice.
Yu et al. (2019) suggest that people adjust the utilization of algorithmic advice according to its perceived
performance. Even without revealed performance measures, humans are capable of detecting the quality of
algorithmic advice through outcomes and adapt their utilization accordingly (De Baets and Harvey 2020).
Dietvorst et al. (2015) observe that people lose confidence in algorithms over time when they see algorithms
err. However, if similar advice is provided by a human, confidence in their advice does not fade as quickly.
They refer to this phenomenon as “algorithm aversion”.
The level of transparency moderates the extent to which the underlying principles and the complexity of an
algorithm are visible to the decision maker. A high level of transparency typically reveals the data used and
explains the procedures that turn data into advice (Bertino et al. 2019). While people are not generally
averse to using ‘black-box’ algorithms (Dietvorst et al. 2018; Logg et al. 2019), Yaniv (2004a, 2004b) argue
that advice is often underutilized for this very reason. Decision makers typically know how they derive their
own judgment, but the rationale behind advice often remains unclear. Consequently, decision makers put
too much weight on their own judgments.
The literature discusses different theories about the effects of algorithm transparency on the use of
algorithmic advice. Most work in this area focuses on subjective tasks that involve personal taste. In an early
study, Sinha and Swearingen (2002) conclude that people appreciate music recommender systems that
they perceive as transparent and show higher confidence in their advice. Similarly, Wang and Benbasat
(2007) show that explaining “how” an e-commerce recommender system derived a product
recommendation increased the users’ trust in the technical competence of the recommender agent. Cramer
et al. (2008) analyze the effects of transparency of an art recommender. While the explanation of “why”
certain artwork is recommended increased the acceptance of the recommendations, it did not affect the
general trust in the recommender system. Kizilcec (2016) examines the effects of transparency of a grading
algorithm. He finds that students with violated grade expectations had lower trust in the algorithm than
students whose expectations were met. Springer and Whittaker (2018) analyze the effects of transparency
of an algorithm that predicts the users’ mood based on a short self-written text about an emotional
experience in the past. Increased transparency led to a reduced perceived accuracy, even when expectations
were met.
The results of these studies point into different directions. While earlier work indicates that transparency
has a positive effect on trust (Wang and Benbasat 2007), appreciation, and confidence (Sinha and
Swearingen 2002), later studies reveal mixed effects (Cramer et al. 2008; Kizilcec 2016) and negative effects
(Springer and Whittaker 2018) on trust or perceived accuracy. The studies were conducted with subjective
tasks, such as product recommendations or grading. For objective tasks, effects of algorithm transparency
have received little attention. Moreover, most studies measure the perception of algorithmic advice with
indicators like trust, perceived accuracy, or algorithm appreciation, without linking it to actual advice
utilization. This paper addresses these gaps and analyzes the effect of algorithm transparency on perceived
value and use of algorithmic advice for simple algorithms and objective managerial task.

Hypotheses Development
Algorithm transparency is increased by disclosing information. The effects of information disclosure
depend on prior expectations and beliefs (Burton et al. 2020; Kizilcec 2016; Springer and Whittaker 2019).
Therefore, we build on findings on humans’ expectations of algorithms to hypothesize effects of
transparency.
Although algorithms outperform human judgment in many areas (Dawes 1979; Dawes et al. 1989; Meehl
1954), they tend to fall short of humans’ expectations (Burton et al. 2020; Dawes 1979; Dietvorst et al. 2015,
2018; Highhouse 2008). People are generally willing to work with algorithms without having much
information on their underlying principles. Dietvorst et al. (2015) perform several studies in which
participants could choose to either work with an algorithm or a human. The participants who did not know
about the algorithmic procedure or performance preferred to work with the algorithm. Logg et al. (2019)
support this observation and find that people are willing to work with such ‘black-box’ algorithms. This led
them to investigate how humans define algorithms. They analyze the definitions of 226 respondents using
thematic coding. 42% of the respondents define an algorithm as a set of mathematical equations and 26%
Forty-First International Conference on Information Systems, India 2020
3

Keep it mystic? – Effects of Algorithm Transparency

think of it as a step-by-step procedure. Only 14% recognize that an algorithm can also be simple and consist
of a single formula. It is well known that – depending on the task – even simple algorithms are able to
provide good advice and to outperform human judgment (Fischer and Harvey 1999; Schweitzer and Cachon
2000). Considering the survey results of Logg et al. (2019), the simple nature of their procedures might,
however, fall short of human expectations. As a result, transparency of the underlying processes of a simple
algorithm through explanation might lead to disappointment, reduce the perceived value of its advice, and
ultimately decrease advice utilization.
We formalize our hypotheses for a setting with a simple algorithm that is well suited to provide statistically
meaningful advice for a prediction under uncertainty. Based on the findings in the literature, we
hypothesize that although the algorithm fits perfectly to the task, people are underwhelmed by its simplicity.
Therefore, we formulate the following:
Hypothesis 1. Increasing transparency of a simple algorithm through explanation leads to a lower
perceived value of its advice compared to not explaining it.
In the literature, it is common to measure the decision maker’s attitude towards algorithmic advice through
approximations like perceived trust or value (e.g., Cramer et al. 2008; Sinha and Swearingen 2002; Wang
and Benbasat 2007). However, the positive correlation between the perceived value of advice and its
utilization has yet to be shown:
Hypothesis 2. A higher perceived value of advice increases the utilization of advice.
Assuming that Hypotheses 1 and 2 hold, we expect that for a simple algorithm, increased transparency
through explanation has a negative effect on the utilization of advice and that this direct effect is mediated
by the perceived value of advice:
Hypothesis 3. Increasing transparency of a simple algorithm through explanation leads to lower
utilization of its advice, mediated by the perceived value of advice.
We test our hypotheses for a critical managerial task, that is, demand forecasting.

Effect of Transparency on the Use of Algorithmic Advice
Demand forecasting is a critical task for organizations and statistical forecasting methods are constantly
improved (Fildes 2006). Many organizations allow human planners to adapt the statistical forecasts (Fildes
and Goodwin 2007); these manual adaptions are sometimes beneficial, but can also increase the ex-post
forecast error (Fildes et al. 2009). Forecasting is a common task in experimental studies (Bendoly et al.
2010; Donohue et al. 2019; Gino and Pisano 2008) and has been used in research that analyzes the general
human attitude towards algorithms (Dietvorst et al. 2015, 2018; Logg et al. 2019).
We therefore use a forecasting task and design an experiment with a Judge-Advisor System structure. At
first, the decision maker (the “judge”) makes an initial demand forecast after which they receive advice.
Then, the decision maker can update the initial forecast and provide a final forecast. Judge-Advisor Systems
are often used in the literature to investigate advice taking and decision-making. The judgment is measured
before and after receiving the advice. The relative shift of judgment indicates the utilization of advice
(Bonaccio and Dalal 2006).
To analyze the effects of algorithm transparency and to test our hypotheses, we evaluate the following
performance measures: understanding of algorithmic principles, perceived value of algorithmic advice,
and utilization of algorithmic advice. In the following, we outline how we assess the performance measures.

Experimental Design
We designed an experiment in which participants had to forecast the demand sequentially for 10 different
products, before and after receiving advice from an algorithm. We used two conditions, a transparent
condition and a non-transparent condition, which differed only in the manipulation of transparency
through an explanation of the algorithmic procedures.
After reading the instructions and answering the comprehension questions, the participants had to make
their initial forecast for the 10 products. For each product, they observed the demand history of 10 periods

Forty-First International Conference on Information Systems, India 2020
4

Keep it mystic? – Effects of Algorithm Transparency

and had to forecast the demand of period 11 by entering a number in a text field. We randomly simulated
the stationary demand data for each product with a mean value between 300 and 1200 and a coefficient of
variation of CV = 0.3. The demand history was displayed in a line chart. The graphical display is not only
common in practice, but is also seen as the best presentation style when the characteristics of the demand
data are unknown (Harvey and Bolger 1996).
After the participants had forecasted the demand for the 10 products, they were informed that an algorithm
had also computed a forecast. The algorithm used the average demand of the 10 periods as forecast for
period 11. This algorithmic forecast is an unbiased and ex-ante error minimizing estimation of the stationary
demand that we used. Participants in the non-transparent condition received no additional information,
while participants in the transparent condition received an explanation of the algorithmic process. They
were informed that the algorithm “calculates the forecast for a product by computing the average of the
demand history of the last 10 periods”. For an example product, we showed the computation of the average
with the corresponding formula. Participants in both conditions had to indicate their perceived value of the
algorithmic advice. On a Likert-type scale from 1 (strongly disagree) to 7 (strongly agree), they had to state
their consensus with the statements “I think the recommendations of the algorithm will be valuable”.
After the participants had indicated their perceived value of advice, they could update their initial forecast
for all products. Participants in both conditions received the algorithmic advice, but only in the transparent
condition we additionally revealed the formula used for the advice computation. After making their final
forecasts for all products, the participants were asked to indicate on a 7-point Likert-type scale how well
they “[…] understood how the algorithm derived its recommendations”.
The participants saw the actual demand and their resulting forecast error only at the end of the study. We
monetarily incentivized a high forecast accuracy. The payoff composition is similar to Kremer et al. (2011).
It consists of a fixed reward of $0.25 and a bonus of up to $0.10 per product, depending on the final forecast
accuracy, measured by absolute percentage error APE = |final forecast − actual demand|⁄actual demand.
The bonus for each product is $0.10 ∗ (1 − APE).
We measure the utilization of the algorithmic advice through the weight on advice (WOA) (e.g., Bonaccio
and Dalal 2006):
WOA =

Initial Forecast − Final Forecast
Initial Forecast − Algorithmic Advice

The WOA computes the relative shift between the initial forecast and the final forecast with respect to the
algorithmic advice. A WOA of 0 implies that the participant did not change their initial forecast while a
value of 1 implies that the initial forecast was replaced by the algorithmic advice. If the initial forecast equals
the algorithmic advice, we set WOA = 0. WOA values less than 0 or greater than 1 are winsorized to increase
interpretability (see e.g., Logg et al. 2019).

Experimental Protocol
The experiment was programmed in oTree (Chen et al. 2016) and conducted on Amazon’s Mechanical Turk
(MTurk) on March 3, 2020. We sought to collect data from 190 participants to detect a medium-sized effect
(Cohen's (1988) d=0.5) based on a two-sample t-test at 80% power and 1% significance level. 442 MTurk
workers accepted to work on the experiment. After eliminating participants that did not finish the
experiment or failed an attention check, 185 were randomly assigned to one of the two conditions. 92
participants were assigned to the non-transparent condition and 93 participants to the transparent
condition. On average, the participants needed 9.2 minutes to conduct the experiment and earned $1.11.

Results
To test whether the transparency manipulation was successful, we analyze the level of understanding, which
was self-reported at the end of the experiment. For participants in the non-transparent condition we expect
a lower level of understanding since they received no explicit information on the algorithmic process. On a
7-point Likert-type scale, the average level of understanding was 6.26 in the transparent condition and 3.87
in the non-transparent condition. From a Mann-Whitney U test, we can conclude that the transparency
manipulation was successful (Table 1-a).

Forty-First International Conference on Information Systems, India 2020
5

Keep it mystic? – Effects of Algorithm Transparency

Transparent
Question
I understand how the algorithm derives
(a)
its recommendation.
I think the recommendations of the
(b)
algorithm will be valuable.

Non-Transparent

Mean

SD

Mean

SD

Mann-Whitney-U
p (two-tailed)

6.26

1.05

3.87

1.61

<0.001

4.86

1.37

5.70

0.86

<0.001

Table 1 Summary of the answers given in the transparent and non-transparent condition.
Participants in the transparent condition reported a significantly lower consensus with the statement “I
think the recommendations of the algorithm will be valuable”. Thus, they reveal a lower perceived value of
advice (Table 1-b), which supports Hypothesis 1. Note that participants in the non-transparent condition
can only answer this question based on their general expectation of advice-giving algorithms and their
perception of the forecasting task.
We run an ordinary least squares (OLS) regression to test the effect of the perceived value of advice on the
utilization of advice. We use the indicated perceived value of advice as independent variable and the
recorded WOA as dependent variable. The results reveal that the perceived value of advice has a positive
effect on the utilization of advice (β = 0.11, p < 0.001), which supports Hypothesis 2.
We further hypothesized that transparency has a direct negative effect on advice utilization, mediated by
the perceived value of advice (Hypothesis 3). We use the Baron and Kenny procedure (Baron and Kenny
1986), which is often used to test for simple and consistent mediations (e.g., Malhotra et al. 2014). In this
approach, mediation is tested with three regressions: 1) the effect of the independent variable
(Transparency) on the dependent variable (Weight on Advice), referred to as total effect, 2) the effect of the
independent variable on the mediator (Perceived Value of Advice), and 3) the effect of the independent
variable (the direct effect) and mediator on the dependent variable. The effects of the first and second
regression should be significant. A mediation is present if the direct effect of the independent variable on
the dependent variable is smaller than the total effect, which is caused by the high explanatory power of the
mediator.

Figure 2 Mediation model with effect of transparency on perceived value of advice (2), effect
of perceived value of advice on weight on advice (3), total effect (4) and direct effect (4’) of
transparency on weight on advice.
Note. *p<0.1; **p<0.01; ***p<0.001 (two-tailed)
As depicted in Figure 2, the mediation analysis reveals a significant total effect c of transparency on the
utilization of advice. Transparency is negatively correlated with the perceived value of advice effect (a),
which provides support for our first hypothesis. To estimate the weight on advice, we use both transparency
and the perceived value of advice as independent variables. We find that the perceived value of advice has
a significant positive effect b, while transparency does not have a significant effect c′. Since the total effect
c is significant and the direct effect c . is not, the weight on advice is completely mediated by the perceived
value of advice. Bootstrapping (Bollen and Stine 1990; Preacher and Hayes 2008) with 5,000 iterations
provides a confidence interval on the indirect effect (transparency mediated by perceived value of advice)
of ab = 0.08 with a 95% confidence interval of CI = ±0.04. The mediation analysis thus supports our
hypothesis that the lower utilization of advice of the transparent algorithm can be explained by its lower
perceived value compared to the non-transparent algorithm.
The explanation of the algorithmic procedure caused participants to shift their final forecast 13.8 percentage
points less towards the algorithmic advice (Figure 3-a). Moreover, we measured the forecast accuracy with
the mean absolute error (MAE), which describes the absolute deviation of the final forecast from the mean
Forty-First International Conference on Information Systems, India 2020
6

Keep it mystic? – Effects of Algorithm Transparency

Figure 3 Comparison of average Weight on Advice (a) and Mean Absolute Error (b) for the
transparent and non-transparent condition. Error bars indicate standard errors.
demand, averaged over all products. In the transparent condition, the MAE was 37.2% higher than in the
non-transparent condition (p < 0.001) as presented in Figure 3-b. In summary, compared to the nontransparent condition, participants that received an explanation of the algorithm had a lower perceived
value of its recommendation, a lower weight on advice and, finally, a lower performance.

Discussion & Future Work
With our research, we analyze how algorithm transparency influences the utilization of algorithmic advice
in managerial decision tasks. Our laboratory experiment reveals that the utilization of algorithmic advice
does not always benefit from increased transparency of the algorithm. For a simple, yet error-minimizing
and therefore ex-ante optimal algorithm, we observed a lower utilization of algorithmic advice with
increased transparency. We used a mediation model to show that the effect of transparency on the use of
advice is mediated by the perceived value of advice. We believe that the reduced perceived value of advice
results from a disappointment in the algorithm (Logg et al. 2019) and that the participants were
underwhelmed by its simple and straightforward methods, despite their appropriateness for the task. To
increase the generalizability of our findings while maintaining external validity, we tested our hypotheses
with non-specialized subjects in our preliminary and main study. Previous research has shown that
undergraduate students (Bolton et al. 2012; Kremer et al. 2016) and MTurk Workers (Buhrmester et al.
2011; Lee et al. 2018) are a suitable subject pool for managerial decisions like the ones we analyze.
Next, we plan follow-up studies to better understand the effect of algorithmic complexity on the perceived
value of the advice of a transparent algorithm and its resulting utilization. We believe that a more complex
algorithm is more likely to meet or even exceed the participants’ expectations. We therefore expect that a
higher complexity will result in a higher perceived value of the transparent algorithm, which in turn will
increase its utilization.
Our research provides important implications on the interaction between humans and algorithms for
academia and practice. We show that revealing the underlying principles of an algorithm can actually do
more damage than good. Therefore, practitioners should carefully analyze and ponder the potential effects
of making advice-giving algorithms more transparent. Particularly if the algorithms apply simple, easy-tounderstand methods, efforts on making algorithmic decisions better understandable can backfire and hurt
the utilization of advice. More research is needed to derive the conditions under which explanations of
algorithms should be provided. Going forward, we want to shed light on the influence of complexity, but
there are many more interesting areas for future research. For example, it would be insightful to analyze
how other dimensions such as ethical standards and fairness influence the use of algorithmic advice if the
underlying principles of the algorithms are revealed. Moreover, our work is oriented towards deriving
recommendations for algorithm transparency that hold for a broad range of human decision makers. This
is useful to establish general routines and if individual treatment of decision makers is not desirable or
possible. It would, however, be insightful to analyze how algorithm transparency influences the use of
advice for different types of users. In this context, response modeling and particularly uplift modeling
(Devriendt et al. 2018; Gubela et al. 2019) offer interesting opportunities. If the decision to make an
algorithm transparent can be made on an individual level, those techniques could help to determine to
which decision makers the algorithmic procedures should be revealed and for whom it is simply better to
“keep it mystic”.
Forty-First International Conference on Information Systems, India 2020
7

Keep it mystic? – Effects of Algorithm Transparency

References
De Baets, S., and Harvey, N. 2020. “Using Judgment to Select and Adjust Forecasts from Statistical
Models,” European Journal of Operational Research (284:3), pp. 882–895.
Baron, R. M., and Kenny, D. A. 1986. “The Moderator-Mediator Variable Distinction in Social Psychological
Research: Conceptual, Strategic, and Statistical Considerations,” Journal of Personality and Social
Psychology (51:6), pp. 1173–1182.
Bendoly, E., Croson, R., Goncalves, P., and Schultz, K. 2010. “Bodies of Knowledge for Research in
Behavioral Operations,” Production and Operations Management (19:4), pp. 434–452.
Bertino, E., Merrill, S., Nesen, A., and Utz, C. 2019. “Redefining Data Transparency: A Multidimensional
Approach,” Computer (52:1), pp. 16–26.
Bollen, K. A., and Stine, R. 1990. “Direct and Indirect Effects: Classical and Bootstrap Estimates of
Variability,” Sociological Methodology (20), pp. 115–140.
Bolton, G. E., Ockenfels, A., and Thonemann, U. W. 2012. “Managers and Students as Newsvendors,”
Management Science (58:12), pp. 2225–2233.
Bonaccio, S., and Dalal, R. S. 2006. “Advice Taking and Decision-Making: An Integrative Literature Review,
and Implications for the Organizational Sciences,” Organizational Behavior and Human Decision
Processes (101:2), pp. 127–151.
Brynjolfsson, E., and McAfee, A. 2014. The Second Machine Age: Work, Progress, and Prosperity in a Time
of Brilliant Technologies., New York, NY, US: WW Norton & Company.
Buhrmester, M., Kwang, T., and Gosling, S. D. 2011. “Amazon’s Mechanical Turk: A New Source of
Inexpensive, Yet High-Quality, Data?,” Perspectives on Psychological Science (6:1), pp. 3–5.
Burton, J. W., Stein, M.-K., and Jensen, T. B. 2020. “A Systematic Review of Algorithm Aversion in
Augmented Decision Making,” Journal of Behavioral Decision Making (33:2), pp. 220–239.
Chen, D. L., Schonger, M., and Wickens, C. 2016. “OTree-An Open-Source Platform for Laboratory, Online,
and Field Experiments,” Journal of Behavioral and Experimental Finance (9), pp. 88–97.
Cohen, J. 1988. “Statistical Power Analysis for the Behavioral Sciences,” Statistical Power Analysis for the
Behavioral Sciences, Hillsdale, NJ: L. Erlbaum Associates.
Cramer, H., Evers, V., Ramlal, S., Van Someren, M., Rutledge, L., Stash, N., Aroyo, L., and Wielinga, B.
2008. “The Effects of Transparency on Trust in and Acceptance of a Content-Based Art Recommender,”
User Modeling and User-Adapted Interaction (18:5), pp. 455–496.
Dantzig, G. B. 1957. “Discrete-Variable Extremum Problems,” Operations Research (5:2), INFORMS, pp.
266–288.
Dawes, R. M. 1979. “The Robust Beauty of Improper Linear Models in Decision Making,” American
Psychologist (34:7), pp. 571–582.
Dawes, R. M., Faust, D., and Meehl, P. E. 1989. “Clinical versus Actuarial Judgment,” Science (243:4899),
pp. 1668–1674.
Devriendt, F., Moldovan, D., and Verbeke, W. 2018. “A Literature Survey and Experimental Evaluation of
the State-of-the-Art in Uplift Modeling: A Stepping Stone Toward the Development of Prescriptive
Analytics,” Big Data (6:1), pp. 13–41.
Dietvorst, B. J., Simmons, J. P., and Massey, C. 2015. “Algorithm Aversion: People Erroneously Avoid
Algorithms after Seeing Them Err,” Journal of Experimental Psychology: General (144:1), pp. 114–
126.
Dietvorst, B. J., Simmons, J. P., and Massey, C. 2018. “Overcoming Algorithm Aversion: People Will Use
Imperfect Algorithms If They Can (Even Slightly) Modify Them,” Management Science (64:3), pp.
1155–1170.
Diubin, G. N., and Korbut, A. A. 2008. “Average Behavior of Greedy Algorithms for the Minimization
Knapsack Problem: General Coefficient Distributions,” Computational Mathematics and
Mathematical Physics (48:9), pp. 1521–1535.
Donohue, K., Katok, E., and Leider, S. 2019. “The Handbook of Behavioral Operations,” The Handbook of
Behavioral Operations, Hoboken, NJ, US: John Wiley & Sons.
Fildes, R. 2006. “The Forecasting Journals and Their Contribution to Forecasting Research: Citation
Analysis and Expert Opinion,” International Journal of Forecasting (22:3), pp. 415–432.
Fildes, R., and Goodwin, P. 2007. “Against Your Better Judgment? How Organizations Can Improve Their
Use of Management Judgment in Forecasting,” Interfaces (37:6), pp. 570–576.

Forty-First International Conference on Information Systems, India 2020
8

Keep it mystic? – Effects of Algorithm Transparency

Fildes, R., Goodwin, P., Lawrence, M., and Nikolopoulos, K. 2009. “Effective Forecasting and Judgmental
Adjustments: An Empirical Evaluation and Strategies for Improvement in Supply-Chain Planning,”
International Journal of Forecasting (25:1), pp. 3–23.
Fischer, I., and Harvey, N. 1999. “Combining Forecasts: What Information Do Judges Need to Outperform
the Simple Average?,” International Journal of Forecasting (15:3), pp. 227–246.
Gigerenzer, G., and Gaissmaier, W. 2011. “Heuristic Decision Making,” Annual Review of Psychology
(62:1), pp. 451–482.
Gino, F., and Pisano, G. 2008. “Toward a Theory of Behavioral Operations,” Manufacturing and Service
Operations Management (10:4), pp. 676–691.
Gubela, R., Bequé, A., Lessmann, S., and Gebert, F. 2019. “Conversion Uplift in E-Commerce: A Systematic
Benchmark of Modeling Strategies,” International Journal of Information Technology & Decision
Making (18:3), pp. 747–791.
Harvey, N., and Bolger, F. 1996. “Graphs versus Tables: Effects of Data Presentation Format on
Judgemental Forecasting,” International Journal of Forecasting (12:1), pp. 119–137.
Harvey, N., and Fischer, I. 1997. “Taking Advice: Accepting Help, Improving Judgment, and Sharing
Responsibility,” Organizational Behavior and Human Decision Processes (70:2), pp. 117–133.
Highhouse, S. 2008. “Stubborn Reliance on Intuition and Subjectivity in Employee Selection,” Industrial
and Organizational Psychology (1:3), pp. 333–342.
Kizilcec, R. F. 2016. “How Much Information? Effects of Transparency on Trust in an Algorithmic
Interface,” Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, pp.
2390–2395.
Kremer, M., Moritz, B., and Siemsen, E. 2011. “Demand Forecasting Behavior: System Neglect and Change
Detection,” Management Science (57:10), pp. 1827–1843.
Kremer, M., Siemsen, E., and Thomas, D. J. 2016. “The Sum and Its Parts: Judgmental Hierarchical
Forecasting,” Management Science (62:9), pp. 2745–2764.
Lecun, Y., Bengio, Y., and Hinton, G. 2015. “Deep Learning,” Nature (521:7553), pp. 436–444.
Lee, Y. S., Seo, Y. W., and Siemsen, E. 2018. “Running Behavioral Operations Experiments Using Amazon’s
Mechanical Turk,” Production and Operations Management (27:5), pp. 973–989.
Logg, J. M., Minson, J. A., and Moore, D. A. 2019. “Algorithm Appreciation: People Prefer Algorithmic to
Human Judgment,” Organizational Behavior and Human Decision Processes (151), pp. 90–103.
Malhotra, M. K., Singhal, C., Shang, G., and Ployhart, R. E. 2014. “A Critical Evaluation of Alternative
Methods and Paradigms for Conducting Mediation Analysis in Operations Management Research,”
Journal of Operations Management (32:4), pp. 127–137.
Meehl, P. E. 1954. “Clinical versus Statistical Prediction: A Theoretical Analysis and a Review of the
Evidence,” University of Minnesota Press.
Preacher, K. J., and Hayes, A. F. 2008. “Asymptotic and Resampling Strategies for Assessing and
Comparing Indirect Effects in Multiple Mediator Models,” Behavior Research Methods (40:3), pp.
879–891.
Schweitzer, M. E., and Cachon, G. P. 2000. “Decision Bias in the Newsvendor Problem with a Known
Demand Distribution: Experimental Evidence,” Management Science (46:3), pp. 404–420.
Sinha, R., and Swearingen, K. 2002. “The Role of Transparency in Recommender Systems,” CHI ’02
Extended Abstracts on Human Factors in Computing Systems, pp. 830–831.
Springer, A., and Whittaker, S. 2018. “What Are You Hiding? Algorithmic Transparency and User
Perceptions,” In 2018 AAAI Spring Symposium Series, pp. 455–459.
Springer, A., and Whittaker, S. 2019. “Making Transparency Clear,” Joint Proceedings of the ACM IUI 2019
Workshops.
Wang, W., and Benbasat, I. 2007. “Recommendation Agents for Electronic Commerce: Effects of
Explanation Facilities on Trusting Beliefs,” Journal of Management Information Systems (23:4), pp.
217–246.
Yaniv, I. 2004a. “The Benefit of Additional Opinions,” Current Directions in Psychological Science (13:2),
pp. 75–78.
Yaniv, I. 2004b. “Receiving Other People’s Advice: Influence and Benefit,” Organizational Behavior and
Human Decision Processes (93:1), pp. 1–13.
Yu, K., Berkovsky, S., Taib, R., Zhou, J., and Chen, F. 2019. “Do I Trust My Machine Teammate? An
Investigation from Perception to Decision,” IUI ’19: Proceedings of the 24th International Conference
on Intelligent User Interfaces, pp. 460–468.

Forty-First International Conference on Information Systems, India 2020
9

View publication stats

