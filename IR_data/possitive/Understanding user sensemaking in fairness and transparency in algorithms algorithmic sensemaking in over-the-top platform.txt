See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/361728721

Understanding user sensemaking in fairness and transparency in algorithms:
algorithmic sensemaking in over-the-top platform
Article in AI & SOCIETY · July 2022
DOI: 10.1007/s00146-022-01525-9

CITATIONS

READS

5

142

4 authors:
Donghee Shin

Joon Soo Lim

Zayed University

Syracuse University

339 PUBLICATIONS 10,474 CITATIONS

38 PUBLICATIONS 1,097 CITATIONS

SEE PROFILE

SEE PROFILE

Norita Ahmad

Mohammed Ibahrine

American University of Sharjah

American University of Sharjah

55 PUBLICATIONS 1,258 CITATIONS

56 PUBLICATIONS 212 CITATIONS

SEE PROFILE

Some of the authors of this publication are also working on these related projects:

Performance Measurement View project

Third-person effect; weight loss advertising; social norms View project

All content following this page was uploaded by Donghee Shin on 26 July 2022.

The user has requested enhancement of the downloaded file.

SEE PROFILE

AI & SOCIETY
https://doi.org/10.1007/s00146-022-01525-9

ORIGINAL ARTICLE

Understanding user sensemaking in fairness and transparency
in algorithms: algorithmic sensemaking in over‑the‑top platform
Donghee Shin1

· Joon Soo Lim2 · Norita Ahmad3,4 · Mohammed Ibahrine4

Received: 9 April 2022 / Accepted: 6 June 2022
© The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2022

Abstract
A number of artificial intelligence (AI) systems have been proposed to assist users in identifying the issues of algorithmic
fairness and transparency. These AI systems use diverse bias detection methods from various perspectives, including exploratory cues, interpretable tools, and revealing algorithms. This study explains the design of AI systems by probing how users
make sense of fairness and transparency as they are hypothetical in nature, with no specific ways for evaluation. Focusing
on individual perceptions of fairness and transparency, this study examines the roles of normative values in over-the-top
(OTT) platforms by empirically testing their effects on sensemaking processes. A mixed-method design incorporating both
qualitative and quantitative approaches was used to discover user heuristics and to test the effects of such normative values
on user acceptance. Collectively, a composite concept of transparent fairness emerged around user sensemaking processes
and its formative roles regarding their underlying relations to perceived quality and credibility. From a sensemaking perspective, this study discusses the implications of transparent fairness in algorithmic media platforms by clarifying how and what
should be done to make algorithmic media more trustable and reliable platforms. Based on the findings, a theoretical model
is developed to define transparent fairness as an essential algorithmic attribute in the context of OTT platforms.
Keywords Algorithmic normative values · Transparent fairness · OTT platforms · Algorithmic sensemaking · Algorithmic
credibility · Algorithmic information processing

1 Introduction
Artificial intelligence (AI) is incessantly affecting the everyday life of billions of media users (Park and Jones-Jang
2022). Algorithms are popular and effective in practice,
but their popularity comes at the expense of systematic
* Donghee Shin
donghee.shin@zu.ac.ae
Joon Soo Lim
jlim01@syr.edu
Norita Ahmad
nahmad@aus.edu
1

College of Communication and Media Sciences, Zayed
University, Dubai, United Arab Emirates

2

Newhouse School Of Public Communications, Syracuse
University, Syracuse, USA

3

Center for Innovation in Teaching and Learning, Sharjah,
United Arab Emirates

4

American University of Sharjah, Sharjah,
United Arab Emirates

discrimination, limited transparency, and vague accountability (Moller et al. 2018). Algorithmic filtering procedures may lead to more impartial, and thus possibly fairer
processes than those processed by humans (Park 2021).
However, algorithmic recommendation processes have been
criticized for their tendency to intensify/reproduce bias, distortion of facts, information asymmetry, and process opacity
(Ananny and Crawford 2018). Algorithmic bias may deteriorate algorithmic injustice that machine learning automates
and perpetuates unjust and discriminatory patterns (Hoffmann 2019).
Recent over-the-top (OTT) platforms have faced similar
problems (Shin 2020). While OTT offers personalized and
relevant content in innovative interactive manners, the ethical and privacy issues are complicatedly intertwined with
algorithmic personalization (Araujo 2018). Issues regarding how to safeguard the goals, values, and personalizing
processes of OTT, to what extent users need to share personal information with algorithms, and how to balance privacy and algorithmic personalization remain controversial
(Crain 2018). Underlying these questions are concerns about

13

Vol.:(0123456789)

AI & SOCIETY

how to mitigate bias and discrimination in data, as well as
urgent tasks on how to design algorithmic systems transparent and fair (Hoffman 2019). As ethical concerns have
peaked recently with the rise of OTT algorithms, the opacity
of black-box algorithm processes has led to calls for studies
on fairness and transparency (Dörr and Hollnbuchner 2017;
Meijer 2014).
Recent research (e.g., Shin and Park 2019; Sandvig et al.
2016) highlighted normative implications and problems of
these algorithms, summarized by fairness, transparency, and
responsibility (FTR). Transparency and fairness particularly
emerge as key attributes for trustable algorithmic systems
while processing user-sensitive data (Helberger et al. 2018).
This issue will be even more critical when OTT relies more
and more on algorithms and users rely more on algorithms
than social influence when making judgments. AI becomes
pervasive across all media industries and service functions.
The key question arises on how to govern these OTT algorithms effectively and legitimately while ensuring that they
are user-centered and socially responsible. Despite the significance, few studies have researched the normative values
in OTT contexts to date. Questions such as, how users make
sense of fairness in their algorithm use, how users construct
algorithmic transparency, and how they perceive algorithm-based processes, will be vital issues to consider as AI
becomes even more prominent in media platforms (Graefe
et al. 2018). As these normative concerns have given rise
to calls for a better explanatory framework that effectively
addresses them (Thurman et al. 2019), we empirically examine these concerns from a user perspective: how fairness and
transparency influence the form and content of sensemaking
processes, with a focus on how users formulate credibility
through quality evaluation. User experiences may be based
on subjective standards of fairness and transparency, generating tension between uniform solutions deemed conforming
to FTR requirements and potentially diverging from user
experiences. Yet, there is little understanding of how FTR
has been conceptualized and how users experience FTR.
There is thus a gap between transparency in practice and the
goal of fairness, since FTR predominantly serves internal
stakeholders reflecting algorithm providers’ interests rather
than users’ interests and benefits. For example, numerous
explainable AI cues (such as counterfactual explanations,
feature importance scores, or source information checking)
are not for users affected by the algorithms but rather for AI
programmers, who use explainability to debug the algorithm
itself (Rosenfeld and Richardson 2019). In this research, we
investigate the users’ perspectives in the context of OTT
platforms offered directly to users and recommend use cases,
specifically how algorithm users engaged in sensemaking of
fairness and transparency on the OTT platforms. In using
the concept of sensemaking, we focus on how users’ roles
in algorithmic personalization influence their decisions to

13

accept the OTT recommendation in particular ways, with
differing outcomes for their own assessment. Our main
objective is to contribute to theorizing and operationalizing OTT platforms that are fairer and more transparent. To
this end, we aim to develop understandings of fairness and
transparency, leading to operational, user-centric definitions
for different areas of media platforms with implications for
both design/developments and sociological/ethical models.
Focusing on user sensemaking will uncover and overcome current inequalities in algorithmic design and potentially inform future regulation (Gu et al. 2021). Whenever
users use algorithmic services, they inevitably face the decision of whether or not to trust the algorithm-based information, and if so, to what extent. Such decisions are based
on user sensemaking of FTR, which is quite contextual and
subjective (Gu et al. 2021). In the process, FTR plays a heuristic cue triggering user trust (Just and Latzer 2017). The
determination of normative values in the algorithm is up to
the user (Crain 2018). FTR is mainly based on the systemic
features of algorithm service, users’ intrinsic interests, and
user information process. It can be argued that an algorithm
is a cognitively and socially reconstructed artifact based on
users’ information processing, existing beliefs, and contexts
(Thurman et al. 2019). Our findings contribute to the theoretical development of transparent fairness and practical
guidelines concerning the interactions between humans and
AIs. Through conceptualization and measurement development of FTR, we contribute to the knowledge of how to warrant such issues in algorithms and how to construct algorithmic media that are user-centered and societally accountable.
Our findings inform both the design of future OTT systems, as well as algorithm systems by incorporating normative dimensions of justice. Within and beyond this study,
these developments shall lead to (1) new recommendation
algorithms, focusing on the needs and preferences of the
users; (2) new meta-recommender frameworks, i.e. uncoupling content provision and recommendation engine; and (3)
improved user interfaces with maximum user control over
the recommendation and data collection process, creating
transparent fairness for real-world applications. Our findings
offer theoretical insight into user information processing
through clarification of algorithmic sensemaking processing.
Our results highlight the role of transparent fairness as part
of broader considerations of ethics by design in algorithmic
OTT media, particularly in influencing user sensemaking
and experience.

2 Literature review
AI is a collection of algorithms that can change its sub-component algorithms and generate new algorithms in response
to learned input data as opposed to relying solely on the

AI & SOCIETY

inputs it was designed to recognize as triggers (Gu et al.
2021; Park 2021). In this study, we focus on the algorithms
used in the OTT platforms and their relations with users
to examine the broader implications of AI in OTT media
ecology.

2.1 Theorizing the effects of fairness
and transparency on sensemaking processes
An OTT media platform is a media service offered directly
to viewers via the Internet, which delivers content over the
top of another platform. Contemporary OTT platforms such
as Amazon Prime, Netflix, YouTube Premium, and Hulu
have significantly transformed the media landscape by utilizing the power of AI. OTT platforms provide innovative
user experiences as well as disruptive and scalable business models (Shin 2021). For the users, access to millions
(on entertainment platforms with professionally produced
content) or billions (on social media and user-generated
content platforms) of pieces of digital content is provided
in a user-customized manner through algorithmically driven
recommendations. On the business side, suppliers can reach
millions of users in a direct and scalable way, without dealing with multiple instances of gatekeepers (Parker et al.
2016). OTT platforms can be seen as high-value opportunities which generate further value through every user interaction. A consequence of this intrinsic, self-propagating, value
creation is the emergence of monopolies, as competitors
are shut out and the roles of different market participants
and intermediaries are absorbed under the single interest of
maximizing the efficiency and profit of the platform. These
effects intensify if the platform itself is also the producer of
the digital goods offered.
In OTT markets, where algorithms have been widely
embedded and operated, the questions of fairness and transparency become relevant. Fairness and transparency emerge
as fundamental attributes against the growing adoption of
machine learning algorithms for processing large amounts of
user data. Algorithms embedded in OTT can replicate possible bias encoded in data. As platforms dominate markets
and reduce user agency, a societal responsibility beyond the
profitability of a single enterprise must be stipulated, making
fairness and transparency in their conduct imperative. While
governments and practitioners have examined the legal and
ethical concerns that arise from the widespread diffusion of
algorithm implementation, there has been little work that
attempts to explain how can we define transparency and
fairness in algorithms, and how can fair and transparent
algorithms be embedded and represented at an operational
algorithmic system (Park 2019)? As such, in this study, we
consider user perspectives of fairness-aware OTT systems
and mechanisms for enhancing their transparency. Although
sensemaking is often considered as an analytical activity

undertaken by AI experts, due to the fact that non-specialist
users are now being asked to make expert-like decisions
in complex AI environments, it is critical to view sensemaking challenges in the context of AI (Kleek et al. 2018).
Sensemaking has been defined as “the ongoing retrospective development of plausible images that rationalize what
people are doing” (Weick et al. 2005). Building on Weick
definition (Weick et al. 2005), the sensemaking process is
conceptualized as “the ongoing creation of coherence by
connecting salient observations, beliefs, and actions as reasons for one another” (Schildt et al. 2020). The need to make
sense of algorithmic systems is fast shifting from experts to
user domains, becoming more challenging as the level of the
complications involved also increases. Framing sensemaking as one of the challenges provides the view to consider
not only the means by which users of AI systems may be
supported in understanding them, but also the practical and
conceptual frameworks required to deal with these changes.
Such frameworks will consistently play a key role in informing not only developers, but policy-makers, regulators, and
user communities as the impact of AI permeates into every
sector of our economies and societies.

2.2 Research questions
Understanding how algorithms are constructed and applied
will be essential to their sustainable practice as a tool for
platforms and users (Shin et al. 2022). Despite their significance, little effort has been made to develop a robust
operational definition of the issues (Shin 2020), and this is
necessary to clarify them for academics and the professionals who develop algorithms adopted by platform users. We
aim to define and conceptualize fairness and transparency
in connection with the use of algorithms, explain the importance of such factors in the consumption of digital content on
OTT platforms and fill the current gap in our understanding
of the relationships between fairness, transparency and the
use of algorithms. We investigate three key questions in this
research:
1. What are relevant definitions of fairness and transparency in the use of algorithmic recommendations on OTT
platforms?
2. What is the user sensemaking process of normative
value assessment in algorithmic personalization?
3. How will more transparent, fair, user-controllable, recommendation algorithms improve the perceived quality
and fairness of recommendations?
These questions would clarify how fairness can be incorporated into OTT platforms, and what forms of transparency
around the platforms are effective for the users. By ensuring

13

AI & SOCIETY

the normative values for AIs, we will help to enhance the
trust and engagement of users.

3 Hypotheses development
In examining the dimensions of normative values, we use
users’ information processing in the context of OTT platforms. The algorithmic processing model likens the cognitive process to how algorithms work. Just like algorithms,
users take in information, organize and process it when using
media platforms.

3.1 Normative values: transparency and fairness
Fairness and transparency are becoming important considerations in the use of algorithms for the recommendation and delivery of digital content (Ananny and Crawford
2018). Automated data gathering and sharing may involve
processes that are unfair, flawed, opaque, or unaccountable
(Crain 2016). The OTT platform content recommendations
embody these issues in highly visible applications. Fairness
and transparency bring up vital requirements in the design
and development of algorithm-supported platforms (Kitchin
2017), which are purportedly designed to offer accurate and
reliable recommendations for the individual user (Lepri et al.
2018). Whether such recommendations match user interest, how the analytic processes are done, and whether the
outcomes are legally responsible remain unresolved (Diakopoulos and Koliska 2016). Thus, FTR emerges as a fundamental factor in the use of algorithms on media platforms
(Montal and Reich 2017). When transparent, open, and fair
services are provided, users are more likely to consider the
recommendations as high quality (Diakopoulos and Koliska
2016). Highly transparent platforms can grant users a sense
of personalization as responsible and fair recommendations
afford users a sense of trust that promotes satisfaction and
willingness to continue and subscribe (Soffer 2019). In
addition, open visibility and clear transparency for relevant
recommendations boost users’ interpretability of the system and the search performance (Shin 2020). We, therefore,
hypothesize:
H1. Users’ perception of fairness positively affects the
user perception of the responsibility of platforms.
H2. Users’ perception of fairness positively affects the
user-perceived quality of platforms.
H3. Users’ perception of transparency positively influences the user interpretability of platforms.
H4. Users’ perception of transparency positively influences the user-perceived quality of platforms.

13

3.2 Interpretability and responsibility:
how do users make sense of interpretability?
Algorithms in general are difficult to comprehend, and often
even the programmers who coded these systems find it hard
to understand them (Park and Jones-Jang 2022). An explanation of the process would help users understand results better and also allow the developers to track the processes and
assess the consequences. There are issues regarding what
constitutes understandable explanations and what level of
explanation is needed, as well as explainable to whom and
for what purpose (Kemper and Kolkman 2019). In this light,
algorithmic interpretability has been proposed (Shin 2021)
as enabling users to interpret how a decision/prediction is
made and mandates it as a requirement for the responsible development of algorithms. Interpretability in algorithmic platforms refers to a mechanism between humans and
algorithms that presents textual and/or visual information
that provides a qualitative understanding of how results are
processed and recommended (Ehsan and Riedl 2019). With
ensured transparency, AI can be understood easily, and algorithms can be justified in terms of how certain outcomes are
made. AI systems make recommendations or suggest content
based on black-box processes that people, in general, cannot
grasp (Renijith et al. 2020). In most current OTT platforms,
there is no understandable explanation of how algorithms
are used or operate, and how AI produces recommendations
for the user. Providing understandable explanations can be
critical in OTT as it gives users a sense of confidence and
empowerment. To confirm this relationship, we hypothesize:
H5. Users’ perception of responsibility positively
affects the user user-perceived quality of platforms.
H6. Users’ perception of interpretability positively
influences the user-perceived quality of platforms.

3.3 Credibility: how do users judge the credibility
of algorithmic recommendation?
Credibility in algorithmic processes is becoming a key
part of OTT platforms and is likely to become an essential
parameter for the operation and organization of algorithmic societies (Kolkman 2021). Media outlets are starting
to think about how their content is perceived and accepted,
because ratings of credibility exert a key role in viewership
and adoption.
Unless users believe the OTT’s information is credible,
they are not likely to be willing to act on the recommendation. Previous research has confirmed that credibility
in algorithms is related to users’ perceived quality of the
system (Kolkman 2021; Park 2019). Responsibility and
interpretability are the two key measures defining a user's

AI & SOCIETY

perceived quality of the system (Shin 2020). When users get
the sense that algorithmic recommendations are optimized to
their preferences, they consider the service valuable and feel
more trusting of the content. Users perceive the algorithms
as credible and reliable as long as they perceive the recommended items or content as high-quality choices for them.
Therefore, we hypothesize:
H7: Users’ perception of quality positively influences
the credibility of platforms.
The research model is shown in Fig. 1. Users' perception
of fairness determines their perception of the responsibility of the platforms (H1) and the perceived quality of the
platforms (H2). Similarly, users’ perception of transparency
influences their perception of the interpretability of the platforms (H3), and the perceived quality of the platforms (H4).
Then, users’ perception of responsibility (H5) and users’
perception of interpretability (H6) determines the perceived
quality of the platforms. Finally, users’ perception of quality
influences the credibility of the platforms.

4 Methods
With AI being increasingly used to support OTT platforms,
there has been a growing interest in designing transparent
platforms. Although many supposedly fair algorithms have
been proposed, there have been relatively few experimental studies testing whether these algorithms achieve their
intended effects, such as making people believe in the fairness of the platforms and allowing them to interpret the algorithms. We present a triangulated mixed method to examine
sensemaking in reference to the FTR issues in algorithms.

4.1 Mixed methods
As little attention has been given to the sensemaking process
underlying emergence in algorithmic development, we chose
to examine this issue through a mixed method combining a
qualitative and a quantitative approach. Mixed methods are
particularly useful in understanding algorithm users’ points

of view as they produce a voice for the users and ensure that
findings are grounded in users’ experiences. Our research
attempts to maintain the interpretations and perceptions of
the users in the foreground in our model and then empirically validate the model.
4.1.1 Qualitative method
Using the interpretive nature of qualitative methods, we
focused on respondent perceptions and interpretations
of algorithmic fairness and transparency, as well as their
understanding of responsibility. The qualitative method is
designed to produce cognitive mapping as a way of eliciting thoughts, feelings, views, and heuristics of how users
outline and make sense of algorithmic OTT. To determine
the notion of fairness that is most compatible with a user’s
perception of fairness, we employed multiple methods using
interviews, think-aloud, and experiments.
As an initial exploratory interview, we recruited a sample
of 29 undergraduate and graduate students from a large public university in South Korea from August 2021 to January
2022. Respondents from stratified random sampling were
selected from 10 campuses of 22 colleges located across
all major cities of Korea (Seoul, Busan, and Daejun), all of
whom had prior exposure to recommender systems (e.g.,
streaming platforms such as Netflix or Hulu). The interview
was designed to develop the dimensionality of user experience with algorithms, identify the subjective sensemaking of
FTR in relation to OTT algorithms, and analyze awareness,
perception, and evaluation of algorithms in social platforms,
news comment prioritization, and online recommendation.
The respondents were selected out of enrolling students for
classes regarding digital media, algorithms, and user design.
Screening interviews were conducted to measure the degree
of OTT usage. The preliminary survey was composed of
four questions: (1) which OTT platforms are you currently
using? (2) How much time do you spend on the OTT platforms per week? (3) What do you use OTT platforms for?
And (4) what contents are recommended by OTT? Unqualified respondents such as non-users, light users, and lapsers were dropped. We selected six major OTT platforms

Fig. 1  The effects of transparent fairness on algorithmic
sensemaking

13

AI & SOCIETY

currently used in Korea based on the market share report by
the Korean Communication Association.
Second, we designed field experiments using a thinkaloud protocol where participants were asked to express
their views as they complete a task. This open method gives
some insight into participants’ sensemaking processes of
fairness and transparency on quality perceptions, search
behavior, and content usage. We asked participants about
their past experiences with recommender systems and their
own understanding of recommendation algorithms and
fairness. We asked them to explain their current knowledge about how these systems work, and what fairness and
transparency meant to them in this context. A total of 100
students were incentivized to participate in the experiments
with varied recommender scenarios to examine OTT media
consumption, behavior in user forums, and content choices.
Next, recruited respondents were brought to a lab in each
university equipped with algorithmic software, computers,
and related applications. There were four steps of the experiment: (1) understanding the nature of the experiment; (2)
familiarization with the OTT algorithms and recommendation mechanism; (3) dual-stage experiment (input personal
information and using OTT search query); and (4) the exit
questions. In the first step, participants were required to
complete a pre-study questionnaire to gather demographic
information and prior experience with algorithmic environments. Next, they received instruction about the experiment
including the issues of FTR in terms of the definitions,
trends, policy, and industry standards. The FTR issues were
explained orally to each respondent. In the second phase,
participants were first instructed to practice using recommender systems to familiarize themselves with a similar
environment and the algorithmic features, OTT, and media
platforms. The participants were asked to search and navigate online to choose any contents that they would like to
view. When the participants finished the collection of contents, the session would then end. This session took approximately 1–2 h. Participants could have more time to complete
if requested. For algorithmic recommendations, we used
the Twitter application programming interface, which lets
people read and write Twitter data, to rebuild the newsfeed
of users. For news article comment recommendations, we
utilized data from Joongang Daily, the online newspaper in
South Korea with the most active and largest user community, to control suggestions of relevant posts and opinions.
To enable the field experiments, the developed smartphone
apps resembled the functions under investigation in popular
usage settings. Supplied algorithms were designed according
to different notions of fairness and transparency, for example
through different objectives of parity and diversity, partly
reflecting the objectives of various stakeholders. The apps
served to request ratings of qualities and feedback. Given
that personal data are sought in interviews and equally

13

personal choices and behaviors in online contexts are traced,
the project touches upon several of the General Data Protection Regulation (GDPR)-relevant dimensions. We have initiated an ethics clearance procedure with the ethics committee
of the university where one of the authors is affiliated.
4.1.2 Quantitative method
Using the conceptual clusters identified from the qualitative
approaches, a survey questionnaire was designed through
multiple rounds conducted by researchers, practitioners,
and experts. Respondents were recruited for the survey by
Embrain, a crowdsourcing website for data to hire remotely
located participants to perform on-demand experiments.
The survey was conducted during the last quarter of 2021
targeting general populations using AI services. Upon entering a survey site, respondents were asked to log in to their
preferred platforms and instructed (1) to search/read recent
trend news and (2) to surf/shop contents (film/drama/TV
series) that were algorithm-generated for about 1–2 h. The
respondents were given a list of news items and visual clips
that they could choose from. They were briefed that the news
items/contents were recommended by OTT platforms. After
the viewing and consumption, they were given a survey to
fill out. A total of 330 usable data were collected over a 6
months period (Table 1).

5 Results
In this section, we synthesized the results of our qualitative
and quantitative data, leading to the role of user sensemaking in OTT platforms.

Table 1  Demographics of samples

Age
Under 19
20–29
Over 30
Prior experience (OTT algorithms)
1–5 months
6–12
13–23
Over 2 years
Gender
Female
Male

Qualitative methods

Quantitative
methods

40
42
18

99
212
19

10
20
49
21

50
49
101
130

51
49

151
179

AI & SOCIETY

5.1 Qualitative findings
We identified a range of components of FTR based on the
interpretive and dialectic methods, focusing on the sensemaking of users with OTT algorithms (Table 2). In algorithmic OTT, the ethical debate revolves around how to
engage in practices of producing content recommendations
through machine learning in fair ways. One example is how
to remove, or at least reduce bias, while maintaining accurate personalization. Not everyone agrees on what constitutes FTR.
5.1.1 Fairness
As more decisions and processes of greater importance
are being made by algorithms in various contexts, participants are becoming increasingly concerned about the fairness of algorithmic results. Some participants focused on
the example of media credit scores, where algorithms are
widely used to determine content scores. Another respondent raised an important question, “How does algorithmically-informed journalism shape the stability of our democracy?” Participants commonly asserted that fairness is an
objective and impartial practice, without discrimination
or favoritism. Respondents explained the concepts of nondiscriminatory treatment, indiscrimination, and impartiality. One group mentioned neutrality, and others pointed out
instances of accurate results from the algorithmic filtering
and recommendations to elucidate the notion of fairness.
Unanimously, participants agreed that fairness is a key
attribute in algorithmic platforms by asking, “What emerging tools or approaches could mitigate opacity or problems
of unfairness/bias?” One respondent said, “I like [that] the

algorithm system does not discriminate and has no favoritism.” Another respondent mentioned, “I hope that the results
are accurate. When I believe the algorithm is fair and transparent, I trust and accept it” As for the due process, it was
noted that “[an] algorithm system should follow [a] due process of impartiality with no prejudice.” Based on participant
responses, it is inferred that user perception of fairness is
beyond mathematical fairness referring to contextual equality such as impartiality, indiscrimination, and accuracy.
5.1.2 Responsibility
Algorithmic responsibility in OTT algorithms was viewed
by respondents as that platform providers should be held
liable for considering platform design, development, recommendation processes, and outcomes. Respondents expressed
concerns that algorithmic decisions are vulnerable to making
errors that may lead to unwanted consequences. Respondents stated that algorithms are likely to create problems due
to an inability to deal with bias or due to simple oversight.
One respondent said, “... bias and mistakes in algorithms
can cause undesired and even hazardous problems.” Another
respondent used an example of a filter bubble, which can
result from personalized searches when OTT algorithms
selectively show what content users would like to see. It
was concluded that firms using algorithms should somehow
be held liable for the consequences of their programmed
machine learnings. In cases where algorithms deliver discriminatory outputs due to bias embedded in data, the systems should be responsible for the harm done as a consequence of this discrimination. One respondent noted, “I wish
I could examine and review the behavior of [the] algorithm
system.” Another respondent expressed, “algorithms can be

Table 2  Ethnographic observations for algorithmic experience
Features

User perception of definitions

Fairness

Frequency Key takeaways

Involves a balance between treatment and outcomes. Without bias/prejudice
Transparency
Allows humans to see whether the algorithms have
been thoroughly tested and make sense and that
they can understand why particular decisions are
made
Transparent fairness AIs should be transparent and fair, which are related
and interwoven. It should provide interpretable
explanations
Responsibility
Firms should be held legally accountable for the
consequences of their encoded automation
Interpretability
Allows users to comprehend why certain decisions or
predictions have been made
Credibility
Trustworthiness, expertise, and reliability

31

Quality

36

Users acknowledge via the use, experience, and
interaction with OTT algorithms

29

Beyond mathematical equality. Contextual judgment
and factual accuracy. Accountable fairness
Beyond visibility. Interpretable explanations that users
can understand

38

Fairness and transparency are the two sides of the
same coin. They are very closely related

45

Beyond legal accountability. Moral and ethical responsibility
Beyond explainability. Understanding interpretability
is more important than explainability
Trustable credibility determining factor of quality
judgment
Users consider AIs higher quality when fairness/transparency are warranted

42
39

13

AI & SOCIETY

equipped to change a system using only certain manipulations.” Based on this qualitative process, three factors
of responsibility are inferred—auditability, equity, and
accountability.
5.1.3 Transparency
Along with fairness, transparency has been considered a
key dimension in platforms. Because transparency itself
is a hypothetical notion, respondents’ views are divergent. Participants’ views can be classified into three main
areas: explainability, understandability, and observability.
Respondents articulated that considering the magnitude of
algorithms, the decisions (or processes) automatized by AI
should be interpretable, explainable, and observable to the
users who adopt, consume, and control the algorithms and
algorithmic services. As to understandability, one respondent stated that “Automated systems are increasingly complex, and they are often hard to understand especially about
[which] decisions are being made.” As to visibility, another
respondent said, “I think the design of algorithms is not visible and observable. A lot of these systems are designed by
private companies and their details are proprietary.” Another
said, “It’s hard to know what they are doing and who is
responsible for the decisions they make.” Others seconded
the need for explanation, saying, “Any outputs produced by
an algorithmic system should be explainable to the people
affected by those outputs.” Based on the analysis, the identified factors of transparency are observability, explainability,
verifiability, and visibility.
5.1.4 Transparent fairness
There has been a tendency for respondents to see transparency and fairness interchangeably. Many respondents considered that transparency is essentially linked to fairness.
Related to fairness, transparency emerges as a coupling
concept as a tool to help users to judge OTT’s value and
performance. One respondent said, “transparent processing
is about being fair and clear with people from the start about
who you are, and how and why you use their personal data.
Thus, the two are the same.” It is noted the two are conceptually and operationally connected. To process user data
fairly, the processing must be open and transparent. When
OTTs gather data through their platforms people expect
the providers to have a fair processing notice, transparently
describing their data collection processes. Many respondents
noted that for algorithms to be credible, users should have a
right to understand how their data are being used and how
the algorithms make decisions. Transparent fairness is providing clear explanations in algorithms, which can be a sign
of transparency as well as ensuring fairness. Respondents

13

note that transparency remains relevant to fairness but can
be neither necessary nor sufficient for it.
5.1.5 Interpretability
Respondents see interpretability as the most important, but
the least understood feature of algorithms. They explained
interpretability as how the mechanisms and processes in the
use of machine learning should be perceived by humans.
Respondents expressed that algorithms can be rationalized in
terms of how certain recommendations are produced in such
a manner that humans can understand. It has been raised
that some AI systems use algorithms based on black-box
practice that non-expert users cannot comprehend. Another
respondent seconded this view, “in most current AI systems,
there is no clear explanation of how the AI produced certain
results or values.” It is noted that “providing interpretable
explanations can be critical in certain fields.” Respondents
raised a need for interpretability beyond explainability to
make algorithms understandable to people. Respondents
also stated that recommendations for content should require
easier descriptions than algorithmic decisions.

5.2 Visual representation of interpretive data
Participants’ interpretive data on the algorithms are presented below in Table 2 and visually in Fig. 2. We visualized
a conceptual map that depicts relationships among different
concepts. As the cognitive maps were constructed through
interview, the maps represented a cognitively constructed
view of reality, showing concepts and relations that were
subjectively meaningful for the users (Table 3).

5.3 Quantitative findings
5.3.1 Algorithmic information processing
toward credibility
The path analysis illustrated that relationships proposed by
hypotheses were coherently supported and the path coefficients between the factors were significant (Table 4). Credibility was significantly affected by quality (β 1.017; C.R.
18.4), which was influenced by interpretability (β 0.252;
C.R. 4.06), and responsibility (β 0.138; 2.274). Perceived
quality accounted for approximately 52% of the variance
of interpretability, responsibility, fairness, and transparency.
The influence of fairness on interpretability and quality
were both significant (C.R. = 10.53***; C.R. = 11.15***).
The influence of transparency on responsibility and quality
were also significant (C.R. = 10.53***; C.R. = 11.15***).
The strong paths imply an underlying connection between
fairness and transparency. The path from fairness to transparency was found significant as well as the opposite path

AI & SOCIETY

Fig. 2  Conceptual mapping: user sensemaking of algorithmic OTT

Table 3  Descriptive statistics
Features

Mean

SD (standard
deviation)

Fairness
Transparency
Transparent fairness
Responsibility
Interpretability
Credibility
Quality

4.25
4.56
4.13
4.51
4.72
4.91
4.14

1.51
1.98
1.43
2.10
1.94
1.52
2.02

Table 4  Hypothesis results
Paths

β

Standard errors

C.R.

H1
H2
H3
H4
H5
H6
H7
Interaction effect 1
Interaction effect 2

0.465
0.223
0.363
0.669
0.707
0.138
1.017
0.839
0.252

0.062
0.040
0.068
0.063
0.087
0.060
0.051
0.074
0.061

7.430***
5.241***
5.243***
10.54***
8.032***
2.273**
18.44***
11.15***
4.061***

**2.58: 99% (0.01), ***3.29: 99.9% (0.001). P value < 0.0001 (Chisquare = 822.61; df = 218); GFI:0.950; NFI: 0.951;TLI: 0.867; CFI:
0.885; RMR: 0.79

direction was identified as significant, which led to further
investigation of interaction effects.
5.3.2 Interaction effects
Findings imply that fairness would have a stronger influence
when users perceive a high level of transparency. To test
the interaction effects, we compared the effect of fairness
on quality in the high transparency perception and the low
transparency perception. The analyses revealed that fairness
had a much larger effect on quality judgments in the high
transparency responses (M 4.41, SD 1.613 for the high fairness case and M 3.65, SD 1.731 for the low fairness case)
than in the low transparency responses (M 2.98, SD 0.293
for the high fairness case and M 4.82, SD 0.98 for the low
fairness case).
An interaction effect was identified between fairness and
transparency, with respect to quality and credibility (F[1,
187] = 4.89, p < 0.01). There was a significant difference in
attributions of quality to OTT in the high transparency group
as compared with the low transparency group (M 2.78 vs.
2.21). In support of the interaction effect, participants had
significantly higher fairness when they had a high level of
transparency (M 3.01, SD 1.02) as compared to when they
had a low level of transparency (M 3.88, SD 1.39), F[1,
187] = 4.98, p < 0.05.
The strong interaction effects imply that the higher fairness perception with the high transparency perception

13

AI & SOCIETY

formed a more positive quality assessment, consequently
leading to higher credibility than did the low fairness perception with the low transparency perception. The positive
effect of fairness on quality was reinforced by a high level
of transparency (Fig. 3). The figure confirms the complementarity between fairness and transparency in influencing
quality assessment.

6 A user‑centered sensemaking
for the algorithmic information seeking
With a notion of algorithmic fairness and transparency in
focus, we posited and tested the user cognitive mechanism
of platforms to examine algorithmic information processes
in interacting with OTT platforms. As content curation is
significantly directed by OTT algorithms, user design progressively becomes an activity of sensemaking, that is, an
awareness of which aspects should or could be examined.
This shift in attention calls for the new perspectives of usercenteredness and brings users closer to AI, which is an activity of sensemaking.

6.1 Measuring normative values from users’
sensemaking
What attributes are important in algorithmic platforms, how
do people perceive them, and with what expectations? Just
as fairness/transparency have been thought essential qualities in a typical system, so too are they in an algorithmbased platform (Shin 2021). Along with fairness and transparency, it is noteworthy to identify the mediating role of
interpretability and responsibility in the influence of normative values on quality. The mediating role implies that users
evaluate the quality of algorithmic platforms in terms of
how interpretable the recommendation results are and how
accountable the platform is. Not only do normative values
play a significant role by influencing perceived quality, they
also have a facilitating role in the course of users’ perception of interpretability and responsibility of platforms. Users
Fig. 3  Interaction plot for perceived quality (fitted means)

consider the system fair at a higher level with the available
understandable interpretation and users perceive it as more
transparent with the notion of perceived responsibility.
This finding has important implications for algorithmic
platform design and algorithmic experience. While fairness
and transparency have been ongoing issues in AI, such issues
have not been thoroughly examined but can be inferred
based on prior research (Sundar et al. 2020) that credibility
is significantly interconnected with these issues as it exerts
a key role in the development of credibility. When users
are assured of such issues, their credibility increases, and
they are willing to allow more of their data to be used and
analyzed. With the improved trust between algorithms and
users, more transparent processes are assured, and more
data enable algorithms to generate more predictive recommendations personalized and tailored to users’ interests and
preferences.

6.2 How do humans process algorithmic
information?
Collectively, our findings reveal user perception of algorithmic information processes by integrating credibility
into the sensemaking processes of users. User understanding of algorithm-driven OTT platforms is nonlinear and not
organized into structured ready-made mechanical processes.
How users feel, perceive, understand, and use algorithms
depends on how they process algorithmic information. Users
actively process the algorithmic decisions they receive from
their cognitions and assess them in terms of fairness and
transparency. Against rising concerns of algorithmic fairness and transparency (Sandvig et al. 2016), users seek to
understand how algorithms work, and how recommendations are made. Credibility dynamics offer insight into how
credibility can be built and how it mediates the connection
between the functional and non-functional quality dimensions (Lee 2018). Credibility in platforms is cognitively
constructed in such a way that processes are transparently
structured and understood in a human way (Guzman and
Lewis 2020). Established credibility allows users to believe

7
6
5
4

Fairness

3

Transparency

2
1
0

13

High Quality

Low Quality

AI & SOCIETY

that the recommendation is relevant and legitimate, and that
the source is credible.
The algorithmic information processes imply the existence of active roles (committed and conscious) for users in
constructing algorithms in OTT platforms (Shin 2021). This
role is consonant with the propositions that sensemaking is
socially constructed through interaction (Dervin 2003). The
processes show how users create a shared understanding of
their experiences with OTT platforms. Some prior research
has considered users to be passive recipients of recommended services who give their data to algorithms without
consideration (Pu et al. 2012). With the rise of algorithmdriven technologies, the users’ role has shifted from being
a passive recipient of automated processes through media
to a proactive creator of a preference profile who generates,
adjusts, and modifies algorithms depending on the framing and contexts of their media consumption. Users want
to view what they would prefer to watch and see, and they
become reinforced through the algorithmic processes. The
more users rely on algorithmic platforms, the narrower their
perspectives become which has been described as an echochamber or filter bubble (Sandvig et al. 2016).
From the findings, we can infer that users are the source
of algorithms and the designers of platforms by evoking profound subconscious cognitive processes. What users view
through algorithms, as far as their cognition is concerned,
is a cognitively constructed representation that emulates the
form of an accumulated experience that has been shaped by
a priori mental constructs. An algorithmic personalization
has become a shared social reality that shapes daily lives
and realities, affecting the perception of the outside (Park
2019). Pursuant to their discussion, humans and algorithms
are coevolving and creating reality together as they influence each other. Through credibility established by transparent fairness, humans, and algorithms actively enhance each
other’s complementary roles.

6.3 Interaction effects: the fairer algorithms are,
the more transparent
Fairness and transparency are interdependent, and they
are interacting with each other forging interaction effects
on the quality of an algorithm platform (Shin 2021). The
interaction effect reveals that normative values have significant effects on the attribution of high quality and credibility evaluation. People perceive the platform recommendation as of the highest quality and credible when they
are aware of how the algorithm operates and when they
believe the algorithm is fair. Normal users rely on their
existing heuristics in algorithms and technologies when
they face issues such as FTR (Sundar et al. 2020). They
normally do not hypothetically understand what FTR is

and how it affects the performance of algorithm services
as the concepts of FTR are interdependent and interwoven.
How to produce user-centered algorithms is a matter
of how to develop algorithmic platforms that use more
responsible and transparent processes. The findings propose that assessments of accuracy and transparency are
not purely objective responses to media content. Rather,
the model in this study lends support to the argument that,
to a significant extent, similar to perceptions of information in general, perceived transparency and fairness in the
algorithmic platforms are indeed in the eye of the beholder
(Kemper and Kolkman 2019). Transparency and fairness
can be considered more subjective perceptions held by
users rather than objective criteria (Park 2019). There
are various dimensions by which we can measure how
“transparent and fair” a recommendation is. In reality, it is
challenging to measure transparency and fairness directly.
This methodological difficulty may be partially contributing to the argument that transparency and fairness may
depend on users’ perceptions and understanding. Per the
sensemaking proposition, the meanings are socially constructed, negotiated, and communicated through various
interactions within the users’ mind and cognitively reenacted within users’ sensemaking processes. Rather than
such issues being unvaryingly or irreversibly provided to
users, users construct their own versions of transparency
and fairness based on their levels of existing trust and
other personal intrinsic factors. This proposition provides
a reference point for sensemaking theory in algorithmic
contexts. Transparency and fairness are cognitively and
contextually constructed realities of the user and depend
on that person’s perceptions.

7 Implications: user‑grounded fairness
and transparency
The goal of this study is to advance the understanding
of fairness and transparency in OTT via sensemaking
processes. By doing so, the contributions of this study
are both theoretical and practical. Practically, our results
provide design guidelines concerning what AI practitioners should do to support effective algorithmic platforms,
specifically, how to de-biasing recommendations, and
how to reflect FTR in the OTT interface. Theoretically,
we propose the algorithmic information process through
sensemaking together with the heuristic role of normative
values in the perception of AI (Shin 2021). Our results
have key implications for the fair and transparent literature and the efforts on formalizing algorithmic fairness in
terms of transparency.

13

AI & SOCIETY

7.1 Theoretical implications: how transparent
fairness influences the sensemaking process
How do we design OTT algorithms taking fairness and
transparency into account? Our results contribute to this
question by building upon research on algorithms from a
sensemaking perspective. The results indicate that algorithmic consumption and interactions were significantly
associated with users’ sensemaking processes, which were
significantly interwoven with users’ perceptions of transparency and fairness. Our results can contribute to theoretical
conceptualizations by highlighting how credibility is established and sustained.
7.1.1 Transparent fairness
Based on the findings, this study proposes the idea of transparent fairness. Qualitative data confirms that the two are
essentially related and even more so in the case of algorithms. When fair, people consider an algorithm transparent. Vice versa, when transparent, people consider it fair. A
transparent system affords the notion of fairness. Transparency remains relevant to fairness but can be neither necessary nor sufficient for it. Combining transparency with
ideas of fairness, we propose requirements on OTT media
platforms. While previous studies have extensively examined the two factors, little is known about how the two are
related conceptually and operationally. Our findings provide a relevant theoretical contribution by clarifying the
interrelated nature and thus proposing a new algorithmic
attribute, transparent fairness. Conceptually, transparency
is fundamentally linked to fairness and together constitutes
two sides of a coin working both technologically and emotionally to build the credibility of media-associated algorithms. Transparency can be considered a system-related
attribute, whereas fairness can be viewed as a user-related
attribute. Transparency is a technical attribute that algorithm
systems bear and involve, whereas fairness is an emotional
judgment of how users feel when transparency is confirmed.
Conversely, when fairness is ensured in the system, transparency is increased as it encompasses the notion of fairness.
This conceptual contribution can be heuristically meaningful
in terms of redefining such concepts in OTT platform contexts. The questions of what system is fair and transparent,
to what extent are the two overlapped, and how the two are
interrelated remains blurry as the two attributes have not
been fully explicated, nor do we know the extent to which
such attributes are influenced by or influence the quality of
algorithmic performance.
Transparent fairness is an extended concept of fairness
with respect to the ability to provide explainable features and
interpretable mechanisms. This extended concept of fairness
is necessary in cases where fairness is prioritized, such as

13

OTT platforms where there is only data labeled with previous user data. Some people consider algorithms fair even if
the system is not transparent. Some people perceive algorithms as transparent despite the perception that the system
is unfair and biased. The concepts of fairness and transparency combine to yield a few possibilities: transparent and
either fair or not; and not transparent and either not fair or
fair. Transparent fairness goes beyond procedural fairness,
which is concerned with the automated procedures used by
algorithms, giving users a sense of absolute and complete
fairness concerning proper data analytical procedures and
recommendation outcomes. Transparent fairness can be an
underlying theoretical attribute of algorithms in OTT as
well as a practical scale for evaluating the performance and
results of algorithms.
7.1.2 Active sensemaking
Methodologically, we showed a mixed approach as a method
for evaluating sensemaking processes, particularly in the
epistemological reflections on the scope and nature of cognitive mapping. The data that our qualitative method generates have a key role to play from a sensemaking perspective.
While the analysis implies that a great deal of user data can
be based on quantitative analytics, our methods proved the
kinds of nuances that interpretative methods can expose
in terms of fairness and transparency of algorithms. Our
mixed approach reveals a new concept of active sensemaking, which advances from contemporary sensemaking and
furthers users’ active role in sensemaking that actively harnesses and leverages the human ability to perceive and forge
patterns in an algorithmic environment. Adding meaning to
those patterns to generate insights, and provide a foundation
for design and development of algorithm systems. Active
sensemaking highlights an interactionist view and is an
essentially social process. Users’ interpretations and behaviors are significantly affected by interactions with algorithms
that allow users to comprehend their context and take action
collectively. While this is not a completely novel argument
in human–AI interaction, we underscore that understanding
the process of user sensemaking is heuristic since it will help
us to figure out how users’ roles should be created, designed,
and managed in OTT platforms.
7.1.3 Algorithmic credibility
How do users establish the quality of the recommendations
and the credibility of the algorithmic sources? Our findings illustrate users’ sensemaking processes on how they
reason and perceive AI characteristics, how their credibility
is configured and sustained, what cognitive information is
processed, and what behavioral results are derived from the
processes. While prior research has confirmed the dimension

AI & SOCIETY

of credibility in AI (e.g., Kolkman 2021; Montal and Reich
2017), our study further clarifies the role and relation of
credibility in algorithmic platforms, its determinants, and
algorithmic sensemaking. Our findings on interaction effects
of credibility are consonant with prior studies (Shin 2021),
signifying the need for a multidimensional analysis of credibility mechanisms.
In algorithmic platforms, users construct a sense of credibility with the processed information on fairness and transparency. Algorithmic credibility exists not in the algorithms,
but in the mind of users who perform sensemaking tasks.
Credibility is a result of the user sensemaking process that
lies in the minds of the users and does not reside in the algorithms being assessed. Various elements of a content source
impact users' assessments of credibility, but credibility lies
in the process of the user of an algorithmic source. Affording strong user trust may assure users that their personal
data would be processed in compliance with transparent and
legitimate manners, thereby generating credibility for the
recommendations and the platforms, ultimately leading to
heightened levels of user involvement. Our results reveal the
sensemaking links among algorithmic attributes, algorithmic experiences, and users’ interactions with algorithms.
Normative values provide users with cues for credibility
and established credibility lets users use algorithms with an
interpretable and accountable assurance.

7.2 Practical implications: algorithm design
as a process of sensemaking
Our results provide media platform practitioners with guidelines on how to integrate fairness and transparent issues with
other features, for example, how to gather users data and/or
implicit feedback effectively while supporting users’ trust
and credibility. Since fairness and transparency comprise
the user sensemaking of algorithmic platforms, the industry
may create a strategy that applies a user-centered method
to personalization mechanisms. Cognitive perceptions and
algorithmic information processing are critical in justifying
how and why people perceive and feel about the issues surrounding algorithmic platforms and how they consume and
involve algorithms in co-developing content. The primary
task of algorithmic platforms is to help people sort contents that are intriguing and matching with their personal
preferences. Understanding how users intuit and perceive
their interactions with algorithms allows OTT practitioners and algorithm designers to perform more efficiently and
naturally.
Funding This project has been funded by the Office of Research and
the Institute for Social and Economic Research at Zayed University
(The Policy Research Incentive Program 2022). It also received the

support from Provost's Research Fellowship Award of Zayed University
(R21050/2022).

References
Ananny M, Crawford K (2018) Seeing without knowing. New Media
Soc 20(3):973–989. https://​doi.​org/​10.​1177/​14614​44816​676645
Crain M (2018) The limits of transparency. New Media Soc
20(1):88–104. https://​doi.​org/​10.​1177/​14614​44816​657096
Dervin B (2003) Sense-making’s journey from metatheory to methodology to methods. In: Dervin B (ed) In sense-making methodology reader. Hampton Press Inc, New York, pp 141–146
Diakopoulos N, Koliska M (2016) Algorithmic transparency in the
news media. Digit J 5(7):809–828. https://​d oi.​o rg/​1 0.​1 080/​
21670​811.​2016.​12080​53
Gu J, Yan N, Rzeszotarski J (2021) Understanding user sensemaking
in machine learning fairness assessment systems. Proceedings
of the Web Conference. ACM, New York. https://​doi.​org/​10.​
1145/​34423​81.​34500​92
Helberger N, Karppinen K, D’Acunto L (2018) Exposure diversity
as a design principle for recommender systems. Inf Commun
Soc 21(2):191–207. https://​doi.​org/​10.​1080/​13691​18X.​2016.​
12719​00
Hoffmann A (2019) Where fairness fails. Inf Commun Soc 22(7):900–
915. https://​doi.​org/​10.​1080/​13691​18X.​2019.​15739​12
Just N, Latzer M (2017) Governance by algorithms. Media Cult Soc
39(2):238–258. https://​doi.​org/​10.​1177/​01634​43716​643157
Kemper J, Kolkman D (2019) Transparent to whom? Inf Commun
Soc 22(14):2081–2096. https://​d oi.​o rg/​1 0.​1 080/​1 3691​1 8X.​
2018.​14779​67
Kitchin R (2017) Thinking critically about and researching algorithms. Inf Commun Soc 20(1):14–29. https://​doi.​org/​10.​1080/​
13691​18X.​2016.​11540​87
Kleek M, Seymour W, Veale M, Binns R (2018) The need for sensemaking in networked privacy and algorithmic responsibility.
Sensemaking Workshop: CHI 2018, April 2018, Montréal,
Canada
Kolkman D (2021) The credibility of algorithmic models to nonexperts. Inf Commun Soc. https://​doi.​org/​10.​1080/​13691​18X.​
2020.​17618​60
Lee M (2018) Understanding perception of algorithmic decisions. Big
Data Soc 5(1):1–16. https://​doi.​org/​10.​1177/​20539​51718​756684
Lepri B et al (2018) Fair, transparent, and accountable algorithmic
decision-making processes. Philos Technol 31(4): 611–627.
https://doi:hdl.handle.net/1721.1/122933
Meijer A (2014) Transparency. In: Mark B, Robert EG, Thomas S
(eds) In the Oxford Handbook of Public Accountability. Oxford
University Press, Oxford. https://​d oi.​o rg/​1 0.​1 093/​oxfor​d hb/​
97801​99641​253.​013.​0043
Moller J, Trilling D, Helberger N, van Es B (2018) Do not blame it
on the algorithm. Inf Commun Soc 21(7):959–977. https://​doi.​
org/​10.​1080/​13691​18X.​2018.​14440​76
Montal T, Reich Z (2017) I, robot you, journalist. Who is the author?
Digit J 5(7):829–849. https://​doi.​org/​10.​1080/​21670​811.​2016.​
12090​83
Park YJ (2021) The future of digital surveillance: why digital monitoring will never lose its appeal in a world of algorithm-driven
AI. University of Michigan Press, Ann Arbor
Park YJ, Jones-Jang SM (2022) Surveillance, security, and AI as
technological acceptance. AI Soc. https://​d oi.​o rg/​1 0.​1 007/​
s00146-​021-​01331-9
Pu P, Chen L, Hu R (2012) Evaluating recommender systems
from the user perspective. User Model User Adapt Interact
22(4):317–355. https://​doi.​org/​10.​1007/​s11257-​011-​9115-7

13

AI & SOCIETY
Rosenfeld A, Richardson A (2019) Explainability in human-agent
systems. Auton Agent Multi Agent Syst 33(6):673–705
Sandvig C, Hamilton K, Karaholios K, Langbort C (2016) When the
algorithm itself is a racist. Int J Commun 10:4972–4990
Schildt H, Mantere S, Cornelissen J (2020) Power in sensemaking processes. Organ Stud 41(2):241–265. https://​doi.o​ rg/1​ 0.​1177/​01708​
40619​847718
Shin D (2021) The perception of humanness in conversational journalism. New Media Soc. https://d​ oi.o​ rg/1​ 0.1​ 177/1​ 46144​ 48219​ 93801
Shin D, Park Y (2019) Role of fairness, accountability, and transparency in algorithmic affordance. Comput Hum Behav 98:277–284.
https://​doi.​org/​10.​1016/j.​chb.​2019.​04.​019
Shin D, Zaid B, Biocca F, Rasul A (2022) In platforms we trust?
Unlocking the black-box of news algorithms through interpretable AI. J Broadcasting Electron Media. https://​doi.​org/​10.​1080/​
08838​151.​2022.​20579​84
Soffer O (2019) Algorithmic personalization and the two-step flow of
communication. Commun Theory 31:297–315

13
View publication stats

Sundar S, Kim J, Beth-Oliver M, Molina M (2020) Online privacy heuristics that predict information disclosure. CHI '20, April 25–30.
https://​doi.​org/​10.​1145/​33138​31.​33768​54
Thurman N, Moeller J, Helberger N, Trilling D (2019) My friends,
editors, algorithms, and I. Digit J 7(4):447–469. https://​doi.​org/​
10.​1080/​21670​811.​2018.​14939​36
Weick K, Sutcliffe K, Obstfeld D (2005) Organizing and the process of
sensemaking. Organ Sci 16(4):409–421. https://​doi.​org/​10.​1287/​
orsc.​1050.​0133
Publisher's Note Springer Nature remains neutral with regard to
jurisdictional claims in published maps and institutional affiliations.

