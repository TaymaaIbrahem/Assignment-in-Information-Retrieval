Understanding Transparency in
Algorithmic Accountability
U of Colorado Law Legal Studies Research Paper No. 20-34

Margot E. Kaminski

Electronic copy available at: https://ssrn.com/abstract=3622657

Draft, pre-edited version, forthcoming in Cambridge Handbook of the Law of Algorithms, ed. Woodrow
Barfield, Cambridge University Press (2020).

Understanding Transparency in Algorithmic Accountability
This is a draft, pre-edited version, forthcoming in Cambridge Handbook of the Law of
Algorithms, ed. Woodrow Barfield, Cambridge University Press (2020).
Margot E. Kaminski

Transparency has been in the crosshairs of recent writing about accountable algorithms.
Its critics argue that releasing data can be harmful, and releasing source code won’t be useful.1
They claim individualized explanations of artificial intelligence (AI) decisions don’t empower
people, and instead distract from more effective ways of governing.2 While criticizing
transparency’s efficacy with one breath, with the next they defang it, claiming corporate
secrecy exceptions will prevent useful information from getting out.3
This chapter bucks the tide. Transparency is necessary, if not sufficient, for building
and governing accountable algorithms.4 But for transparency to be effective, it has to be
designed. It can’t be sprinkled on like seasoning; it has to be built into a regulatory system from

1

See, e.g., J. A. Kroll, J. Huey, S. Barocas, et al., Accountable Algorithms (2017) 165 U. Pa. L. Rev.

633, 657–60.
2

L. Edwards and M. Veale, Slave to the Algorithm? Why a “Right to an Explanation” Is Probably Not

the Remedy You Are Looking for (2017) 16 Duke L. & Tech. Rev. 18, 67.
3

S. Wachter, L. Floridi, and B. Mittelstadt, Why a Right to Explanation of Automated Decisionmaking

Does Not Exist in the General Data Protection Regulation (2017) 7 Int’l Data Privacy L. 76, 79 n. 13, 84, 89.
But see M. Brkan, Do Algorithms Rule the World? Algorithmic Decision-Making in the Framework of the
GDPR and Beyond (2019) 27 Int’l JL & Info. Tech. 91.
4

For a similar view, see M. Ananny and K. Crawford, Seeing without Knowing: Limitations of the

Transparency Ideal and Its Application to Algorithmic Accountability (2018) 20 New Media & Soc’y 973, 982.

Electronic copy available at: https://ssrn.com/abstract=3622657

Draft, pre-edited version, forthcoming in Cambridge Handbook of the Law of Algorithms, ed. Woodrow
Barfield, Cambridge University Press (2020).

the onset. And determining the who, what, when, and how of transparency requires first
addressing the question of why.5
Building on my work elsewhere, I thus begin by discussing the rationales behind
regulating algorithmic decision-making, or decision-making by AI.6 I discuss the growing
awareness in the literature that the object of regulation is not the technology of the algorithm
in isolation, but includes the human systems around it. I then outline a taxonomy of
transparency for accountable algorithms, building on the work of earlier authors and my own
research on the European Union’s General Data Protection Regulation (GDPR).7
WHY REGULATE ALGORITHMS?

An algorithm is a computer program. A subset of algorithms build predictions and
correlations using large data sets as input.8 Increasingly, these predictive algorithms are used
by humans to make significant decisions – from housing to employment to criminal sentencing
and release decisions – about other human beings. A quickly growing literature charts how the
use of a variety of kinds of algorithms to make significant decisions about people can raise a
spectrum of harms.9 Algorithmic analysis and decision-making are increasingly employed
5

M. E. Kaminski, The Right to Explanation, Explained (2019) 34 Berkeley Tech. LJ 189, 211.

6

I here draw on my work in M. E. Kaminski, Binary Governance: Lessons from the GDPR’s Approach

to Algorithmic Accountability (2019) 92 S. Calif. L. Rev. 1529.
7

See, e.g., F. Pasquale, The Black Box Society (2015), pp. 140–88 (calling for a system of layered

“qualified transparency”); A. D. Selbst, Disparate Impact in Big Data Policing (2017) 52 Ga. L. Rev. 109, 169–
72 (describing algorithmic impact assessments); A. Tutt, An FDA for Algorithms (2017) 69 Admin. L. Rev. 83,
110–11 (identifying a “spectrum of disclosure”); T. Z. Zarsky, Transparent Predictions (2013) U. Ill. L. Rev.
1503, 1523 (identifying three “segments of information flow”).
8

D. Lehr and P. Ohm, Playing with the Data: What Legal Scholars Should Learn about Machine

Learning (2017) 51 UC Davis L. Rev. 653, 658–62 (describing machine-learning algorithms).
9

See, e.g., D. K. Citron, Technological Due Process (2008) 85 Wash. UL Rev. 1249; D. Citron and F.

Pasquale, The Scored Society: Due Process for Automated Predictions (2014) 89 Wash. L. Rev. 1, 16–18; S.

Electronic copy available at: https://ssrn.com/abstract=3622657

Draft, pre-edited version, forthcoming in Cambridge Handbook of the Law of Algorithms, ed. Woodrow
Barfield, Cambridge University Press (2020).

across a variety of sectors, including in government.10 They are used to determine loan rates,
to hire and fire employees, to track and label people, and to predict and manipulate behavior.11
Roughly speaking, there are three reasons people call for regulating algorithms.12 The
first is the most prevalent, and easiest to understand: people call for regulating algorithms
because the decisions made based on algorithmic reasoning can be biased, discriminatory, and
wrong. This instrumental rationale characterizes regulation as a tool for correcting concrete,
potentially measurable, problems.
Algorithms are neither neutral, nor perfect. Programmers and the institutions that
employ them make a host of decisions, from what data sets to use to train the algorithm, to how
to define what the algorithm’s target output is, to how likely the algorithm is to produce false
positives versus false negatives.13 For example, the designers of a widely used recidivism risk
algorithm, relied on by judges across the United States in pre-trial and sentencing
determinations, chose to label a person as a recidivist (a re-offender) if they were re-arrested,
rather than re-convicted.14 This decision isn’t math; it’s policy. And it has disparate

Barocas and A. D. Selbst, Big Data’s Disparate Impact (2016) 104 Calif. L. Rev. 671, 714–23; J. M. Eaglin,
Constructing Recidivism Risk (2017) 67 Emory LJ 59, 67–88; P. T. Kim, Auditing Algorithms for
Discrimination (2017) 166 U. Pa. L. Rev. Online 189.
10

D. R. Desai and J. A. Kroll, Trust but Verify: A Guide to Algorithms and the Law (2017) 31 Harv.

JL & Tech. 1.
11

See, e.g., A. D. Selbst, A New HUD Rule Would Effectively Encourage Discrimination by

Algorithm, Slate (August 19, 2019), https://slate.com/technology/2019/08/hud-disparate-impact-discriminationalgorithm.html.
12

Kaminski, above note 6. See also A. D. Selbst and S. Barocas, The Intuitive Appeal of Explainable

Machines (2018) 87 Fordham L. Rev. 1085, 1117–19.
13

Eaglin, above note 9; Lehr and Ohm, above note 8; Barocas and Selbst, above note 9.

14

Eaglin, above note 9, p. 78.

Electronic copy available at: https://ssrn.com/abstract=3622657

Draft, pre-edited version, forthcoming in Cambridge Handbook of the Law of Algorithms, ed. Woodrow
Barfield, Cambridge University Press (2020).

consequences: black men are arrested at higher rates than Hispanics or whites.15 A human
policy choice – to ask the algorithm to predict re-arrest, and to affirm its accuracy when a
person is arrested rather than convicted – skews the algorithm’s output, with significant
consequences for those individuals to whom it is applied. Another widely discussed example
of the problem of algorithmic bias is the use of biased data sets. When an algorithm is trained
on biased data, its outputs will be biased as well.16 The instrumental rationale for regulating
algorithms argues for regulation to correct these problems.
There are, however, other reasons to regulate algorithmic analysis and decisionmaking, beyond instrumentalism. We expect decisions with significant consequences to be
reasoned, if not reasonable – and some reasons for decision-making may be socially
unacceptable, or even illegal. Some calls for regulating algorithmic decision-making thus focus
on a need for justification: demonstrating the legal and social legitimacy of algorithmic
decision-making, whether by requiring an explanation of the decision, or by providing
oversight over a decision-making system.17
Algorithmic decision-making raises significant concerns about justification.
Algorithms draw correlations and make predictions based on data, and are both constrained by
the limitations of their input and comparatively unconstrained by social context or social

15

Ibid., p. 95. See also J. Eaglin and D. Solomon, Brennan Center For Justice, Reducing Racial and

Ethnic Disparities in Jails: Recommendations for Local Practice (2015), pp. 17–18,
www.brennancenter.org/sites/default/files/publications/Racial%20Disparities%20Report%20062515.pdf .
16

Barocas and Selbst, above note 9. See also K. Crawford and R. Calo, There Is a Blind Spot in AI

Research, Nature (October 13, 2016), www.nature.com/news/there-is-a-blind-spot-in-ai-research-1.20805.
17

K. Brennan-Marquez, “Plausible Cause”: Explanatory Standards in the Age of Powerful Machines

(2017) 70 Vand. L. Rev. 1249, 1288 (“A key tenant of legality, separating lawful authority from ultra vires
conduct, is the idea that not all explanations qualify as justifications”).

Electronic copy available at: https://ssrn.com/abstract=3622657

Draft, pre-edited version, forthcoming in Cambridge Handbook of the Law of Algorithms, ed. Woodrow
Barfield, Cambridge University Press (2020).

norms.18 For example, an algorithm may discover a strong correlation between the color of
one’s shoes and the likelihood of loan repayment – yet normatively, we might not want banks
making lending decisions based on the color of one’s footwear (which might also lead to
disparate impacts based on gender, sexual orientation, economic status, race, or other
characteristics, many of which governments treat as protected classes).19 An algorithm, too,
won’t always bring in additional context to a particular decision, for purposes of determining
either accuracy or leniency. It may make and follow rules; it won’t necessarily be equipped to
modify or break them.20 Algorithmic decision-making has the potential in practice to be highly
acontextual, compared to its human equivalent.21
Thus, the justificatory rationale for regulating algorithmic decision-making calls for
transparency into the data, model, and heuristics used. This is to ensure decisions aren’t made
based on illegal or otherwise normatively unacceptable factors and reasoning, and to enable
individuals to argue why the system’s reasoning, as applied to them, might be erroneous or
unfair.
The justificatory rationale also leads to calls for oversight over the company building
and/or using the technology. Just as in the law we have both individual procedure (due process)
and systemic versions of accountability (such as the Administrative Procedure Act), an
algorithmic accountability regime aimed at producing legitimacy might use either, or both, an

18

Kaminski, above note 6, p. 118 (“When we replace human decision makers with nonhuman decision

makers, we potentially eliminate important work that a human decision maker does to both fill in and
circumscribe decisional context in a particular case”).
19

E. Felten, What Does It Mean To Ask for an “Explainable” Algorithm?, Freedom to Tinker

(May 31, 2017), https://freedom-to-tinker.com/2017/05/31/what-does-it-mean-to-ask-for-an-explainablealgorithm.
20

Citron, above note 9, p. 1301.

21

Kaminski, above note 6, p. 118.

Electronic copy available at: https://ssrn.com/abstract=3622657

Draft, pre-edited version, forthcoming in Cambridge Handbook of the Law of Algorithms, ed. Woodrow
Barfield, Cambridge University Press (2020).

individualized and systemic approach.22 A person can use individual process to reassure herself
that a decision-making system is legitimate, or she could rely on the promises and oversight of
a number of external experts and stakeholders to assure her of its legitimacy. Or she might want
both: expert oversight over the system as a whole, and an individualized ability to assess
whether its reasoning applies fairly to her, in a particular decision.
The third rationale for regulating algorithmic decision-making is dignitary. Algorithmic
decision-making potentially objectifies individuals; it treats them as fungible.23 There are a
number of flavors of this argument. The first states that using a machine to make a decision
about a human being violates her dignity as a person.24 This position tends to be more
polarizing, and is arguably more accepted in the European Union than it is in the United
States.25 To some people, machine decisions are inherently creepy, while to others, they’re
efficient and normalized. A second version of the dignitary argument claims that creating
profiles of individuals harms their autonomy and dignity by freezing a version of them, created
without their input, and then making decisions based on this “data double.”26 If individuals
lack the ability to access, correct, or even erase data in these profiles, their dignity and
22

Ibid., p. 149; Citron, above note 9, pp. 1305, 1308.

23

Kaminski, above note 6, p. 113.

24

Ibid., p. 114 (“The first, largely European, criticism of algorithmic decision-making is that allowing a

decision about humans to be made by a machine inherently treats humans as objects, showing deep, inherent
disrespect for peoples’ humanity”); L. A. Bygrave, Minding the Machine: Article 15 of the EC Data Protection
Directive and Automated Profiling (2001) 17 Computer L. & Security Rep. 17, 18.
25

M. L. Jones, The Right to a Human in the Loop: Political Constructions of Computer Automation

and Personhood (2017) 47 Social Stud. Sci. 216, 231; T. Z. Zarsky, Incompatible: The GDPR in the Age of Big
Data (2017) 47 Seton Hall L. Rev. 995, 1016–17.
26

Kaminski, above note 6, p. 115 (“A data double objectifies an individual by taking this dynamic,

participatory process and placing it in the hands of other entities and out of the hands of the individual”);
Bygrave, above note 24, p. 18; D. Lyon, Surveillance, Snowden, and Big Data: Capacities, Consequences,
Critique, Big Data & Society (July–December 2014), pp. 1, 6.

Electronic copy available at: https://ssrn.com/abstract=3622657

Draft, pre-edited version, forthcoming in Cambridge Handbook of the Law of Algorithms, ed. Woodrow
Barfield, Cambridge University Press (2020).

autonomy are harmed. Finally, a third version of the dignitary argument points to story after
story of digital manipulation, demonstrating the real ways in which algorithmic profiling and
its consequences restrict the autonomy of individuals.27 This argument appears to have a
broader cross-cultural appeal. All three subsets of the dignitary argument counsel toward
implementing a set of individual data-protection-like rights: access, correction, explanation,
and even erasure and contestation rights.
These three rationales are clearly connected. So are the regulatory solutions their
proponents propose. Requiring the users of algorithmic decision-making to justify their
decisions or legitimize the process of building the system (addressing justificatory concerns)
will also provide insight into bias and error in the system (addressing instrumentalist concerns).
Requiring programmers or institutions that use algorithms to disclose decision-making
heuristics, too, can lead to uncovering bias and discrimination. Respecting somebody’s dignity
by allowing her to access and correct errors in her digital profile can also serve the instrumental
purpose of making a system less prone to error.
But as I’ve argued, the great difficulty in crafting algorithmic accountability is that
these rationales sometimes push toward divergent models of regulation.28 Focusing on
instrumental reasons leads to an approach that emphasizes systemic, ex ante, continuous, and
collaborative modes of governance. For a variety of reasons – inherent to the technology, its
use, and to the types of harms it may cause – this approach is likely more effective at
systemically rooting out discrimination and bias. Focusing on dignitary reasons, by contrast,
27

Kaminski, above note 6, p. 116 (“Secret profiling and decision-making can lead to manipulation”);

Zarsky, above note 7, pp. 1541−53; D. Susser, B. Roessler, and H. Nissenbaum, Online Manipulation: Hidden
Influences in a Digital World, https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3306006; J. Luguri and L.
Strahilevitz, Shining a Light on Dark Patterns, https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3431205.
28

Kaminski, above note 6, pp. 149–53.

Electronic copy available at: https://ssrn.com/abstract=3622657

Draft, pre-edited version, forthcoming in Cambridge Handbook of the Law of Algorithms, ed. Woodrow
Barfield, Cambridge University Press (2020).

leads to arguments for individual rights and empowerment. The justificatory rationale can lead
to arguments for either form of governance, or for both. Most scholars who reject
individualized or public forms of transparency do so because their primary focus is on
correcting bias and error; they do not appear to place much value on dignitary or justificatory
rationales. As I have argued, this is wrong. Even for those concerned solely about fixing errors
and addressing bias, transparency and accountability have important roles to play.
WHICH SYSTEM ARE YOU REGULATING?

To understand how best to employ transparency, we have to know not only the why of
regulation – what we want transparency to do – but also the object of regulation: what system
we want to see into. There is a growing understanding among scholars in this space that
algorithmic accountability is as much about seeing into, and affecting, the human systems
around algorithms as it is about seeing into the technology.
This is not to say that increased transparency into the technology itself is unimportant.
Much of the concern around algorithmic decision-making stems from a concern about
delegating decision-making to “black box” systems that we can neither understand nor
predict.29 A growing community of computer science researchers focuses on making these
“black box” systems explainable, or otherwise transparent or accountable.30 And certain types
of legal transparency requirements – such as those in the European Union’s GDPR’s Articles

29

Pasquale, above note 7. W. Nicholson Price II, Regulating Black-Box Medicine (2017) 116 Mich. L.

Rev. 421.
30

A. Abdul et al., Trends and Trajectories for Explainable, Accountable and Intelligible Systems: An

HCI Research Agenda, ACM Digital Library (2018), https://doi.org/10.1145/3173574.3174156 (“We
investigate how HCI researchers can help to develop accountable systems by performing a literature analysis of
289 core papers on explanations and explainable systems, as well as 12,412 citing papers”).

Electronic copy available at: https://ssrn.com/abstract=3622657

Draft, pre-edited version, forthcoming in Cambridge Handbook of the Law of Algorithms, ed. Woodrow
Barfield, Cambridge University Press (2020).

13, 14, 15, and 22, and in the US Fair Credit Reporting Act (FCRA)31 – may either require use
of algorithmic explainability technology, or may functionally prohibit the use of algorithms
that are not explainable.
But to focus only on technological fixes misses the growing consensus that algorithmic
accountability is about making human systems accountable, too.32 A growing number of
scholars have arrived at this observation. Earlier scholarship with its array of proposed
accountability measures tacitly acknowledges that accountability is a problem of human
organizations, not just technology.33 Michael Ananny and Kate Crawford more explicitly call
for “going beyond ‘algorithms as fetishized objects’” and looking instead to “an algorithmic
system . . . [as] not just code and data but an assemblage of human and non-human actors.”34
As Jessica Eaglin has observed in her insightful analysis of the use of recidivism risk
software, algorithms are very much embedded in social systems. Algorithmic decision-making
reflects the values of programmers making design choices at the onset, and the context in
which, and means by which, algorithms are used.35 Technologies, in other words, are
political.36 Many human choices go into building a risk assessment algorithm. And once it is

31

15 USC § 1681m (2012); Regulation B, 12 CFR §§ 1002.1–16 (2018).

32

Ananny and Crawford, above note 4. See also A. D. Selbst, d. boyd, S. Friedler, et al., Fairness and

Abstraction in Sociotechnical Systems (November 7, 2018),
https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3265913.
33

Citron, above note 9, p. 1271; Citron and Pasquale, above note 9, pp. 20–7; K. Crawford and J.

Schultz, Big Data and Due Process: Toward a Framework to Redress Predictive Privacy Harms (2014) 55 BCL
Rev. 93, 124–8.
34

Ananny and Crawford, above note 4, p. 11.

35

Eaglin, above note 9, p. 63.

36

L. Winner, Do Artifacts Have Politics? (1980) 109 Daedalus 121.

Electronic copy available at: https://ssrn.com/abstract=3622657

Draft, pre-edited version, forthcoming in Cambridge Handbook of the Law of Algorithms, ed. Woodrow
Barfield, Cambridge University Press (2020).

deployed, that algorithm may produce one measurement of racial disparity in the abstract, and
perhaps another entirely when combined with patterns of judicial discretion or deference.37
The law plays an important role here, in addressing how human users are trained, what
disclosures are made to them, and even the degree of deference required to algorithmic
decisions. Danielle Citron very early on in discussions of algorithmic accountability called for
training the humans who work with algorithms, in order to avoid the problem of “automation
bias”: the tendency of human workers to defer to algorithmically produced decisions.38 By
contrast, the Wisconsin Supreme Court recently refused to hold that a trial court’s use of
recidivism risk software violated a defendant’s due process rights, requiring instead a “written
advisement” to judges alerting them to five potential issues with the program, including bias
against minorities.39 Whether this notice (in contrast with more extensive judicial training) will
be effective at combating automation bias on the part of judges is questionable. At the least, it
fails to establish a standard of human scrutiny of automated decision-making.
The implications of this move – to see and address the human builders, users, and
systems around the algorithm – are important: we need insight not just into code or a data set,
but into the processes and outputs of human decision-makers building and implementing the
algorithm. Andrew Selbst and co-authors similarly push for moving away from looking at
algorithms as abstract tools, and instead examining algorithms in the context of the complex
human systems in which they are embedded.40 They observe that “technical systems are

37

Selbst et al., above note 32, p. 7.

38

Citron, above note 9, p. 1271.

39

State v. Loomis, 881 NW.2d 749, 769 (Wis. 2016); State v. Loomis: Wisconsin Supreme Court

Requires Warning Before Use of Algorithmic Risk Assessments in Sentencing (2017) 30 Harv. L. Rev. 1530,
https://harvardlawreview.org/2017/03/state-v-loomis/.
40

Selbst et al., above note 32.

Electronic copy available at: https://ssrn.com/abstract=3622657

Draft, pre-edited version, forthcoming in Cambridge Handbook of the Law of Algorithms, ed. Woodrow
Barfield, Cambridge University Press (2020).

subsystems” embedded within larger contexts and organizations.41 Taking this “sociotechnical
frame” includes being aware of the added role of “decisions made by humans and human
institutions.”42 Relying on technological fixes alone will lead to a number of missed
opportunities and mistakes.43
Broadening the definition of the regulated system to include human users and
organizations has the added benefit of making algorithmic decision-making more easily
regulable. Law may not be good with technological black boxes, but it has a host of techniques
for, and long experience with, handling humans and organizations. This consequence has not
gone unnoticed. David Lehr and Paul Ohm chastise legal scholarship for focusing on the
running model of machine learning algorithms, rather than on the earlier stages of algorithmic
development that they refer to as “playing with the data.”44 While black boxes may be hard to
regulate, the humans who build and use black boxes are not. This aligns with instrumentalist
calls for regulating algorithms as or before they are built, rather than through after-the-fact
forms of individual due process.45
Thus, one target of accountability and transparency is the algorithm itself, while another
is the system of human decision-makers, or organizations and firms, around it. Making both
transparent is necessary (but again, not sufficient) for the goals of algorithmic accountability.
Yet even this broadened framing misses a crucial insight about what systems need to be made
visible and accountable.

41

Selbst et al., above note 32, p. 2.

42

Ibid., p. 3.

43

Ibid., pp. 3–8.

44

Lehr and Ohm, above note 8, p. 658 (“widening the view . . . will be crucial for solving some

seemingly intractable problems of our increasingly automated world”).
45

Kroll et al., above note 1, p. 659.

Electronic copy available at: https://ssrn.com/abstract=3622657

Draft, pre-edited version, forthcoming in Cambridge Handbook of the Law of Algorithms, ed. Woodrow
Barfield, Cambridge University Press (2020).

Much of the literature on algorithmic accountability relies implicitly on delegating at
least some governance of algorithms to the private sector and other third parties.46 That is,
these proposals suggest that governments leave the law a little vague and ask private parties,
formally or informally, to come up with ways of applying principles or standards in their
particular sectors or applications. Many proposals also involve civil society and academia and
the public at large in monitoring and enforcing these delegations.
These are the techniques of what is known in regulatory theory as “collaborative
governance” or “new governance”: partnerships between the public and private sector.47 These
techniques include formal collaboration (for example, the establishment of codes of conduct
subject to the approval of government) and informal collaboration (for example, the use of
standards rather than rules, which effectively delegates implementation of the details of
regulation to the private sector).48
A smaller but growing group of scholars calls more explicitly for, and thus directly
evaluates, these collaborative governance techniques.49 I have observed, too, the GDPR’s
extensive use of collaborative governance in governing algorithmic decision-making.50 Even

46

Kaminski, above note 6, p. 129.

47

See, e.g., O. Lobel, The Renew Deal: The Fall of Regulation and the Rise of Governance in

Contemporary Legal Thought (2004) 89 Minn. L. Rev. 342, 371–6.
48

Kaminski, above note 6, pp. 136–42.

49

R. Binns, Data Protection Impact Assessments: A Meta-Regulatory Approach (2017) 7 Int’l Data

Privacy L. 22, 29–30; M. Guihot, A. Matthew, and N. Suzor, Nudging Robots: Innovative Solutions to Regulate
Artificial Intelligence (2017) 20 Vand. J. Ent. & Tech. L. 385, 427; M. Perel and N. Elkin-Koren, Accountability
in Algorithmic Copyright Enforcement (2016) 19 Stan. Tech. L. Rev. 473, 529–31 (“We advocate a
collaborative-dynamic regulation . . .”); W. Nicholson Price II, Regulating Black-Box Medicine (2017) 116
Mich. L. Rev. 421, 465–71 (discussing collaborative governance of black box, medical algorithms).
50

Kaminski, above note 6, pp. 167–83.

Electronic copy available at: https://ssrn.com/abstract=3622657

Draft, pre-edited version, forthcoming in Cambridge Handbook of the Law of Algorithms, ed. Woodrow
Barfield, Cambridge University Press (2020).

where not explicit, much of what has been proposed as solutions for algorithmic accountability
in fact entails collaborative governance.
This leads to a crucial observation: transparency is necessary for algorithmic
accountability not just because it is a good in itself, but because it is necessary for effective
collaborative governance. For collaborative governance to work, there has to be external input
and oversight, or partnerships with the private sector give way easily to regulatory capture. 51
When algorithmic accountability proposals call for the involvement of civil society or for
public oversight, they thus sound in longstanding discussions of how to make public-private
partnerships accountable and effective. This role of transparency in producing oversight not
over the algorithm but over the people effectively tasked with governing it, however, is largely
ignored in the literature.
Thus, the algorithmic accountability literature often conflates two levels of
transparency: what I call first-order and second-order transparency.52 First-order transparency
targets the algorithm and human decisions about its construction. Second-order transparency
focuses on making the system of governance itself transparent and accountable. The first aims
to make visible the rationales behind, and biases in, algorithmic decision-making. The second
aims to watch, and make accountable, the watchmen.
There are thus not one, not two, but three target systems at which transparency can be
aimed. The first is the technology. The second is the human systems around the technology.
And the third is the governance regime, which aims to impact and alter both the technology

51

Ibid., p. 138 (“Critics observe that such a regime can easily become subject to collusion or capture”).

52

Kaminski, above note 6, p. 108 (“Because regulators will be delegating rulemaking of sorts to private

parties, we need not just transparency and oversight over the algorithm, but second-order transparency and
oversight over that rulemaking and compliance process”).

Electronic copy available at: https://ssrn.com/abstract=3622657

Draft, pre-edited version, forthcoming in Cambridge Handbook of the Law of Algorithms, ed. Woodrow
Barfield, Cambridge University Press (2020).

and these human systems. When we discuss algorithmic transparency, we need to evaluate not
only why we want transparency, but what system we are aiming it at.

A Taxonomy of Transparency for Accountable AI

Most calls for algorithmic accountability propose a wide array of information flows and types,
beyond calling for everything to be released to the public. Yet public-facing transparency (of
source code, of data sets) has unfortunately been turned into a strawman in this discussion, full
of harms to businesses, national security, and individuals in the data sets. 53 This dismissal of
transparency in general by dismissing full transparency to the public both oversimplifies
existing policy proposals, and mischaracterizes what transparency is.54 The literature on
algorithmic accountability is awash in calls for various forms and degrees of transparency, from
public disclosure to internal oversight to stakeholder involvement, auditing, and expert boards,
and nearly everything in between.55 Many, in fact most, of these versions of transparency do
not raise the kinds of harms its detractors are concerned about. Moreover, in practice, purported
transparency detractors often end up calling for policy solutions that fall squarely within a
transparency toolkit.
For example, Frank Pasquale calls for a system of “qualified transparency,” with
different depths, types, and quantities of information going to different actors.56 Andrew Tutt

53

Kroll et al., above note 1, pp. 657–8.
Many discussions of transparency acknowledge that information flows occur on a spectrum of
disclosure, and are not merely on or off. See, for example, Andrew Keane Woods, The Transparency Tax, 71
Vanderbilt L. Rev. 1, 16 (observing that the legal system is “semitransparent in different ways,” and laying out a
taxonomy of transparency in lawmaking) (2018).
54

55 Citron, above note 9, pp. 1305–13; Citron and Pasquale, above note 9, pp. 18–30.
56 Pasquale, above note 7, pp. 140–88.

Electronic copy available at: https://ssrn.com/abstract=3622657

Draft, pre-edited version, forthcoming in Cambridge Handbook of the Law of Algorithms, ed. Woodrow
Barfield, Cambridge University Press (2020).

has called for a similar “Spectrum of Disclosure.”57 Tal Zarsky’s exhaustive work on
transparency in governmental use of algorithms charts nearly every possible mode of disclosure
and recipient.58 Calls for requiring algorithmic impact assessments, too, focus on creating
information flows – within a company, to stakeholders, to experts, and to the general public.59
Selbst and Barocas more recently have called for documentation requirements, similar to the
GDPR’s recording requirements, to force companies building algorithms to chart what
decisions they have made and why.60 Even these recording requirements can be understood as
a version of transparency, as they establish information flows within a company, and
potentially later to those who regulate or sue it.
To my knowledge, I am the first to think comprehensively about the question of
transparency and algorithmic accountability in light of the first-order/second-order issues
discussed above. That is, while transparency has been thrown at the problem of algorithmic
accountability with near abandon, few if any are thinking about the interactions between
different kinds of transparency, aimed at different goals and different targets of regulation.

57

Tutt, above note 7, p. 110 (table 3).

58

Zarsky, above note 7, pp. 1521–30.

59

A. D. Selbst, Disparate Impact in Big Data Policing (2017) 52 Ga. L. Rev. 109, 169–72; AI Now

Institute, Algorithmic Impact Assessments: Towards Accountable Algorithms in Public Agencies, Medium
(February 21, 2018), https://medium.com/@AINowInstitute/algorithmic-impact-assessments-towardaccountable-automation-in-public-agencies-bd9856e6fdde; M. E. Kaminski and G. Malgieri, Algorithmic
Impact Assessments under the GDPR: Producing Multi-Layered Explanations (October 6, 2019),
https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3456224; A. Mantelero, AI and Big Data: A Blueprint for a
Human Rights, Social and Ethical Impact Assessment (2018) 34 Computer Law & Security Review 754,
https://doi.org/10.1016/j.clsr.2018.05.017.
60

Selbst and Barocas, above note 12, pp. 1129–38.

Electronic copy available at: https://ssrn.com/abstract=3622657

Draft, pre-edited version, forthcoming in Cambridge Handbook of the Law of Algorithms, ed. Woodrow
Barfield, Cambridge University Press (2020).

Transparency takes many different shapes and sizes. When it is implemented for
multiple reasons, and targeted at multiple systems, complex gaps and overlaps emerge. These
are visible only from a holistic perspective.
With this in mind, I suggest we think about transparency in algorithmic accountability
in the following way. There is individualized transparency, and there is systemic
transparency.61 The first largely aims – along with accompanying substantive rights of
contestation – at dignitary and justificatory goals; the second, at instrumental but also systemic
justificatory goals. We should be able to articulate what is going to whom, why, when, and
how.
We should additionally be aware of the distinction between first-order transparency,
aimed at letting somebody see into the system (whether the system is the technology or the
system is the human decision-makers and users around it), and second-order transparency,
which is aimed at making sure delegations of governance of algorithms to the private sector
remain accountable rather than captured.
In the following paragraphs, I lay out several examples of how this taxonomy works.
Then, in Tables 5.1, 5.2, and 5.3 below, I provide a larger array of examples. Much, though not
all, of my perspective is influenced by the European Union’s GDPR.
From all of this, an insight emerges: often, one type of transparency (say, individual
access rights) will be relied on to do more than one type of work (say, both empowering

61

Kaminski, above note 6, p. 105 (“governing algorithmic decision-making should include both

individual rights and systemic approaches”).

Electronic copy available at: https://ssrn.com/abstract=3622657

Draft, pre-edited version, forthcoming in Cambridge Handbook of the Law of Algorithms, ed. Woodrow
Barfield, Cambridge University Press (2020).

impacted individuals and helping to ensure the regulatory system is working toward the public
good, rather than captured).62
Individualized transparency consists of information flows targeted at the individual
impacted by algorithmic decision-making. For example, a person impacted by a lending
decision might be provided an explanation of that decision, at an abstract enough and simple
enough level so as to be understandable, but also complex enough to be actionable, to allow
her to contest the decision. The goal of this kind of transparency is not just to enable a person
to change her behavior, but also to empower her, to protect her dignity and increase her
autonomy, and to make visible the system’s reasoning – to justify it.63 This is a very different
purpose than, say, the kind of transparency one might put in place to best enable expert
oversight. Yet if the algorithmic accountability regime as a whole lacks other transparency
mechanisms, a right to explanation, such as that required by the GDPR’s Article 22, may have
to do dual or even triple work.64 An explanation may have to simultaneously justify a system
to the impacted individual, serve as broader oversight over the system, and even serve as a
form of accountability over private governance of algorithmic decision-making. It is unlikely
that one transparency instrument can effectively accomplish all three things.
Systemic transparency, by contrast, aims to make visible error, bias, and discrimination
in both machine and human systems, so they can be addressed and mitigated, if not corrected.

62

Ibid., p. 106 (“we may find ourselves needing to rely on individual transparency rights to accomplish

systemic accountability goals”).
63

Kaminski, above note 6, p. 106 (describing “a justificatory rationale, concerned with ensuring that

decisions are made based on socially and legally acceptable reasoning and are legitimized by acceptable process
or oversight”); Selbst and Barocas, above note 12, p. 1118. For a different take, limiting individual explanations
to counterfactuals, see S. Wachter, B. Mittelstadt, and C. Russell, Counterfactual Explanations without Opening
the Black Box: Automated Decisions and the GDPR (2018) 31 Harv. JL & Tech. 841.
64

Kaminski, above note 6, p. 106.

Electronic copy available at: https://ssrn.com/abstract=3622657

Draft, pre-edited version, forthcoming in Cambridge Handbook of the Law of Algorithms, ed. Woodrow
Barfield, Cambridge University Press (2020).

These information flows go to more than one type of person or actor. For example, a board of
technical experts may get access to an algorithm’s source code, training data sets, and
interviews with the data scientists designing the system. In another mode of transparency, a
company might be required to convene civil society members and stakeholders to disclose (and
provide input into) policy decisions such as how to define discrimination.65 In yet another mode
of systemic transparency, companies might be required to record decisions for possible later
inspection, or to assemble reports or impact assessments. They might be required to put in place
internal information flows between engineers and, say, a company’s lawyers or privacy officer.
Many of these modes of systemic transparency will never involve releasing information to the
public. Others will set up records that might be released to government regulators or to
members of the public later on.
These systemic forms of transparency differ in their temporal design, as well. Some of
these modes of transparency will be more static, requiring a single report or a single meeting
or set of meetings with a defined end point. Others will be closer to continuous, either in the
form of regular spot-checks or constant oversight.
All of this is similar to Pasquale’s concept of qualified transparency: different actors
receive information of different depth or breadth or duration, depending on what the purpose
of a particular disclosure is.66 The overarching goal of systemic transparency is to provide
visibility into and oversight over both technical systems (the algorithm) and human systems
around them, largely to uncover and hopefully try to fix problems of error and bias. For a

65

Kim, above note 9, p. 193; Lehr and Ohm, above note 8, p. 705 n. 187.

66

Pasquale, above note 7, pp. 140–88.

Electronic copy available at: https://ssrn.com/abstract=3622657

Draft, pre-edited version, forthcoming in Cambridge Handbook of the Law of Algorithms, ed. Woodrow
Barfield, Cambridge University Press (2020).

variety of reasons, scholars are more confident that this approach will work to address
instrumental concerns about algorithms than individualized transparency.67
Thus far, however, all of these types of transparency are aimed at seeing into a
governed object, organization, or process. That is, I have discussed them thus far as operating
only as first-order transparency. If, as many proposals on algorithmic accountability suggest
at least tacitly, we are going to defer to some private sector decision-making about how to
correct social problems with algorithms, there needs to be an eye toward not just seeing into
the governed system, but watching the private actors who are tasked with implementing or
even coming up with the law.
This is where disclosures to the public and to third parties become essential. These
third-party actors are necessary, in a system of collaborative governance, to check private
actors from acting only in their own self-interest. They are also necessary as a force multiplier
for government actors; collaborative governance works best when other voices and expertise
outside of government can do some of the enforcement and oversight work, too.
This is also where many forms of systemic transparency end up serving multiple goals.
Take again the example of stakeholders and civil society members convening to provide
oversight over, and possibly input into, a company’s decision of how to define what it means
by discrimination. On the one hand, this is clearly a systemic form of accountability aimed at
instrumentally making the algorithm and the systems around it less discriminatory. On the
other, third-party transparency also plays the role of checking company discretion and ensuring
company decisions do not stray too far from the public good.68 Similarly, external audits by

67

See, e.g., Edwards and Veale, above note 2, p. 67 (describing a possible “transparency fallacy”).

68

I. Ayres and J. Braithwaite, Responsive Regulation (1992), pp. 57–60; I. Ayres and J. Braithwaite,

Tripartism: Regulatory Capture and Empowerment (1991) 16 L. & Soc. Inquiry 435, 491 n. 137; see also M. E.

Electronic copy available at: https://ssrn.com/abstract=3622657

Draft, pre-edited version, forthcoming in Cambridge Handbook of the Law of Algorithms, ed. Woodrow
Barfield, Cambridge University Press (2020).

third parties or oversight by external experts serve both to correct problems with the algorithm
itself, and as a means of watching, and thus checking, a company’s behavior as a self- or coregulator.
This insight about the often dual, or triple, or even quadruple purposes (when you add
in dignity) served by transparency in these proposed governance regimes has practical
significance. Take, for example, the conversation over algorithmic impact assessments. Under
Europe’s GDPR, impact assessments are on the face of the GDPR envisioned largely as an
internal process of oversight and risk mitigation, triggering direct regulatory oversight only in
certain high-risk scenarios.69 They are not required to be released to the public, although this
is encouraged as a best practice.70 The non-public version of an impact assessment largely
serves instrumental ends: it requires companies to mitigate risks to impacted individuals,
including through disclosing information to external experts or auditors.71
But non-public impact assessments fail when it comes to second-order transparency.
While disclosures to third-party experts can do some of the work of holding a company
accountable as it self-regulates, they don’t do nearly as much as releasing the report to the
public, or to other interested stakeholders. Release to the public triggers not just public scrutiny,
but the possibility of non-legal sanctions such as public shaming and market consequences.

Kaminski, Regulating AI Risk through the GDPR (October 22, 2019), pp. 31–2 (unpublished manuscript, on file
with author).
69

Council Regulation 2016/679, 2016 OJ (L 119) 1 (EU), arts. 35–6. Kaminski, above note 6, p. 176 n.

70

Article 29 Data Prot. Working Party, Guidelines on Data Protection Impact Assessment (DPIA) and

393.

Determining Whether Processing Is “Likely to Result in a High Risk” for the Purposes of Regulation 2016/679,
WP248 rev.01 (April 4, 2017), p. 16.
71

Article 29 Data Prot. Working Party, Guidelines on Automated Individual Decision-Making and

Profiling for the Purposes of Regulation 2016/679, WP251 rev.01 (February 6, 2018).

Electronic copy available at: https://ssrn.com/abstract=3622657

Draft, pre-edited version, forthcoming in Cambridge Handbook of the Law of Algorithms, ed. Woodrow
Barfield, Cambridge University Press (2020).

These non-legal enforcement mechanisms are crucial for any attempt, formal or informal, to
delegate regulation to the private sector. They increase regulators’ practical enforcement
capacity, as well as harnessing both technical expertise and legitimizing voices outside of the
regulated sector and the government.
A FINAL NOTE ON SUBSTANCE

No chapter on transparency would be complete without an acknowledgment of its very
real limitations. Transparency is arguably necessary, but not sufficient, for algorithmic
accountability, as it is for many other things. As the famed Panopticon illustrates, transparency
by itself can sometimes lead to changes in behavior, whether from fear of social shaming or a
tendency to conform to the perceived majority’s norms.72 Sometimes a chilling effect or
conforming effect is desirable. And sometimes, a chilling effect alone is not enough.
What good is an individual right to explanation, if an individual doesn’t also have a
right to contest a decision?73 What good is a regulator’s ability to inspect a company’s
documents, if it lacks the ability to issue substantial fines? Or, what good is the ability to
observe that a company is discriminating, if you lack a substantive right to challenge that
discrimination?
Once again, I turn to Europe in determining what some of transparency’s substantive
legal backstops might look like. A regulator might be given both information-forcing
capabilities and the substantive ability to enforce against certain practices, such as the ability
to levy large fines. An individual might be given both a right to meaningfully agree to be subject
to a system, and a right to opt out of it. She might be given correction rights, contestation rights,
72

M. E. Kaminski and S. Witnov, The Conforming Effect: First Amendment Implications of

Surveillance, beyond Chilling Speech (2015) 49 U. Rich. L. Rev. 465, 466–7.
73
Margot E. Kaminski & Jennifer Urban, The Right to Contestation, draft on file with the author.

Electronic copy available at: https://ssrn.com/abstract=3622657

Draft, pre-edited version, forthcoming in Cambridge Handbook of the Law of Algorithms, ed. Woodrow
Barfield, Cambridge University Press (2020).

and the right to a human in the loop, or human decisions, or human review. She might have a
substantive right not to be subjected to decisions based on particular categories of sensitive
data (for example, health data, or her political views). She might be given a substantive right
not to be discriminated against based on particular immutable, or otherwise protected,
characteristics.
Thus, this discussion of transparency in isolation is admittedly limited. And it may be
temporally limited, as well. It may be that someday clearer best practices and standards develop
in these sectors such that we need not lean so heavily on a collaborative regulatory approach.
When that time comes, it may be worthwhile to simplify: to give individuals a simple cause of
action if they have been subjected to what we ultimately determine to be bad or badly designed
algorithmic decision-making. But until then, transparency is an essential component of the kind
of regulatory approach most scholars, knowingly or unknowingly, have proposed.

Begin Table 1
Table 5.1
Individual Transparency

Access to
profiling data
held about an
individual74

74

To Whom?

Why?

What?

When?

Impacted
individuals

So an individual
can know
what
information is
held about
her; to respect
her dignity; so
she can
correct the
accessed
information or
opt out of
processing

Personal data in a
format
comprehensible
to lay persons
Inferences made
about an
individual

At reasonable
intervals

See Council Regulation 2016/679, 2016 OJ (L 119) 1 (EU), art. 15.

Electronic copy available at: https://ssrn.com/abstract=3622657

How?
Access
requests
(pull)

Draft, pre-edited version, forthcoming in Cambridge Handbook of the Law of Algorithms, ed. Woodrow
Barfield, Cambridge University Press (2020).
Disclosure that
one has been
or will be
subjected to an
algorithmic
decision
system

Impacted
individuals

An explanation
of the model
(modelcentric75)

Impacted
individuals

An explanation
of a particular
decision
(subjectcentric78)

Impacted
individuals

75

So an individual
knows she
may be
subjected/has
been
subjected to
algorithmic
decisionmaking; to
respect her
dignity; to
allow her to
opt out
So an individual
can
meaningfully
agree to be
subject to the
system (or opt
out of it).
So an individual
can
assess/contest
the legitimacy
of broader
decisionmaking
heuristics
To respect the
dignity of
impacted
individuals
To provide
insight into
whether the
system’s
decision-

A disclosure
statement that
one is being
subjected to
algorithmic
analysis or
decision-making

When the
processing
entity knows
a person is
being
subjected to
its system

Affirmative
notification
(push)

A description
intelligible to
laypersons
Information about:
the family of
model, input data,
performance
metrics, how the
model was
tested76
An interface for
meaningfully
experimenting
with the system77

Before a
decision is
made

Affirmative
notification
(push)
Access (pull)

An explanation
intelligible to
laypersons,
including to the
extent possible
statistics used,
policy decisions
made, and
decision-making
heuristics

After a decision
is made, but
before it is
implemented

Affirmative
notification
(push)

Edwards and Veale, above note 2, p. 22 (“we identify two types of algorithmic explanations: model-

centric explanations (MCEs) and subject-centric explanations (SCEs)”). But see A. D. Selbst and J. Powles,
Meaningful Information and the Right to Explanation (2017) 7 Int’l Data Privacy L. 233, 239–40 (arguing that
often a meaningful system-level explanation will yield information about specific decisions).
76

Edwards and Veale, above note 2, p. 55.

77

Citron and Pasquale, above note 9, pp. 28–30; M. Hildebrandt, The Dawn of a Critical Transparency

Right for the Profiling Era, in J. Bus, M. Crompton, M. Hildebrandt, et al. (eds.), Digital Enlightenment
Yearbook 2012 ( 2012), pp. 53–4. For a similar suggestion in the copyright law context, see M. Perel and N.
Elkin-Koren, Black Box Tinkering: Beyond Disclosure in Algorithmic Enforcement (2017) 69 Fla. L. Rev. 181,
190–200.
78

Edwards and Veale, above note 2, p. 22.

Electronic copy available at: https://ssrn.com/abstract=3622657

Draft, pre-edited version, forthcoming in Cambridge Handbook of the Law of Algorithms, ed. Woodrow
Barfield, Cambridge University Press (2020).
making is
legitimate
So an individual
can correct,
contest, or
invoke related
rights.

Subject-centric
information:
counterfactuals,79
characteristics of
others similarly
classified, the
confidence the
system has in a
particular
outcome80

End Table 1
Begin Table 2
Table 5.2
Systemic Transparency
To Whom?

Why?
To check the
system (for
bias,
discrimination)
To provide
systemic
legitimacy

Third-party
auditing

Independent
auditors

Disclosure to
expert boards

Board of external
experts
Technical/legal/ethic
al expertise

To check the
overall system
(for bias,
discrimination)
To provide
systemic
legitimacy

Recording
Requirements

Internal company
actors

To check the
overall system

79

Wachter et al., above note 63, p. 880.

80

Edwards and Veale, above note 2, p. 58.

What?
All
informatio
n
necessary
for the
audit to be
conducted,
potentially
including
source
code and
data sets,
performan
ce metrics
All
informatio
n
necessary
for the
board to
provide
substantiv
e input
(see
above)
Information
about
substantiv

When?
When the
system is
being
designed;
when the
system is
operating
At regular
intervals
Ongoing
When the
system
substantively
changes
At design
stages
(“playing
with the
data”)
For
performance
review

How?
Affirmative
obligation
(push)
In response
to auditors’
requests
(pull)

As a system is
designed

Information
flow within
a company

Electronic copy available at: https://ssrn.com/abstract=3622657

Affirmative
obligation
(push)
In response
to board
requests
(pull)

Draft, pre-edited version, forthcoming in Cambridge Handbook of the Law of Algorithms, ed. Woodrow
Barfield, Cambridge University Press (2020).
Regulators (either a
system of
recording/reportin
g, or in response to
informationforcing/discovery
capabilities)

To alter
company
decisionmaking
heuristics

Impact
Assessments

Other members of
the company
Stakeholders (both
impacted
individuals and
civil society)
The public

To check the
system
To risk mitigate
To provide
legitimacy

Disclosure to
impacted
stakeholders

Impacted
stakeholders or
civil society
representatives

To check the
system
To provide
legitimacy
To
identify/contes
t company
policy
decisions

e design
decisions81

When a system
is
substantively
changed
Ongoing
(continuous)

(push and
pull)
In response
to
regulator
requests
(pull) or
affirmative
obligation
(push)

All info
necessary
for each
involved
actor to
play her
part in
effective
risk
mitigation/
human
rights
impact
assessment
Both expert
and nonexpert
informatio
n

Before a
system is
implemented
Ongoing
Substantive
changes

Info flow
within a
company
(push and
pull)
Affirmative
obligations
(push)

Before a
system is
implemented
Substantive
changes

Affirmative
obligations
(push)
Stakeholder
requests
(pull)

End Table 2
Begin Table 3
Table 5.3
Second-Order Transparency
To Whom?
Third-party
auditing

81

Independent
auditors

Why?
To ensure a
company’s
involvemen
t in selfregulation
is effective,

What?
All information
necessary for
the audit to
provide
substantive
oversight over

When?
When the
system is
being
designed;
when the
system is
operating

Selbst and Barocas, above note 12, pp. 1129–38.

Electronic copy available at: https://ssrn.com/abstract=3622657

How?
Affirmative
obligation
(push)
In response
to
auditors’

Draft, pre-edited version, forthcoming in Cambridge Handbook of the Law of Algorithms, ed. Woodrow
Barfield, Cambridge University Press (2020).
and not
captured

Disclosure to
expert
boards

Impact
Assessmen
ts

Board of external
experts
Technical/legal/ethi
cal expertise

Stakeholders (both
impacted
individuals and
civil society)
The public

Disclosure to
impacted
stakeholde
rs

Impacted
stakeholders or
civil society
representatives

An
explanation
of a
particular
decision

Impacted individuals

To ensure a
company’s
involvemen
t in selfregulation
is effective,
and not
captured
To ensure a
company’s
involvemen
t in selfregulation
is effective,
and not
captured
To trigger
non-legal
enforcemen
t
To ensure a
company’s
involvemen
t in selfregulation
is effective,
and not
captured
In the
absence of
other forms
of external
transparenc
y, to ensure
a company’s
involvemen
t in selfregulation
is effective
and not
captured; to
trigger both
legal and
non-legal
enforcemen
t (e.g. press
coverage

company
policies

All information
necessary for
the board to
provide
substantive
oversight over
company
policies
All info
necessary for
external
actors to
ensure
company
policies are
not just selfserving, and
possibly to
provide
enforcement
Both expert and
non-expert
information

An explanation
intelligible to
laypersons,
including to
the extent
possible
statistics
used, policy
decisions
made, and
decisionmaking
heuristics
Subject-centric
information:
counterfactua
ls,
characteristic
s of others
similarly

At regular
intervals
Ongoing
When the
system
substantive
ly changes
At design
stages
(“playing
with the
data”)
For
performanc
e review
Before a
system is
implemente
d
Ongoing
Substantive
changes

requests
(pull)

Affirmative
obligation
(push)
In response
to board
requests
(pull)
Info flow
within a
company
Affirmative
obligation
s (push)

Before a
system is
implemente
d
Substantive
changes

Affirmative
obligation
(push)
Stakeholde
r requests
(pull)

After a
decision is
made, but
before it is
implemente
d

Affirmative
notificatio
n (push)

Electronic copy available at: https://ssrn.com/abstract=3622657

Draft, pre-edited version, forthcoming in Cambridge Handbook of the Law of Algorithms, ed. Woodrow
Barfield, Cambridge University Press (2020).

An
explanation
of the
model

Impacted individuals

and
shaming)

classified, the
confidence
the system
has in a
particular
outcome

To help
ensure a
company’s
involvemen
t in selfregulation
is effective
and not
captured

A description
intelligible to
laypersons
Information
about: the
family of
model, input
data,
performance
metrics, how
the model
was tested
An interface for
meaningfully
experimentin
g with the
system

Before a
decision is
made

End Table 3

Electronic copy available at: https://ssrn.com/abstract=3622657

Affirmative
notificatio
n (push)
Access
(pull)

