Computers in Human Behavior 98 (2019) 277–284

Contents lists available at ScienceDirect

Computers in Human Behavior
journal homepage: www.elsevier.com/locate/comphumbeh

Role of fairness, accountability, and transparency in algorithmic aﬀordance
a,*

Donghee Shin , Yong Jin Park

T

b

a

College of Communication and Media Sciences, Zayed University, United Arab Emirates
Media Management Sequence Coordinator, Department of Strategic, Legal, Management Communications, Graduate Department of Communication, Culture and Media
Studies School of Communications Howard University, 2400 Sixth Street NW, Washington, DC, 20059, USA
b

A R T I C LE I N FO

A B S T R A C T

Keywords:
Algorithms
Algorithm experience
Algorithm acceptance
Perceived transparency
Perceived fairness
Accountability
Aﬀordance

As algorithm-based services increase, social topics such as fairness, transparency, and accountability (FAT) must
be addressed. This study conceptualizes such issues and examines how they inﬂuence the use and adoption of
algorithm services. In particular, we investigate how trust is related to such issues and how trust inﬂuences the
user experience of algorithm services. A multi-mixed method was used by integrating interpretive methods and
surveys. The overall results show the heuristic role of fairness, accountability, and transparency, regarding their
fundamental links to trust. Despite the importance of algorithms, no single testable deﬁnition has been observed.
We reconstructed the understandings of algorithm and its aﬀordance with user perception, invariant properties,
and contextuality. The study concludes by arguing that algorithmic aﬀordance oﬀers a distinctive perspective on
the conceptualization of algorithmic process. Individuals’ perceptions of FAT and how they actually perceive
them are important topics for further study.

1. Introduction
The use of algorithms and analytics in society is drastically increasing. Nowadays, artiﬁcial intelligence (AI) increasingly arbitrates
decisions in our lives through a wide variety of implementations, such
as online machine learning recommender systems, tailored news aggregation services, credit scoring methods, and location-based services.
Advancements in algorithms provide unprecedented venues for breakthroughs in important decision-making ﬁelds, such as content curation,
health and safety, security, and public management (Shin, 2019).
Driven by the substantial amounts of big data that have become
available, algorithms have emerged as the new power agents in society
(Diakopoulos, 2016), as algorithm technology is drastically revolutionizing society and becoming an integral part of everyday life.
The rapid adoption of algorithm technologies has the potential to
greatly improve user experiences and human life, but poses problems
and challenges that must be addressed when such a system is widely
diﬀused and pervasively used in societies (Ettlinger, 2018). Although
algorithms have the potential to oﬀer increasingly sophisticated products and services, thorny topics arise such as fairness, accountability,
and transparency (FAT). The question as to whether an algorithm is fair
or does not discriminate, who should be held liable for the results of
algorithms, and how to ensure the purpose, structure, and underlying
actions of algorithms remain unsolved and controversial (Beer, 2017).

∗

These topics, including user privacy, data policy, and ethical considerations regarding how we design and develop the algorithms, will
be critical to its sustainability and long-term eﬀects (Ananny &
Crawford, 2018; Mittelstadt, Allo, Taddel, Wachter, & Floridi, 2016).
Despite the importance of such issues, little consensus is observed regarding a single, testable deﬁnition of the issues (Lee, 2018), and this
creates confusion for the academia and industry involved in algorithms.
On the basis of this context, this study aims to conceptualize FAT in
relation to the increasing use of algorithms and clariﬁes the roles of
such problems in the user acceptance of algorithm services. The following research questions guide this study:
RQ1. What are the normative and operational deﬁnitions of FAT in the
algorithm context? How do users perceive FAT and what constitutes
FAT?
RQ2. How and what does perceived FAT aﬀord users? How is FAT
related to other factors in the course of algorithm experience?
RQ3. How does FAT inﬂuence the adoption and diﬀusion of
algorithms? What are the roles of aﬀordance in algorithms?
This study views an algorithm as a socially recreated artifact based
on users' cognitions and contexts. FAT in the algorithm context may be
a subjective concept and is reconstructed through user cognitive processes (Just & Latzer, 2018). In fact, evaluating how transparent, fair,

Corresponding author.
E-mail addresses: Donghee.Shin@zu.ac.ae (D. Shin), yongjin.park@howard.edu (Y.J. Park).

https://doi.org/10.1016/j.chb.2019.04.019
Received 20 November 2018; Received in revised form 27 March 2019; Accepted 20 April 2019
Available online 23 April 2019
0747-5632/ © 2019 Elsevier Ltd. All rights reserved.

Computers in Human Behavior 98 (2019) 277–284

D. Shin and Y.J. Park

industry, and policymakers. Although governments and practitioners
have actively addressed the social and ethical issues arising from the
algorithmic society, questions remain (Just & Latzer, in-press) and are
controversial: ﬁrst, how can fair algorithms be designed and developed?
and second, how we can develop algorithms that are more transparent
and accountable (Lee, 2018)? Companies should be able to prepare for
these questions because discriminatory and opaque algorithms may
turn into serious risks, which lend urgency to a debate on how to develop AI transparent, fair, and accountable (Diakopoulos, 2016).
Fairness in algorithm contexts means that algorithmic decisions
should not create discriminatory or unjust consequences (Yang &
Stoyanovich, 2017). Fairness in algorithm is related to algorithmic bias,
which occurs when algorithms reﬂect the implicit values of the humans
who are involved in coding, programming to train the algorithm (Beer,
2017). Examples of unfair discrimination can be (1) banks providing
loans based on race, or gender and not on ﬁnancial score, (2) ﬁrms
hiring people based on race and not qualiﬁcations, and (3) realtors
renting houses to speciﬁc communities and not on capability. The
fairness in algorithms is from a matter of accuracy and a perspective of
fairness. Uncertainties have been increasing regarding the unfair
practices of algorithmic decision-making systems. Yet, algorithmic
fairness can be a complicated topic, because the deﬁnition of fairness is
largely contextual and subjective.
The concept of transparency involves the details of the service
reasoning, and of other types of data management, involving sensible
data, and/or possible consequences about the knowledge that the
system is gaining of the user implicitly (Ananny & Crawford, 2018). The
black box nature of algorithms refers to people not knowing the inner
operations of the algorithms because this information is proprietary
and/or suﬃciently complex to not be understood. Algorithmic transparency plays a key role in resolving the question of Facebook's role in
the Russian interference of the 2016 American Presidential Election.
The concepts of understandability and explainability become hot issues:
Can stakeholders interpret and understand the operating of a system
and its results? Users may forgo the need for complete and transparent
access to the underlying algorithm and the dataset if easily understandable information about the system is provided to them by a qualiﬁed, trustworthy expert or entity. When people understand how
system works, they are more likely to use the system properly and trust
the designers and developers (Lee & Boynton, 2017).
Accountability in algorithms and their application begins with the
designers and developers of the system that relies on them
(Diakopoulos, 2016). Eventually, designers and managers are responsible for the consequences or impacts an algorithmic system has on
stakeholders and society. Understanding the possibility of unintended
consequences is a critical condition when addressing algorithmic accountability. Normally, senior management is unaware of the business
risks inherent with the design decisions related to the algorithms their
business depends on. Conversely, algorithm designers are often not in a
position to make critical executive decisions on behalf of their business.
The result is an accountability loophole that goes unaddressed and
uncommunicated, leaving the business vulnerable to unexpected risks
for which they later maybe held responsible. Example of algorithmic
accountability is that when news recommendation systems generate
news articles that contain misinformation or libel, who are accountable
for the wrong content? To what extent algorithm producers or carriers
is responsible will be an elusive question.
Here it is important to point that the use of citizen data is closely
related to FAT. That is to say, how to collect people data in a transparent way, what to collect in a fair manner, and who are responsible
for data management are important considerations to address.
Eventually, these issues are dependent upon how citizens perceive FAT
because the more they believe their data would be treated fair, transparent and accountable way, the more they allow companies to collect
their data. Thus, societal attitudes toward algorithms are eventually
shaped through public discourse. However, public understanding is

and accurate are under the users' disposition (Kemper & Kolkman, inpress) and these are subjective in nature as they are dependent upon
contextual and user disposition, importantly related to users' trust (Bedi
& Vashisth, 2014). An argument that we posit in this study is that FAT is
largely based on the character of algorithm service, users’ intrinsic
traits, and user cognition (Shin, 2019). Users with higher levels of trust
were observed to be more likely to see algorithms as fair, accurate, and
transparent, while trust moderating the relationship between FAT and
satisfaction.
The results from this study contribute to the literature in three aspects. First, the results contribute to the insights and practical knowledge regarding the interactions between users and algorithms.
Algorithm services are increasingly featured with an ecosystem of
complex, socio-technical issues (Moller, Trilling, Helberger, & van Es,
2018). By conceptualizing and contextualizing FAT, this study contributes to the understanding of how to ensure such abstract issues in
algorithms, how to design algorithm systems that are human-centered
and socially-accountable, and how to govern algorithm systems eﬀectively and legitimately.
Second, the ecological interface approach advances the current user
literature (user-based experience model) by identifying contextual
variables and the underlying relations among them (Shin & Choi,
2014). Although a user-centered approach is useful, it has focused on
the individual user or a speciﬁc task in systems. The focus of the ecological interface approach is on context or environment; thus, it is
better suited to algorithm contexts. That is, an examination of how
users perceive algorithm functionality, how their intentions are carried
out, what cognitive perceptions are held, and what consequences from
cognitive process are derived is critical. The ﬁndings of this study,
particularly the ecological interface approach and experience-based
quality measurement, will enable future researchers to make important
strides in the formation of a User Experience (UX) framework.
Third, this study provides guidance to the design for algorithms and
future related algorithm services. The AI industry is striving to ensure
that algorithm-driven services are accurate and eﬀective. The industry
is being increasingly challenged by the development of improved FAT
and satisfaction indicators on which many important decisions are
based (Helberger, Karppinen, & D'Acunto, 2018). As more interesting
content and innovative services are introduced through algorithms, FAT
becomes a critical diﬀerentiator among diverse algorithm providers
(Diakopoulos, 2016). How users perceive and process FAT is considered
more important than the technical properties/qualities, such as accuracy and predictability (Lee, 2018).
The fundamental premise of this study is that algorithms' functional
features are embodied by users' cognitive perceptions eliciting perceived aﬀordance, which is moderated by trust factors. Aﬀordance thus
inﬂuences the cognitive processes of quality, usability, and satisfaction
(Shin & Biocca, 2018). From an interface and design perspective, algorithmic aﬀordance can be a key concept for user interface for algorithm services (Ettlinger, 2018). This study's results provide additional
details regarding the users' cognitive process of algorithms through the
aﬀordance frame. Understanding user aﬀordance facilitates the development of a user-centered interface for algorithms.
2. Literature review
2.1. FAT of algorithm
As the FAT (fairness, accountability, and transparency) of algorithms is of growing importance in the recent algorithm system, the
nature/deﬁnition of FAT and its operationalization become urgent tasks
in academia and industry. The issues have emerged from an abstract
ideology to a pressing contemporary matter in societies with drastic
market environment changes triggered by recent algorithm advancements. With the rapid advancement of impressive algorithms, AI has
created unprecedented opportunities, but also concerns from users,
278

Computers in Human Behavior 98 (2019) 277–284

D. Shin and Y.J. Park

and output the data in another form. The ability for users to understand
how an algorithm reached its conclusions can be challenging. Some
algorithms further restrict user understanding and use by reducing high
dimensional data into single dimensional list. If users have no means to
perceive an algorithm's aﬀordances, or no knowledge of its invariants,
the possibility for users to use it eﬀectively signiﬁcantly decreases.
Thus, algorithms fundamentally change the nature of the cognitive
demands and responsibilities of the ﬁrms, and often in ways that were
unintended or unanticipated by their developers. Algorithm ﬁrms can
design aﬀordances into their algorithms to facilitate their use.
Algorithms must provide some level of observability to users, that is, an
aﬀordance they can perceive. Providing explainable observability to the
users is a necessary ﬁrst step to make algorithms more useable and
accessible. The key is to oﬀer the user heuristics into the algorithm's
process. A means to accomplish this task would be to provide information about the algorithm's training set and its provenance, size,
variability, and any known limitations. This information provides a
rationale for the algorithm's output. The algorithm could include explanations for individual outputs. For example, recommendation algorithms could identify which features lead to the selection of music or
news. Another local approach would be to provide a conﬁdence rating
for algorithm outputs. Similar to humans, algorithms have response
criteria, that is, they let users know if an output exceeded the response
criteria by a large or small amount, which allows users to know to how
more eﬀectively interact with the algorithm. Algorithm developers
must provide operators with control points to direct input, transformation, and output processes. The input side could include the new
cases into the training data set, or mechanisms for providing feedback
to the utility of algorithm outputs. Transformation control options
could result in changes to feature weighting or approaches used to build
the algorithm. Finally, with respect to outputs, developers could provide means for users to personalize or customize algorithm outputs to
better accomplish their tasks, that is, tailoring them to their goals and
needs.
Although integrating aﬀordances into algorithms will not be easy,
the task is not impossible. If the algorithms used by automation are
highly complex and diﬀerent from human cognition or not understood
by humans, the algorithmic results will not be accepted. To take full
advantage of algorithm tools, we must make the algorithm's opportunities for action observable and understood by its users (Shin, 2019).
Without these improvements, algorithms will not realize their potential
across industries for improving forecasting and predictive analytics.

limited by a technical barrier. Although algorithms are technologically
complex, the public may want to understand an algorithm's inner
workings, the black box of the algorithm. The public is eager to precisely know how their data are collected and how the input data are
used to produce outputs. Citizens expect visibility and opacity in the
algorithmic process. The right to explanation by the EU General Data
Protection Regulation, a right to obtain an explanation of speciﬁc algorithmic results are in line with such citizens' motivation as to FAT.
Users want to have a just and equitable means to participate in the
evolution of algorithms.
2.2. What matters in designing algorithms, users, process, and technology?
We re-conceptualize algorithm as a socio-technical system that
handles human interaction with technological systems (Kitchin, 2017;
Shin, 2019). Algorithm systems comprise one or more technological
algorithms, services, platforms, user knowledge and social experience,
and interaction with users. What establishes an algorithm system as a
socio-technical system is that it is generated by or related to a system
adopted and used by social users in societies (Shin & Choi, 2014). Although FAT has been approached from various disciplines and perspectives, research from the user perspective is limited. An approach to
understanding user values not typically reﬂected in economic analyses
or market-based indices.
Accordingly, this study argues that the algorithm debate should
focus on user interests, because public users should be the eventual
beneﬁciaries. Analytical tools for assessing social impacts of technologies tend to focus on microeconomic approaches such as cost-beneﬁt
analyses. As social values are not easily expressed in economic analyses,
speciﬁc methods of establishing values, such as public value mapping,
are adequate tools. This study approaches FAT from the individuals'
perspective, also known as user-centered design or user-centered policy
evaluation (Baumer, in-press). Per Baumer's argument (in-press), policy
design should be based on user perspective, and its evaluation should
be performed by users. The need for this user-centered approach is
apparent and because the existing approaches to algorithm analysis
(although powerful and eﬀective) do not suﬃciently analyze the causal
impacts between policy and user perspectives.
Therefore, user-centered value mapping can provide insights and
implications on the governance and design of algorithms. Regarding the
governance, a user-centered view can improve policy capacity by involving individuals not typically involved in the policy-making process.
Regarding the design of algorithms, the user-centered approach provides insights into the design of algorithm-driven services. This way,
the user-centered approach plays a formative role by helping to continually reﬁne and update policies and a summative role by helping to
ascertain whether goals and objectives are being satisﬁed. Considering
the public nature and magnitude of algorithms, considering user-centeredness is useful, speciﬁcally under complex socio-technical situations.

3. Hypotheses development
In this study, FAT is proposed as an antecedent variable aﬀecting
satisfaction and trust is hypothesized as a moderator inﬂuencing the
relation path (Fig. 1). There have been growing concerns that
Fairness

2.3. Algorithmic aﬀordance: from blackbox algorithms to interactive
aﬀordances
Despite technical sophistication, algorithms rarely provide useful
means for users to interact with them (Fink, 2018). In other words,
algorithms themselves do not have the aﬀordance that would allow
users to understand them or how best to utilize them to achieve their
goals. Aﬀordances require the user to detect an object's invariants (e.g.,
functional properties) relative to the user's capabilities (Shin, 2017).
Algorithmic aﬀordances should provide users opportunities for action
that people can perceive with respect to features in their environment
such as fair, transparent, and accountable.
From a user's perspective, the algorithm is a “black box,” and it is
not possible for the user to know how the computation is completed
(Shin, 2019). Algorithms collect data from users, transform the data,

U

Satisfaction

Accountability

Transparency
Trust U

Fig. 1. FAT conceptual model.
279

Computers in Human Behavior 98 (2019) 277–284

D. Shin and Y.J. Park

Fairness

extended eﬀort to justify a decision. Consequently, users will be more
likely to use and be satisﬁed when algorithms are held accountable for
outcomes, which could reduce negative results.

Indiscrimination
Impartiality
Accuracy
Responsibility
Auditability
Equity

H3. Users' assessment of accountability positively inﬂuences user
satisfaction with algorithms.
3.1. Moderating role of trust

Understandability
Explanability
Observability

Accountability

Much of the research examining the role of trust is based on the
assumption that trust about a service/product mitigates the uncertainties and risks related to vulnerabilities such as information
sharing, legal issues, and privacy (Shin, 2010). Hence, a higher level of
trust in a speciﬁc algorithm service is posited to increase the likelihood
of potential adopters to take risks inherent in adopting algorithm services; thus, greater trust facilitates the greater use and willingness in the
experience of algorithms. To test the moderating role of trust, the split
sample approach was used: this study divided respondents into two
groups based on their prior trust level. Respondents were asked to express their trust, credibility, and beliefs of general algorithm-driven
services. The trust measurements of Bedi and Vashisth (2014) were
used, and the median value was used to divide the respondents into
high and low trust groups.

Transparency

Fig. 2. FAT conceptual model.

algorithms raise new challenges for ensuring transparency, non-discrimination, and due process in decision-making (Lee, 2018). In particular, such factors have been frequently discussed in the design and
development of a recommendation system and algorithm-driven services in general (Diakopoulos, 2016). A recommendation system is
designed to provide accurate recommendations (Klinger & Svensson, InPress). Whether such recommended results really reﬂect user preferences or how the processes are conducted remain open questions
(Kitchin, 2017). Thus, transparency and fairness emerge as the most
critical factors in recommendation systems (Diakopoulos & Koliska,
2016) (see Fig. 2).
Understanding the relationship between the input to the system and
output allows us to start a predictable and eﬀective interaction with the
system. Users are inclined to trust the system because they know how
the data are analyzed and thus how recommendations are generated.
With transparent processes, users can revise the input to improve recommendations. Algorithm users can understand the logic and process
of recommendation systems.
The algorithm developers should do their utmost to ensure results
are accurate and legitimate. Transparency and fairness play signiﬁcant
roles in algorithms by improving user trust in an algorithm (Ananny &
Crawford, 2018). When transparent and accurate services are ensured,
users are more likely to view the news in more engaged manners.
Highly transparent recommendation systems can grant users a sense of
assurance; additionally, accurate news aﬀords users a sense of trust. The
user understanding of why and how a particular recommendation is
generated was found signiﬁcant. A thought could be that the high visibility and transparency for relevant feedback helped search performance and satisfaction with system. Thus, the following hypotheses are
proposed:

4. Method
With the rise of algorithms, growing emphasis has been placed on
the issues of FAT to ﬁnd means to provide more stable and socially
desirable services. The research questions (RQs) are addressed with
diﬀerent methods and approaches to triangulate the ﬁndings. For RQ1
(what constitutes FAT), interpretive methods such as in-depth interviews and critical methods were used. For RQ2 and RQ3 (examine the
FAT-based UX model of the algorithm), a survey was conducted to
identify the relations of factors and ﬁnd a user model based on experience.
This study utilized a triangulated mixed method design to understand the FAT issues in algorithms. The quantitative data analysis examines factors in users' perceptions and experiences. The qualitative
data analysis identiﬁes users’ underlying thinking regarding FAT
(Table 1) (see Table 2).
5. Findings
5.1. Exploratory interpretive analysis
A series of components of FAT were identiﬁed after the evaluation
and analysis of the interview and critical incidence data. Some of the
components were comparable to those researched in the literature on
algorithms.
Fairness. The concept of fairness was raised during interviews in

H1. Users' assessment of transparency positively aﬀects user
satisfaction with algorithms.
H2. Users' assessment of fairness positively aﬀects user satisfaction with
algorithms.

Table 1
Demographics of survey respondents.

Accountability has become as important as transparency and fairness. Perceived accountability of algorithms is the expectation that a
user's beliefs, actions, or feelings may need to be justiﬁed regarding
algorithms. For organizations to be perceived as accountable, users can
expect an evaluative process with negative or positive consequences to
follow task outcomes. When people perceive accountability, they search
more conscientiously for relevant information, ﬁnd more evidence for
decisions, develop stronger rationalizations for choices, and complete
tasks themselves more often. This increased accountability can be
considered a mechanism to alleviate potential negative consequences
from automated algorithmic processes (Aggarwal & Mazumdar, 2008).
High levels of accountability may also lead organizations to exert
280

Age (years)

Percent

Under 20
21–35
36–45
Over 46
Prior experience (months)
1–5
6–9
10 to 12
More than 1 year
Gender
Female
Male

10
35
35
20
19
21
22
38
49
51

Computers in Human Behavior 98 (2019) 277–284

D. Shin and Y.J. Park

tests were used to assess the statistical signiﬁcance of the diﬀerence
between the strength of relationship among the variables from the two
datasets. The results of the moderation eﬀect show the signiﬁcantly
diﬀerent structural relationships of all the paths in the model (see
Table 4).

Table 2
Results of hypothesis tests.

∗

Hypothesis

Standardized
coeﬃcient

S.E.

C.R. (tvalue)

H1: Transparency → Satisfaction
H2: Fairness → Satisfaction
H3: Accountability →
Satisfaction

0.394
0.518
0.215

0.047
0.052
0.038

7.638a
9.357a
4.566a

6.2. Findings from survey
Eﬀects of trust level and algorithms. A two-way analysis of variance (ANOVA) was used to examine the eﬀects of trust tendency and
algorithms on the measured variable. The participants who interacted
with algorithm services eliciting high algorithmic features experienced
greater satisfaction, F(1, 95) = 18.38, p < .001, ηp2 = 0.18, demonstrated higher credibility regarding the results, F(1, 95) = 21.13,
p < .001, ηp2 = 0.14, and rated higher in transparency, F(1,
95) = 20.02, p < .001, ηp2 = 0.15, than those who interacted with
non-algorithm services. The results of the ANOVA showed that participants with a high algorithmic tendency experienced greater satisfaction, F(1, 95) = 4.09, p < 0.05, ηp2 = 0.14, demonstrated a more positive intention for the algorithm services, F(1, 96) = 21.31, p < .001,
ηp2 = 0.15, and rated the transparency, accountability, and fairness as
higher in quality, F(1, 95) = 15.03, p < .001, ηp2 = 0.21; F(1,
95) = 14.34, p < .001, ηp2 = 0.11; than did those who used non-algorithm services.
Interaction eﬀects. The ANOVA identiﬁed an interaction eﬀect
between algorithm and trust level regarding attitude and value [F(1,
32) = 8.09, p < 0.01]. There are combined eﬀects of algorithm and
trust level on satisfaction. The results of the interaction eﬀects suggest
that the higher trust tendency with the algorithm services (i.e., recommendation system, Chatbot) produced a greater positive value, and
thus, a more positive attitudethan the low trust tendency with nonalgorithm services (conventional search engine, online news). That is,
people with a high trust tendency found algorithm-driven services more
satisfactory and useful than non-algorithm services, whereas people
with a low trust level found non-algorithm services more satisfactory
and comfortable than algorithm-driven services. That is, users' intrinsic
trust tendencies and the service's properties have combined eﬀects on
satisfaction. The high trust group perceives higher value and feels more
satisﬁed, whereas the low trust group feels more comfortable with nonalgorithm features.

1.96: 95% (0.05), ∗∗2.58: 99% (0.01).
a
3.29: 99.9% (0.001).

combination with accuracy. People mentioned fairness is an impartial
and just process of algorithms without favoritism or discrimination.
Respondents referred to indiscrimination, non-discriminatory treatment, and unbiasedness. Although one group talked about impartiality,
other people stated accurate results of the searches and recommendations by algorithms. Commonly, people unanimously agreed that fairness is critical factor in algorithms. Three components of fairness can be
drawn: indiscrimination, impartiality, and accuracy.
Accountability. Respondents were concerned that algorithmic
systems are vulnerable making mistakes or lead to undesired consequences. The respondents agreed that algorithms can have problems
due to human bias or simple oversight. Thus, they viewed that ﬁrms
producing algorithms should be held responsible in some way and
somehow for the results of their algorithms. In case an algorithm delivered discriminatory results because of the bias embedded in data, the
system must be liable for the result. Based on the qualitative process,
three components of accountability can be drawn: responsibility, auditability, and equity.
Transparency. Transparency has been viewed as a crucial factor in
algorithms. As more everyday decisions are automated by algorithms,
there is a growing need for transparency as the processes may be
opaque and have a risk of biased proﬁling and discrimination. People
viewed that given the impact of algorithms, the decisions made by algorithms should be visible, understandable, and transparent to the
people who use, generate, and regulate. Others also raised a balanced
view that algorithmic transparency should be balanced with promoting
commercial development and industrial proﬁts. Based on the qualitative process, the three components of transparency are derived: understandability, explainability, and observability.

7. Discussion
6. Results from the FAT model
This study is an eﬀort to deepen the current understanding of the
issues surrounding algorithms and clarify the deﬁnitions and the eﬀects
of FAT on the users' cognitive processes. The ﬁndings show that user
perceptions of algorithm FAT can signiﬁcantly inﬂuence users’ cognition and adoption, with (1) the perceived FAT playing signiﬁcant roles
in user satisfaction of algorithm services and (2) trust playing a moderating role in the eﬀects of FAT on satisfaction. This study also found
that the interaction eﬀect between trust and algorithm features inﬂuences the satisfaction with the algorithm. The ﬁndings together implied
the heuristic role of FAT regarding its underlying link to trust, as this
study contributes to the understanding of what constitutes the public
notions of FAT, how people view them, and what leads users adopt
algorithms.

The overall ﬁt of the model was acceptable; all relevant goodnessof-ﬁt indices surpassed acceptance levels as recommended by previous
research. All other signiﬁcant ﬁt indices showed that the model provided a suitable ﬁt to the data. Per the root mean squared error approximation, there was no evidence of an unsuitable ﬁt to the data. The
standardized root mean residual was also acceptable, and the normed
chi-squared value was less than the benchmark of three, which shows
satisfactory overall model performance.
The hypothesized causal paths were tested, and all the proposed
hypotheses were supported. The results supported the proposed relations and illustrated the signiﬁcant roles of FAT in the adoption of algorithms. The key relationships between FAT and satisfaction were
supported by the data, as indicated by the signiﬁcant critical ratios
(CRs). Fairness had the highest signiﬁcant eﬀect on satisfaction (H2,
CR = 9.357), followed by transparency (H1, CR = 7.638). Overall, the
model implies the importance of FAT in algorithmic process and further
implies the users’ cognitive process.

7.1. Contextuality and subjectivity of FAT
The quantitative ﬁndings of this study imply that FAT issues are
diversely accepted and multi-functionally related, and that user attitudes about FAT are highly dependent on the context in which it takes
place, as well as on the basis who is looking at. This is in line with the
previous studies (e.g., Lee, 2018), which have shown diverse views
regarding FAT. The qualitative ﬁndings, on the other hand, have implied that topics regarding FAT are somehow related and overlapping,

6.1. Findings from moderation eﬀect
With the high and low trust group in place, a series of Chow tests
(1960) was conducted for each moderator individually (Table 3). Chow
281

Computers in Human Behavior 98 (2019) 277–284

D. Shin and Y.J. Park

Table 3
The Chow tests of trust moderation.
F/p-value

Transparency-Satisfaction

Fairness-Satisfaction

Accountability-Satisfaction

High trust (n = 110)
Low trust (n = 120)
F(4; 1312)
p-value

0.481
0.181
3.45
< 0.05

0.712
0.210
1.31
< 0.05

0.641
0.225
2.13
< 0.05

Social Construction of algorithm: an algorithm and its ecosystems can
be considered a socially constructed artifact. Algorithm technologies
are the results of technological innovations that industry has made, and
the algorithm services reﬂect the people's aspirations. For example,
algorithmic selection has become an increasing source of social order,
of a shared social reality (see Park, Chung, & Shin, 2018). Algorithm
applications shape and form everyday lives and realities, inﬂuence the
perception of the outside, and aﬀect actions. As users become increasingly involved with the personalization and customization of the algorithm, co-evolutionary perspective become more persuasive than
pure social construction of algorithm.
Diﬀerent from pure social constructionism by technologies, algorithmic reality construction increases personalization, individualization, and customization to increase transparency, accuracy, and predictability. For example, the algorithm search recommendations try to
match with users’ previous experiences. Just like selective exposure,
users want to see what they would prefer to see, they want to view what
they would prefer to view, and they want to be reinforced by the algorithmic process (Beam, 2014). The more users use algorithms, the
narrower their perspectives become. For example, people perceive what
they want to in algorithmically recommended contents while try to
ignore other viewpoints. Cognitive processes form the way users perceive FAT.
In this sense, a reasonable assertion is that users are the source and
creators of algorithms by invoking deep subconscious cognitive processes. What users see through algorithms is a cognitively constructed
reality (or its version) that emulates the form of an accumulated experience shaped by a priori mental constructs. As Just and Latzer (in
press) argued, algorithmic selection has become a shared social reality
and shapes daily lives and realities, aﬀecting the perception of the
world; additionally, humans and algorithms co-evolve and create reality together as they inﬂuence each other.
Although algorithms reﬂect users this way, the negative eﬀects of
such a reﬂection have social ramiﬁcations at diﬀerent levels. At a micro
level, how to create user-centered algorithms is a matter of how to
create algorithms that are more responsible and processes that are more
transparent. The notion of accuracy is not a matter of reﬂecting what
users want, but suggesting socially and politically correct results to
users. In fact, algorithms are prone to amplify racist and sexist biases
from the real word. COMPAS is an algorithm widely used in the U.S. to
guide sentencing by predicting the likelihood of a criminal reoﬀending.
Yet it is viewed racially biased as the system predicts that black defendants pose a higher risk of recidivism than they do, and the reverse

making them diﬃcult to distinguish or separate (as shown Fig. 1). The
issues altogether still raise active user roles in the algorithm development and users' rights regarding data collection and analysis. Although
the model shows the general relationship among the factors, the further
tests will corroborate the argument that the FAT issues are dependent
on the users’ cognition and perception. As FAT issues are high abstract
terms, of course, people understand, perceive, and process FAT diﬀerently. For example, while personalized results have great beneﬁts to
certain users, other users may ﬁnd it as unfair, depending on what
characteristics are perceived in personalized experiences. Transparency
can disadvantage some users when it is predicated on negative assumptions. Moreover, it is often hidden from users, limiting their requests of responsibility.
The moderating roles and the interaction eﬀects imply that the issues are mostly subjective matters. A common sentiment is that FAT is
inexorably subjective, and maybe this is true in algorithm contexts. FAT
may be incapable of being measured by objective standards. User perception of FAT may be formed out of their own intrinsic process of trust
and motivation independent from the actual nature of FAT in algorithms. Users with a lower trust level intrinsically consider algorithms
less trustworthy and consider FAT issues skeptically. This ﬁnding is
consonant with the moderating role of trust. Trust is found to play
signiﬁcant role in processing FAT, which then aﬀects satisfaction.
Although the positive role of trust on adoption behavior has been
widely validated (e.g., Shin, 2010), the interaction and moderating
eﬀect imply that an algorithm is dependent on users’ trust.
An inference is that algorithmic applications form everyday lives
and actualities just like the construction of realities by the mass media.
Just and Latzer (2018) argued that automated algorithms inﬂuence
people's perception of reality and behavior. From the view, the coevolutionary perspective of algorithms can be drawn: organizations,
ideologies, intermediation, and people inﬂuence the construction of
reality. Similarly, Mager (2012) argued that personalized searches inﬂuence user perspectives because algorithms selectively guess what
information a user would like to see based on information about the
users past experiences and search history. User perception and belief
are created, ampliﬁed, or reinforced by algorithms and vice versa.

7.2. FAT is eye of the beholder
On a broader conceptual ground, these lend support to the idea of
social construction of algorithms. Extending the frame of the Social
Construction of Technology, an assertion can be made regarding the
Table 4
Two-way analyses of variance indicating the eﬀects of the independent variables.
Measured variables

Mean (Standard error)
Algorithm level
Algorithm

Transparency
Accountability
Fairness
Satisfaction
∗

5.24(.14)
5.12(.09)
5.23(.13)
5.38(.14)

Trust level
Non-Algorithm

F
∗∗

4.87(.15)
4.56(.11)
5.23(.15)
4.56(.14)

22.11
15.22∗∗
19.49∗∗
20.17∗∗

p < 0.05, ∗∗p < 0.001.
282

High trust

Low trust

F

5.98(.10)
5.14(.09)
5.72(.11)
5.49(.12)

5.92(.14)
5.16(.04)
5.97(.19)
5.43(.12)

15.11∗∗
15.99∗∗
4.15∗
21.23∗∗

Computers in Human Behavior 98 (2019) 277–284

D. Shin and Y.J. Park

the user perspective. How users see, perceive, and feel should be the
ﬁrst criteria for measuring and operationalizing FAT.
Here the concept of aﬀordance is helpful for a theoretical conceptualization of algorithmic cognitive processes. Aﬀordance denotes
the direct perception of the utility of an object through the perception
of its features. Users expect algorithms to oﬀer accurate, convenient,
and credible results, and such desirable properties are ensured with
FAT. Trust and aﬀordance have a circular relationship that once users
trust algorithm services or the providers, they perceive that the services
are easy to use, adopt, and continue to use. Algorithmic aﬀordance
provides a plausible cognitive process for perception in humans and the
perceptions of transparency, fairness, and accountability (Butcher,
2017).
With the users' trust, the users continue to cooperate with algorithms by allowing their data to be collected by algorithms, and such
increased data provide better predictive analytics to users (Lee, 2018).
When users acknowledge a certain process to be transparent, accurate,
and fair, their trust levels increase. Transparency and accuracy build
trust. When trust is created, it leads to a heightened sense of intention
and satisfaction. The literature has widely shown that transparency and
accuracy positively inﬂuence users' levels of trust (Shin, 2010). The
trust feedback loop is a positive feedback loop that diminishes users’
concerns regarding transparency and accuracy, and user satisfaction
and intention increase signiﬁcantly. Positive feedback is possibly positively related to trust, satisfaction, transparency, fairness, and intention. Such feedback may demonstrate the importance of examining the
complex cognitive mechanisms relating to feedback. Future studies
should examine the positive feedback loop of trust.

for white defendants.
7.3. Are transparency and accuracy objectives?
The overall ﬁndings of this study indirectly suggest that perceptions
of transparency and accuracy are not purely objective responses to
media content. Instead, the model in this study lends strong support to
the argument that, to a signiﬁcant extent, similar to perceptions of information in general, perceived transparency and accuracy in the algorithmic media is subjective. Transparency and accuracy can be more
subjective perceptions held by users rather than objective criteria.
Given this insight, we can use various dimensions to measure the
transparency and accuracy of a recommendation. Transparency and
accuracy depend on users' perceptions. Although transparency and accuracy have been popular topics in algorithmic services, such concerns
are socially constructed and cognitively reconstructed within users'
cognitive dimensions. Rather than such issues being uniformly or collectively given to users, users create their own versions of transparency
and accuracy based on their levels of trust and other personal intrinsic
factors (Shin & Biocca, 2018). In other words, transparency and accuracy are cognitively constructed realities of their own making (as argued by constructivism) because they depend on users' perceptions.
This result is consistent with the literature: Shin's argument that “immersion may be in the eye of the beholder” (2018). The argument can
be applied to algorithms, in that transparency, fairness, and accuracy
depend on the user's perspective, and all are at the disposition of the
user (Lee & Boynton, 2017). This point can help clarify the concept that
algorithmic qualities are more about the users' own cognitive perceptions and less about technical features.
Although UX emphasizes user focus and perspectives in relation to
technology, the UX of algorithms advises us to focus on users in terms of
experiencing the quality provided by technologies. In this regard, the
algorithm experience comprises users’ perceptions of quality, interactions with services, and the technological features that users see, feel,
and interact with. This argument can make sense because algorithms
are reﬂections of user preferences, experiences, and values. Thus, what
users expect from algorithms and what users experience may be subjective (Just & Latzer, in press).

8. Conclusion and practical implications
For industry, this study provides insights on a new design principle
for eﬀective algorithm design and development. The recent advances in
algorithm technologies enable establishing socio-technical systems that
closely interweave users and their social structures with technologies.
With the emerging importance of the algorithm, the question is how to
make human-centered algorithms (Baumer, in-press). The ﬁrst practical
suggestion is that the related industry should address the UX of algorithm services. Subjective perceptions and psychological eﬀects are
critical in rationalizing how and why people perceive what they do
regarding the issues of algorithms, and how they use and engage with
algorithm-generated news. The results suggest that the user perceptions
of the FAT issues of algorithm systems induce aﬀordance, which aﬀects
UX. Understanding how users search, ﬁnd, and consume algorithms
allows ﬁrms and algorithm designers to perform more eﬃciently and
naturally. This study suggests FAT framework evaluating UX of algorithm users. Transparency, fairness, and accuracy have been thorny
issues in algorithms and recommendation systems. UX is important to
make more precise and accurate recommendations. The FAT framework
provides the algorithm designers with guidelines on how to combine
transparency and fairness issues with the other factors, for example,
how to collect user data/implicit feedback eﬀectively while promoting
the user trust and assurance.
Second, industry can utilize the algorithmic aﬀordance as a guiding
principle of programing algorithm services. As Bucher (2017) argues,
users’ understanding and perceptions of algorithms, the ways in which
users imagine and expect certain algorithmic aﬀordances, aﬀect how
they approach technologies. Industry can use algorithmic aﬀordance as
a key base to create feedback-loop of machine learning systems like
Facebook make user beliefs an important component in shaping the
overall system behavior. For example, ﬁrms can constantly monitor
online product reviews to identify consumer aﬀordance. As user activity
is generative of the system itself, industry can establish some forms of
aﬀordance repository to better predict user needs and preferences.
Future algorithms must go beyond perfunctory transparency or mechanical accuracy and toward actual user needs and perspectives.

7.4. Measuring transparency and accuracy
Transparency and accuracy cannot be measured as features of algorithms or in the context of legal terminology because they involve
subjective dimensions that can be experienced and perceived by users.
Per Kemper and Kolkman's argument (in press), transparency may be
viewed from users' perspectives or user-based approaches. Transparency is a form of awareness in the eye of the beholder, the degree of
which reﬂects the concentration of their emotional, cognitive, and
sensual links to the content and modality of technology. Because
transparency and accuracy partially constitute the user experience of
algorithms, industry may develop a framework that applies a usercentric approach to recommendation systems. An increasing criticism of
FAT is that such issues are inevitably subjective and incapable of being
determined by objective criteria. There might not be an objective
standard of transparency, fairness, and accountability. The subject of
FAT lies in the eye of the beholder. Its implementation, however, is in
the hands of the policymakers or company's algorithm developers.
What is a seemingly transparent and equitable solution to a service
provider may not be the same to its users. What each user deems
transparent and fair is subjective. In algorithm services, the problem is
the gap between the users' views and how the issues were applied. This
criticism poses a serious problem in measuring and operationalizing
FAT in algorithm services. Related to this problem, Kemper and
Kolkman (2018) raised a question: “if transparency is a primary concern, then to whom should algorithms be transparent?” If FAT is a
complicated topic, then the best means to conceptualize FAT is through
283

Computers in Human Behavior 98 (2019) 277–284

D. Shin and Y.J. Park

Hence, understanding algorithmic aﬀordance will be critical in predicting the future interests of users for better recommendations. This
task will be even more diﬃcult as users may have ever-evolving interests and predicting the changes in a dynamic ecology is diﬃcult. The
aﬀordance model in this study provides guidelines on how to integrate
transparency and fairness with trust factors and behavioral intention.
The eventual goal of algorithms and recommendation systems is to
develop the services human-centered approach. Applying a user cognitive process to UX design presents users with relevant information
they need. Algorithms that are user-centered and trust-based feedback
loops will be key in designing such human-centered systems.
Third, based on the ﬁndings of FAT, industry is suggested to develop
observable or explainable algorithms. Deep learning and AI algorithms
are considered opaque and are not allowed for users to look inside or
understand. Users want interpretable, understandable explanations
about why certain results are presented. Exposing users to their algorithmically-derived attributes led to heightened FAT. Revealing parts of
the algorithm process can satisfy user's “right to explanation” (EU
General Data Protection Regulation) of algorithmic computation that
aﬀects them. Given the importance of FAT and the user perception of
such issues, it is worthwhile to develop new methods to make algorithms more explainable and interpretable, and a way revealing parts of
the algorithmic process to users. The explainable algorithms involve
three levels of explainability: (1) Explain the intention behind how
system impacts users, (2) Explain the data sources algorithm use and
how it audits outcomes, and (3) Explain how inputs in a model lead to
outputs in a model. When companies able to explain these to users, to
governments, and even to other competing industry in markets, algorithm become more responsible, observable, and eﬀective socio-technical system.
Algorithmic aﬀordance not only beneﬁts users by providing them
opportunities and understanding of how transparent, fair, and accountable of algorithms, but could also help industries to establish user
trust and satisfaction with their algorithmic services.

20(1), 1–13. https://doi.org/10.1080/1369118X.2016.1216147.
Bucher, T. (2017). The algorithmic imaginary: Exploring the ordinary aﬀects of Facebook
algorithms. Information, Communication & Society, 20(1), 30–44. https://doi.org/10.
1080/1369118X.2016.1154086.
Chow, G. C. (1960). Tests of equality between sets of coeﬃcients in two linear regressions. Econometrica, 28(3), 591–605.
Diakopoulos, N. (2016). Accountability in algorithmic decision making. Communications
of the ACM, 59(2), 58–62.
Diakopoulos, N., & Koliska, M. (2016). Algorithmic transparency in the news media.
Digital Journalism, 5(7), 809–828. https://doi.org/10.1080/21670811.2016.
1208053.
Ettlinger, N. (2018). Algorithmic aﬀordances for productive resistance. Big Data & Society,
5(1), 1–13. https://doi.org/10.1177/2053951718771399.
Fink, K. (2018). Opening the government's black boxes. Information, Communication &
Society, 12(1), 1–10. https://doi.org/10.1080/1369118X.2017.1330418.
Helberger, N., Karppinen, K., & D'Acunto, L. (2018). Exposure diversity as a design
principle for recommender systems. Information, Communication & Society, 21(2),
191–207. https://doi.org/10.1080/1369118X.2016.1271900.
Just, N., & Latzer, M. (2018). Governance by algorithms. Media, Culture, and
Societyhttps://doi.org/10.1177/0163443716643157 (In-press).
Kemper, J., & Kolkman, D. (2018). Transparent to whom? Information. Communication &
Societyhttps://doi.org/10.1080/1369118X.2018.1477967 (In-press).
Kitchin, R. (2017). Thinking critically about and researching algorithms. Information,
Communication & Society, 20(1), 14–29. https://doi.org/10.1080/1369118X.2016.
1154087.
Lee, M. (2018). Understanding perception of algorithmic decisions: Fairness, trust, and
emotion in response to algorithmic management. Big Data & Society, 5(1), 1–16.
https://doi.org/10.1177/2053951718756684.
Lee, B., & Boynton, L. (2017). Conceptualizing transparency: Propositions for the integration of situational factors and stakeholders' perspectives. Public Relations Inquiry,
6(3), 233–251. https://doi.org/10.1177/2046147X17694937.
Mager, A. (2012). Algorithmic ideology. Information, Communication & Society, 15(5),
769–787. https://doi.org/10.1080/1369118X.2012.676056.
Mittelstadt, B., Allo, P., Taddel, M., Wachter, S., & Floridi, L. (2016). The ethics of algorithms. Big Data & Society, 7(1), 1–21. https://doi.org/10.1177/
2053951716679679.
Moller, J., Trilling, D., Helberger, N., & van Es, B. (2018). Do not blame it on the algorithm. Information, Communication & Society, 21(7), 959–977. https://doi.org/10.
1080/1369118X.2018.1444076.
Park, Y. J., Chung, J. E., & Shin, D. H. (2018). The structuration of digital ecosystem,
privacy, and big data intelligence. American Behavioral Scientist, 62(10), 1319–1337.
https://doi.org/10.1177/0002764218787863.
Shin, D. (2010). The eﬀects of trust, security and privacy in social networking: A securitybased approach to understand the pattern of adoption. Interacting with Computers,
22(5), 428–438. https://doi.org/10.1016/j.intcom.2010.05.001.
Shin, D. (2017). The role of aﬀordance in the experience of virtual reality learning:
Technological and aﬀective aﬀordances in virtual reality. Telematics and Informatics,
34(8), 1826–1836. https://doi.org/10.1016/j.tele.2017.05.013.
Shin, D. (2019). Toward fair, accountable, and transparent algorithms: Case studies on
algorithm initiatives in Korea and China. Javnost: The Public, 26(3), 1–17. https://doi.
org/10.1080/13183222.2019.1589249.
Shin, D., & Biocca, F. (2018). Exploring immersive experience in journalism what makes
people empathize with and embody immersive journalism? New Media & Society,
20(8), 2800–2823. https://doi.org/10.1177/1461444817733133.
Shin, D., & Choi, M. (2014). Ecological views of big data: Perspectives and issues.
Telematics and Informatics, 32(2), 311–320. https://doi.org/10.1016/j.tele.2014.09.
006.
Yang, K., & Stoyanovich, J. (2017). Measuring fairness in ranked outputs. Proceedings of
the 29th international conference on scientiﬁc and statistical database managementhttps://doi.org/10.1145/3085504.3085526 Chicago, IL, USA — June 27 - 29,
2017.

References
Aggarwal, P., & Mazumdar, T. (2008). Decision delegation: A conceptualization and
empirical investigation. Psychology and Marketing, 25(1), 71–93.
Ananny, M., & Crawford, K. (2018). Seeing without knowing: Limitations of the transparency ideal and its application to algorithmic accountability. New Media & Society,
20(3), 973–989. https://doi.org/10.1177/1461444816676645.
Baumer, E. (2017). Toward human-centered algorithm design. Big Data & Society, 1–12.
https://doi.org/10.1177/2053951717718854 (in-press).
Beam, M. (2014). Automating the news. Communication Research, 41(8), 1019–1041.
https://doi.org/10.1177/0093650213497979.
Bedi, P., & Vashisth, P. (2014). Empowering recommender systems using trust and argumentation. Information Sciences, 279, 569–586. https://doi.org/10.1016/j.ins.
2014.04.012.
Beer, D. (2017). The social power of algorithms. Information, Communication & Society,

284

