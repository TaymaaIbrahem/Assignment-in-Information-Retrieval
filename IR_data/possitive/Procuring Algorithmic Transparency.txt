PROCURING ALGORITHMIC TRANSPARENCY
Elizabeth A. Rowe* and Nyja Prior+
TABLE OF CONTENTS
INTRODUCTION ............................................................................................. 2
I. ALGORITHMS IN THE CRIMINAL JUSTICE SYSTEM ................... 12
A. THE EXAMPLE OF RISK ALGORITHMS................................................... 13
B. PROCUREMENT OF ALGORITHMS ........................................................... 18
1.
Exemplar Developers................................................................... 19
2.
Algorithms Need Data ................................................................. 21
3.
Inconsistent Testing and Implementation. ................................... 23
C. LEGAL CHALLENGES & CONSIDERATIONS ........................................... 25
1.
Access Denied .............................................................................. 28
2.
State v. Pickett ............................................................................. 31
3.
Special Constitutional Concerns for Criminal Justice? .............. 33
4.
FOIA Disclosures Unlikely .......................................................... 36
II. THE TENSION BETWEEN TRADE SECRECY AND PUBLIC
TRANSPARENCY ................................................................................................. 37
A. BUILT FOR COMPETITIVE ENVIRONMENT ............................................. 39
B. FOR DEVELOPERS, ALGORITHMS AND DATA ARE PROPERTY ............. 39
C. NO ROBUST ROLE FOR PUBLIC INTEREST IN GOVERNMENTAL
TRANSPARENCY .................................................................................................. 41
III. THE FIT: PROCUREMENT POLICIES & CONTRACTING .......... 44
A. CONTRACTING FOR ALGORITHMS & AI ................................................ 46
B. EXISTING MECHANISMS FOR AI REVIEW ARE NOT SUFFICIENT ......... 48
C. PROPOSAL CONSISTENT WITH PROCUREMENT POLICIES .................... 49
1.
Competitive Negotiation .............................................................. 50
2.
Qualification and Responsibility ................................................. 50
3.
Collateral Socio-Economic Policies ............................................ 51
D. PROPOSED CONTRACT TERMS TO PROTECT TRADE SECRETS &
PERMIT LIMITED DISCLOSURE .......................................................................... 53
Irving Cypen Professor of Law, Distinguished Teaching Scholar, and
Director, Program in Intellectual Property Law, University of Florida
Levin College of Law. + Associate, Knobbe Martens Olson & Bear, LLP.
We express our appreciation to Robert Brauneis, Christopher Crawford,
John Duffy, Tait Graves, Michelle Jacobs, Sonia Katyal, Sharon Sandeen,
Steven Schooner, Joshua Schwartz, Christopher Slobogin, and Rebecca
Wexler for insights, comments, or conversations about the ideas expressed
in earlier versions of this work, as well as to participants of workshops at
the University of Virginia School of Law and the 2021 IP Scholars
Conference. Thank you to Pete Love, Vieux Toure, and Kenneal Harrigan
for excellent research assistance.

*

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

2

1.
Who? ............................................................................................ 54
2.
What? ........................................................................................... 55
3.
When and How?........................................................................... 56
E. BENEFITS OF CONTRACT APPROACH ..................................................... 57
1.
Better control and accountability ................................................ 59
2.
Better System Integrity................................................................. 60
3.
Better for Vendors Too ................................................................ 61
4.
Better for Public Interest in Governmental Transparency .......... 63
5.
Potential Drawbacks ................................................................... 64
CONCLUSION ................................................................................................ 65

INTRODUCTION
One evening in May 2020, 64-year-old Michael Williams was returning
home from an after-dinner trip to his neighborhood convenience store in
Chicago’s South Side.1 He was flagged down by a 25-year-old acquaintance,
Safarian Herring, who asked for a ride.2 Williams obliged, and Herring
climbed into the front seat.3 As Williams proceeded down South Stony Island
Avenue toward an intersection, Herring was shot on the side of his head.4
Williams told police that the shot was fired when a car pulled up beside him,
the passenger fired into Williams’ car, and Williams then ran a red light to
escape.5 All parties agree that after Herring was shot, Williams drove him
directly to a hospital, where Herring would survive a few days before
succumbing to his injuries.6
A few months later, Williams was charged with first degree murder on
the theory that it was Williams who shot Herring from inside the car that
night.7 Prosecutors did not have an eyewitness, a gun, or a motive.8 Instead,
they had ShotSpotter – an artificial intelligence (“AI”) surveillance system
that uses hidden microphone sensors to detect sounds.9 The sounds are then
Garance Burke et al., How AI-Powered Tech Landed Man in Jail with
Scant
Evidence,
ASSOCIATED
PRESS
(Aug.
19,
2021),
https://apnews.com/article/artificial-intelligence-algorithm-technologypolice-crime-7e3345485aa668c97606d4b54f9b6220.
2
Id.
3
Id.
4
Id.
5
Id.
6
Todd Feathers, Police Are Telling ShotSpotter to Alter Evidence From
Gunshot-Detecting AI, VICE: MOTHERBOARD (Jul. 26, 2021, 9:00 AM),
https://www.vice.com/en/article/qj8xbq/police-are-telling-shotspotter-toalter-evidence-from-gunshot-detecting-ai.
7
Burke et al., supra note 1.
8
Id.
9
Feathers, supra note 6.
1

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

3

processed through a secret algorithm that determines whether they are
gunshots, their location, and then alerts the police.10 The government’s theory
against Williams was that ShotSpotter proved the fatal shot was fired at a
particular corner on South Stony Island Avenue, that evidence from a security
video camera demonstrated that Williams’s car was at that location, and there
was no passing car that could have fired the shot.11
However, there were three main weaknesses in the government’s
reliance on ShotSpotter to support its theory. First, ShotSpotter’s algorithm
initially characterized the sound as a firecracker (with 98% confidence), but
a ShotSpotter analyst manually overrode the algorithm and “reclassified” the
sound as a gunshot.12 Second, the algorithm placed the shot location on Lake
Shore Drive which is about a mile away from the South Stony Island Avenue
location where prosecutors claim the murder occurred.13 Once again, that
location was manually changed months later in “post processing,” allegedly at
the request of the Chicago police, (by another ShotSpotter analyst) to
coordinates on South Stony Island Avenue where Williams’ car was seen on the
video camera.14 Third, the government’s theory required the shot to have been
fired inside Williams’ car, but according to ShotSpotter, its contract warns
against relying on the algorithm to locate shots fired inside vehicles or
buildings, a fact which it claims to have communicated to prosecutors.15
Williams’ attorneys attacked those weaknesses in the evidence. They
sought discovery on the government's communications with ShotSpotter.
They also filed a motion to exclude the ShotSpotter evidence and to obtain
ShotSpotter’s secret operating protocols.16 Further, they challenged the afterthe-fact manual changes by the analysts and sought their identities, arguing
that “[t]hrough this human-involved method, the ShotSpotter output in this case
was dramatically transformed from data that did not support criminal charges of
any kind to data that now forms the centerpiece of the prosecution’s murder case
against Mr. Williams.”17 However, the company refused to identify the names
of the employees who altered the algorithm.18 Indeed, prosecutors chose to
withdraw the ShotSpotter evidence rather than reply to the defense’s Motion
See State of Illiniois v. Williams, Motion to Exclude Shotspotter
Evidence Pursuant to Frye and Rule 403, 20CR0899601 at 3-4.
11
The surveillance camera did show another car running the red light next
to Williams’s car, but because the windows in that car appeared to be rolled
up, prosecutors dismissed the idea that the shot could have been fired from
that car. Burke et al., supra note 1.
12
Id.; Feathers, supra note 6.
13
Feathers, supra note 6.
14
Id.
15
Burke et al., supra note 1.
16
See State of Illiniois v. Williams, Motion to Exclude Shotspotter
Evidence Pursuant to Frye and Rule 403, 20CR0899601.
17
State of Illiniois v. Williams, Motion to Exclude Shotspotter Evidence
Pursuant to Frye and Rule 403, 20CR0899601, at 30.
18
See State of Illiniois v. Williams, Motion to Exclude Shotspotter
Evidence Pursuant to Frye and Rule 403, 20CR0899601.
10

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

4

to Suppress,19 and the government eventually dismissed all charges.20 The
outcome for Williams was, at best, a mixed bag. From the standpoint of a
criminal defendant’s constitutional rights, perhaps in this instance Williams
was “lucky” to not have the evidence used against him. Regrettably, however,
he remained in jail for eleven months (twice infected with Covid-19 behind
bars) until the case was finally dismissed.21
Multiple news investigations revealed that the company’s analysts
frequently alter alerts when requested by police departments.22 ShotSpotter,
used in over 100 cities in the U.S., claims its algorithms as trade secrets23 and
does not permit independent testing.24 Prior to publication of the news stories
about the Williams case, the MacArthur Justice Center at Northwestern
Pritzker School of Law also investigated ShotSpotter.25 It reported that the
City of Chicago had entered into a $33 million, three-year contract with
ShotSpotter.26 Further, there were no studies on the program’s reliability or on
its accuracy in distinguishing gunshots from “firecrackers, backfiring cars,
construction noises, helicopters, and other loud, impulsive sounds.”27
Analysis revealed that in about a two-year period, 89% of the time gunshot
alerts were not gun-related.28 During that same period, police were deployed
40,000 times (in mostly Black and Brown neighborhoods) in search of gunfire
See Feathers, supra note 6.
Burke et al., supra note 1.
21
Id.
22
Feathers, supra note 6.
23
See, e.g., ShotSpotter Agreement with City of Fresno, California,
FRESNO.GOV (Mar. 26, 2015), https://www.fresno.gov/cityclerk/wpcontent/uploads/sites/9/2016/10/SSTalsoShotSpotterSrvsandLicenseAgm
texp2018.pdf (expressly restricting the city of Fresno, CA from disclosing
ShotSpotter’s data due to trade secrecy); Matt Drange, We’re Spending
Millions On This High-Tech System Designed To Reduce Gun Violence.
Is It Making A Difference?, FORBES (Nov. 17, 2016, 8:30 AM),
https://www.forbes.com/sites/mattdrange/2016/11/17/shotspotterstruggles-to-prove-impact-as-silicon-valley-answer-to-gunviolence/?sh=6eddabbc31cb (detailing an interview with economics
Professor Jennifer Doleac of the University of Virginia and her failed
attempts to obtain ShotSpotter data due to claims of trade secrecy. Even
after contacting ShotSpotter’s CEO Ralph Clark directly, Doleac was
informed that the data could only be obtained if she paid $50,000 for each
city to be studied).
24
Feathers, supra note 6.
25
See, e.g. Press Release, McArthur Justice Center, Shotspotter Generated
Over 40,000 Dead-End Police Deployments in Chicago in 21 Months,
According
to
New
Study
(May
3,
2021),
https://www.macarthurjustice.org/shotspotter-generated-over-40000dead-end-police-deployments-in-chicago-in-21-months-according-tonew-study/.
26
Id.
27
Id.
28
Id.
19
20

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

5

that did not exist.29
The ShotSpotter story typifies the symbiotic relationship between private
sector sellers-of-technology and government purchasers-of-technology at
every stage of the criminal justice system. Just as in the private marketplace,
artificial intelligence tools (“AI”) undoubtedly have the potential to improve
government functions and efficiency. However, as ShotSpotter reveals, both
law enforcement and taxpayers may gain little if the technology is unreliable
or ultimately inadmissible for convictions because of secrecy.
Scholars30 in both criminal law and intellectual property have thoughtfully
examined several related factors at the intersection of intellectual property,
criminal justice, algorithms, and transparency. They have written about what
should be done to address the secrecy problem, proposing a range of potential
approaches, inter alia – greater transparency, abolishing trade secret
protection for algorithms in criminal justice, or banning algorithms absent full
disclosure.31 For instance, Christopher Slobogin, in his recent book notes that
“[e]ven if it turns out that advanced machine-learning [risk assessment
systems] are demonstrably more accurate than simpler versions . . . they
should be banned from criminal proceedings, at least when they are
‘inscrutable’; litigants, policymakers and decision-makers must be provided
information about how they work.”32 Robert Brauneis and Ellen Goodman
suggest that more meaningful transparency will be achieved if vendors are
required “to create and deliver records that explain key policy decisions and
validation efforts, without necessarily disclosing precise formulas or
algorithms.”33 From the broader legislative standpoint, Tait Graves and Sonya
Katyal call for greater transparency through suggested federal and state
statutory reforms that may “mandate disclosure of certain data . . . or explicitly

The city’s predictive policing technology also uses ShotSpotter data. Id.
See, e.g., Charles Tait Graves & Sonia K. Katyal, From Trade Secrecy
to Seclusion, 109 GEO. L.J. 1337 (2021); Christopher Slobogin, Just
Algorithms: Using Science to Reduce Incarceration and Inform a
Jurisprudence of Risk (2021); Cary Coglianese & Lavi M. Ben Dor, AI in
Adjudication and Administration, Faculty Scholarship at Penn Law (Feb.
15,
2020),
https://scholarship.law.upenn.edu/cgi/viewcontent.cgi?article=3120&con
text=faculty_scholarship; Sonia K. Katyal, The Paradox of Source Code
Secrecy, 104 Cornell L. Rev. 1183 (2019); Rebecca Wexler, Life, Liberty,
and Trade Secrets: Intellectual Property in the Criminal Justice System,
70 Stan. L. Rev. 1343, 1359-60 (2018); Robert Brauneis & Ellen P.
Goodman, Algorithmic Transparency for the Smart City, 20 Yale J.L. &
Tech. 103 (2018); Natalie Ram, Innovating Criminal Justice, 112 Nw. U.
L. Rev. 659 (2018); Danielle Citron & Frank Pasquale, The Scored
Society: Due Process for Automated Predictions, 89 Wash. L. Rev. 1, 21
(2014).
31
See id. .
32
SLOBOGIN, supra note 30, at 111 (internal citations omitted).
33
Brauneis & Goodman, supra note 30, at 176.
29
30

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

6

permit the sharing and disclosure of such data.”34 Finally, as an evidentiary
matter, Rebecca Wexler makes the case that the trade secret privilege should
have no place in criminal proceedings because “trade secret holders should
wield no special power to block criminal defendants’ access to evidence
altogether.”35
In this Article, we expand upon that body of work by offering a realistic
and practical approach as to how to achieve transparency given existing trade
secret law and the relatively strong property rights that private vendors have
under that law. In short, we point to a specific legal tool from the already
existing arsenal of private law in an effort to address some of the larger public
law concerns. While the prior literature has focused on constitutional
challenges to algorithmic tools, including due process challenges, and on ex
post judicial remedies in individual litigation long after the government has
purchased software from a developer, this Article pivots to a novel viewpoint.
We go back to the basics of the initial transaction.
Significantly, we reorient the analysis to an earlier time period—the point
at which the government agency initially decides to purchase the software.
Looking back to that earlier period, we point to a different field of law
entirely—the law on government procurement and contracting—and suggest
that many of the relevant concerns regarding excessive trade secrecy and
essential government transparency could be resolved through negotiations in
the shadow of that body of law. Contract law is thus central to our proposal.
Contracts can balance the competing interests of secrecy and disclosure, and
contractual negotiations between government agencies and private vendors
are the means for achieving such balance on a transaction-by-transaction
basis.
Using the criminal justice system as an illustration, this Article observes
that there is a perceived theoretical incongruity between the underlying
purpose of trade secret law in a private competitive sphere and the values of
openness that are fundamental to public governmental functions. Yet trade
secret law is not designed to foster absolute secrecy. To the contrary, trade
secret law is built to solve Arrow’s information paradox by facilitating the
sharing of information in a manner that does not result in loss of the value of
the information to its owner. We therefore posit that the existing theoretical
framework of trade secret law reveals both the limited nature of the problem
(mere initial secrecy) and the evident solution (contractually authorized
disclosures).
The Article thus aims to build on the developing literature in this area by
offering a novel transaction-by-transaction procurement-based proposal that
ultimately integrates and carefully weaves together criminal law and
procedure, constitutional law, contract law, intellectual property law, and
procurement law. It attempts to offer a solution that is immediate and
practical. It also underscores and highlights the fundamental problem for
34
35

Graves & Katyal, supra note 30, at 1420.
Wexler, supra note 30, at 1353.

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

7

which a theoretical and doctrinal solution has yet to be discovered. Namely,
intellectual property law was not designed to promote the public interest in
governmental transparency.36 To the contrary, it was designed to protect and
enforce private property rights, an objective which may seem to be at odds
with some of the values of democratic governance.37 Yet intellectual property
law, like property law more generally, is designed to foster contractual
transfers of rights and accommodations of conflicting interests. Where the
government procures trade-secret protected algorithmic tools, the
procurement contracts can and should accommodate the public’s needs for
some disclosure and transparency.
Overall, our proposal is unique and combines several key components:
First, it is an ex ante approach. Rather than trying to solve a transparency
issue during the course of a criminal trial, which is long after the government
has purchased the software and begun to use it, our approach tries to resolve
the issue at the front end. This is ultimately in the interest of both sides of the
transaction ex ante. It is more efficient to have a system in place that gets it
right the first time. Moreover, it reduces the risk that courts will find forensic
algorithms inadmissible at Frye or Daubert hearings if they have not been
independently tested and validated and do not provide access to defense
counsel because of overly broad secrecy provisions.38 Thus, police
departments would be incentivized to negotiate disclosure terms ex ante rather
than risk not being able to use evidence to obtain a conviction, as happened
with ShotSpotter in Chicago.39
Second, it is a transaction-by-transaction approach. This offers at least
two important benefits. States (and even state courts) would not need to create
a one-size-fits-all approach to algorithmic transparency. Perhaps some
vendors might not fear disclosure as much as others based on the nature of the
technology at issue or its use. For example, some vendors might rely more on
copyright and patent rights to protect their IP; they might have less to fear
from disclosure and thus might demand more modest protections. For other
vendors, trade secrets might be their crown jewels of the technology; thus,
they are likely to demand, and to legitimately need, more protection. In our
view, this would not mean no transparency, but it might mean more limited
disclosures, more protections against unauthorized disclosure.
The transaction-by-transaction approach also allows tailoring based on
considerations on the governmental side of the procurement contract. For
example, the public interest in governmental transparency might be more
pressing in some contexts than in others. Algorithms used in setting criminal
sentences or used to generate admissible evidence (like ShotSpotter) may be
at one end of the spectrum, where constitutional rights and traditions seem
most clearly to demand that the defendants’ lawyers need disclosure and have
See infra Part II.C.
See infra Part II.B.
38
See Wexler, supra note 30; SLOBOGIN, supra note 30.
39
See Feathers, supra note 6.
36
37

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

8

rights to challenge the algorithmic model. On the other hand, algorithms to
allocate police resources or perform administrative tasks might be different,
and not require (as much, if at all) disclosure.
Another reason for favoring a transaction-by-transaction approach is that
it promotes federalism and localism. A pro-transparency city government can
negotiate for more transparency, even if the courts or state government in that
state are unwilling to take such an aggressive stance. One could argue that in
any new area of law involving new circumstances, conditions and
technologies, public policy should favor localism for the traditional
Brandeisian “laboratories of democracy” reasons.40 It is also easier to change
city contracting policies than it is to enact new federal and state laws on trade
secrets. For instance, only one state has passed legislation addressing trade
secrecy in criminal justice. On March 28, 2019, Idaho became the first state
to completely remove trade secret protections within the criminal justice
system for pretrial risk assessment tools and require algorithmic transparency
and open access (to the public) for such tools.41 While a bold step, this
legislation is nevertheless narrow in that it is limited to pretrial risk
assessment tools. It therefore does not apply to many other algorithmic
models such as evidence-generating software, like ShotSpotter, DNA analysis
software, or facial recognition software. There has also been some movement
toward proposing legislation at the federal level to eliminate the trade secret
evidentiary privilege in criminal proceedings,42 but reading political tea leaves
one might expect it is unlikely to become law any time soon.43
Third, our approach is negotiated. Typically, consumers enter into
standardized contracts with sellers, without the ability to negotiate their terms.
See New State Ice Co. v. Liebmann, 285 U.S. 262, 311 (1932) (Brandeis,
J., dissenting) (“It is one of the happy incidents of the federal system that
a single courageous State may, if its citizens choose, serve as a laboratory;
and try novel social and economic experiments . . . .”).
41
IDAHO CODE § 19-1910 (2020).
42
See Press Release, Mark Takano, House of Representatives, Reps.
Takano and Evans Reintroduce the Justice in Forensic Algorithms Act to
Protect Defendants’ Due Process Rights in the Criminal Justice System
(April 8, 2021), https://takano.house.gov/newsroom/press-releases/repstakano-and-evans-reintroduce-the-justice-in-forensic-algorithms-act-toprotect-defendants-due-process-rights-in-the-criminal-justice-system; see
also Justice in Forensic Algorithms Act of 2021, H.R. 2438, 117th Cong.
(2021),
https://www.congress.gov/117/bills/hr2438/BILLS117hr2438ih.xml
43
See,
e.g.,
Lexis
Legislative
Outlook,
available
at
https://plus.lexis.com/document/documentlink/?pdmfid=1530671&crid=
28f14df7-3387-45ba-994202ae22a38a08&pddocfullpath=%2Fshared%2Fdocument%2Fstatuteslegislation%2Furn%3AcontentItem%3A62DF-5H01-JSXV-G38W0000000&pdcontentcomponentid=133053&pdproductcontenttypeid=undefined
&pdiskwicview=false&pdpinpoint=&ecomp=7gktk
40

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

9

These contracts are not necessarily invalid, but courts may examine the terms
more closely to determine whether they are unconscionable.44 Naturally, these
agreements will often be more favorable to the party who drafted them, rather
than the consumer.45 In many ways this appears to be the current status quo,
where vendors dictate terms that forbid disclosure and the government
agency-consumer accepts, probably with little push back. However, the
government is no regular consumer. It is the largest purchaser of goods and
services, having spent over $14 billion on technology products and services
in 2020.46 It has leverage and bargaining power. As such, our negotiated
approach envisions use of that power to contract for greater transparency in
the procurement context, while respecting the rights of vendors.47
Notwithstanding the government agencies and vendors who may not be
inclined toward sharing, and might instinctively prefer the status quo’s
absolute-confidentiality terms, it is worth taking seriously the risks that a
court down the road might not agree with those terms48 or that a legislature
may mandate access (similar to the Idaho statute).49 Thus, our negotiated
contract approach helps mitigate against these uncertainties, as it affords a
more tailored and flexible solution that meets the parties’ interests.
Additionally, a negotiated approach allows each side of the potential
transaction to decline proposed licensing terms. There is informational value
in knowing which vendors and which government agencies value
transparency and which do not. For instance, by encouraging market
negotiations about transparency, our approach could deliver valuable
information about how much transparency vendors may be willingly to
tolerate. If vendors say “no” to especially pro-transparency jurisdictions, the
willingness to forego seemingly profitable licensing deals tells public policy
makers just how high a price vendors place on secrecy. Similarly, on the
government side, agencies are likely to prioritize the benefits that AI
technologies provide over transparency. When AI produces the results that
governmental officials want, such as fingerprinting, image identification, or
DNA matching, few questions are likely to be raised about the validity of their
algorithmic models.50 Even more, after an agency has adopted an AI tool that
See Jay Kesan et al., Information Privacy and Data Control in Cloud
Computing: Consumers, Privacy Preferences, and Market Efficiency, 70
WASH. & LEE L. REV. 341, 424 (2013).
45
Id.
46
A Snapshot of Government-Wide Contracting for FY 2020, WATCHBLOG
(June 22, 2021), https://www.gao.gov/blog/snapshot-government-widecontracting-fy-2020-infographic.
47
See Andrea Matwyshyn, Privacy, The Hacker, 87 S. CAL. L. REV. 1, 5
(2013) (arguing that contract law can be used as a means to protect
consumer privacy).
48
State v. Pickett, 246 A.3d 279 (N.J. Super. Ct. App. Div. 2021).
49
See IDAHO CODE § 19-1910 (2020).
50
See Erin Murphy, The New Forensics: Criminal Justice, False Certainty,
44

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

10

produces the desired results, there is little incentive for the agency to question
the tool.51 Ultimately, and for accountability, an agency’s choices and actions
convey valuable information to relevant constituents.
Yet another advantage of our negotiated approach is that reliance on
constitutional and legal minima may not be optimal. If the best alternative to
negotiated transparency is that “base” level to which a party may be deemed
constitutionally (or otherwise) entitled as a post-litigation remedy, it may be
largely insufficient. Not only is it likely to benefit or be directed toward only
that party (rather than the general public), but in the ex post litigation approach
courts are relatively limited in that they may enforce only the degree of
transparency that is deemed “necessary” under the law. However, such
minima may not be optimal for the particular circumstances (or more
broadly). Instead, our focus on ex ante, negotiated transparency holds at least
the potential for more transparency.
Fourth, our approach is consistent with procurement policy. The
transaction-by-transaction approach is consistent not only with trade secret
law but with existing federal and general law and policy on government
procurement. While procurement tends to conjure images of government
transactions based on awards to the lowest bidder, it is important to
understand that procurement law and policies are fundamentally about more
than just price.52 Accordingly, procurement law includes the kind of flexibility
that would permit government agencies to negotiate and contract for the kinds
of terms that we propose for greater transparency. Indeed, the World
Economic Forum’s guidelines for AI procurement encourage consideration of
both trade secrecy protections for vendors and possibilities for facilitating
transparency.53 Our proposal could supplement those guidelines with
additional specificity on how to negotiate transparency within the confines of
US law.
In sum, there are three features of procurement law that support our
proposal: competitive negotiation, qualification, and consideration of
collateral social and economic policies. Competitive negotiation allows an
agency significant discretion to establish criteria, other than price, to be
included in its solicitation or request for proposals (including our limited
disclosure terms). The qualification feature supports an agency’s authority to
demand that technology it procures meets certain terms and conditions
(perhaps disclosure and performance standards) even if they are different
and the Second Generation of Scientific Evidence, 95 CALIF. L. REV. 721,
745-47 (2007).
51
See id. at 746 (noting that “[s]o long as [police and prosecutors] remain
satisfied, the [forensic method] laboratories need not engage in any new
development or self-criticism.”).
52
Joshua Schwartz, Cases and Materials for a Survey of Government
Procurement Law (2021).
53
WORLD ECON. F., GUIDELINES FOR AI PROCUREMENT 10 (2019),
https://www3.weforum.org/docs/WEF_Guidelines_for_AI_Procurement.
pdf.

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

11

from what is required by the private sector or even other agencies. Finally, an
established history of using procurement law to achieve social and economic
policies in other contexts (e.g., nondiscrimination and small business support)
fits entirely with our proposal to use contract terms to further algorithmic
transparency, especially in the absence of appropriate legislation or other
regulations.54 Indeed, we entertain the possibility that a by-product of using
procurement in this context is that it may spur legislation, particularly as the
public and corporate interests realize that the executive branch could be
establishing de facto norms through the procurement process.
As the Article proceeds, Part I uses the criminal justice system as a casestudy from which to analyze the transparency issue and provides summary
background on the current status of the procurement and use of forensic
algorithmic tools by law enforcement. It also explores legal challenges and
considerations, including cases where criminal defendants have (mostly
without success) sought access to source code through the courts or FOIA.
Part II presents the tension between trade secrecy and transparency,
illustrating that the very foundations of this area of intellectual property
including its competitive purpose, property rationale, and the absence of
robust consideration of the public interest in governmental transparency make
the two areas (secrecy and transparency) seemingly inconsistent. Part III then
attempts to identify the common ground that could bridge the gap between
these two areas with a contractual approach that already fits the procurement
practices of government agencies and the existing framework of protective
measures for trade secret disclosure. It explores the specific shortcomings of
the current system of acquisition of artificial intelligence and other
algorithmic technologies in the criminal justice system and discusses the types
of contractual provisions that might be negotiated by private developers and
government agencies to simultaneously protect proprietary interests while
permitting limited disclosure. The section ends by exploring, along with
potential drawbacks, the benefits of this approach for the various stakeholders
including, better control and accountability, preserving the integrity of the
criminal justice system and creating more certainty and consistency for
vendors.

See generally, Christopher McCrudden, Buying Social Justice: Equality,
Government Procurement, and Legal Change (2007).

54

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

12

I. ALGORITHMS IN THE CRIMINAL JUSTICE SYSTEM
An increasing number of jurisdictions continue to adopt statistical
algorithmic software in various criminal justice contexts in an attempt to
maximize resources, reduce bias, and promote justice.55 Forensic
technologies that incorporate algorithms are utilized throughout the system
including for facial recognition, DNA analysis, fingerprint analysis, and
ballistic analysis.56 As specifically relevant to this exemplar, they are also
used at the law enforcement level, during trial as evidence, and for sentencing
determinations. Despite the well-intentioned motivations to adopt such
technologies, there are many instances where algorithm implementation
occurs before rigorous testing has been conducted.57
Algorithms in the criminal justice system are intended to make the
process more efficient, cost-effective, and fair, but unfortunately this is not
always the case.58 While in general, algorithms help humans perform tasks
faster, artificial intelligence systems are ultimately built by humans and do
not contain the independent ability to evaluate moral or ethical distinctions
beyond how the algorithm or machine is initially programmed.59 Indeed,
55

Risk of recidivism predictions are most commonly considered in frontend sentencing but can be a factor for consideration in back-end sentencing
where parole revocation is at issue. See Eric Holder, National Association
of Criminal Defense Lawyers 57th Annual Meeting and 13th State
Criminal Justice Network Conference, U.S. DEP’T OF JUSTICE (Aug. 1,
2014) https://www.justice.gov/opa/speech/attorney-general-eric-holderspeaks-national-association-criminal-defense-lawyers-57th;
Jeremy
Travis, Back-End Sentencing: A Practice in Search of a Rationale, 74 SOC.
RES. 631, 632-34, 637-38 (2007) (front-end sentencing occurs in criminal
courts and is more transparent and legally constrained than back-end
sentencing, which involves parole revocation that only needs to be found
by a preponderance of evidence and does not provide the parolee with the
same rights afforded to criminal defendants).
56
See, e.g., GAO Survey of federal forensic algorithmic technologies,
available
at
https://www.gao.gov/assets/gao-20479sp.pdf#:~:text=Federal%20law%20enforcement%20agencies%20GA
O; see also Wexler, supra note 30, at 1363-64.
57
See, e.g., Brauneis & Goodman, supra note 30, at 152 (describing
researchers’ inability to obtain records about the creation and
implementation of algorithms already in use in twenty-three States);
Kashmir Hill, Wrongfully Accused by an Algorithm, N.Y. TIMES,
https://www.nytimes.com/2020/06/24/technology/facial-recognitionarrest.html (last updated Aug. 3, 2020) (describing DataWorks’ (a facial
recognition company) lack of accuracy or bias testing for its algorithm that
has been on the market since 2005).
58
See SLOBOGIN, supra note 30 (noting that algorithmic decision making
in the criminal justice system could provide less biased outcomes than
those by humans).
59
Ashley M. London & James B. Schreiber, AI Report: Humanity Is
Doomed. Send Lawyers, Guns, and Money!, 58 DUQ. L. REV. 97, 105
(Winter 2020).

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

13

while an algorithmic approach may seem objective on the surface, human
developers preprogram every system with factors weighed with inherent
power differentials decided upon by the initial developers, as well as what
results define success.60 The wide ranging presence of AI at every stage of the
criminal justice system raises concerns about unchecked uses and the
potential for complicating and exacerbating systemic problems.61
Nevertheless, these adoptions will undoubtedly continue to flourish,
especially because they offer benefits and efficiencies. While it is beyond the
scope of this Article to delve into a more comprehensive sampling, one
representative example (risk assessment algorithms) is discussed in more
detail below.

A. The Example of Risk Algorithms
The U.S. criminal justice system has increasingly come to rely on
algorithms to sentence criminal defendants in response to calls for reducing
racial disparities and mass incarceration. According to the NAACP, the
United States comprises 5% of the world’s population yet has 25% of the
world’s prisoners.62 Of those prisoners, African Americans constitute 34% of
the 6.8 million prisoners in the United States—five times the rate of
incarceration of non-minorities.63 Evidence-based initiatives, academics and
sentencing commissions assert, can combat these disparities by determining
more consistent criminal penalties based on less biased predictions of a
defendant’s risk of reoffending.64 Proposed revisions of the Model Penal Code
(“MPC”), for example, currently support the use of “actuarial instruments or
processes” to estimate individual risks to public safety and advocate their
formal incorporation into sentencing guidelines.65
See id. See also Ngozi Okidegbe, Discredited Data (February 18, 2021).
CORNELL L. REV., Vol. 107 (2022 forthcoming), available at
SSRN: https://ssrn.com/abstract=3835414 (discussing biases resulting
from training algorithms using carceral data sources).
61
See infra Part I.B. See generally Sonja B. Starr, Evidence-Based
Sentencing and the Scientific Rationalization of Discrimination, 66 STAN.
L. REV. 803, 815 (2014).
62
Criminal Justice Fact Sheet, NAACP, https://www.naacp.org/criminaljustice-fact-sheet/ (last visited Sep. 9, 2020).
63
Id.
64
See generally Starr, supra note 61, at 815.
65
Id. at 815; MODEL PENAL CODE § 6B.09 Sentencing cmt. (a) at 53, 55
(AM. LAW INST., Tentative Draft No. 2, 2011) (The MPC argument in
favor of evidence-based sentencing is demonstrated by the official
commentary: “Responsible actors in every sentencing system - from
prosecutors to judges to parole officials - make daily judgments about …
the risks of recidivism posed by offenders. These judgments, pervasive as
they are, are notoriously imperfect. They often derive from the intuitions
and abilities of individual decisionmakers, who typically lack professional
training in the sciences of human behavior.”).
60

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

14

The United States’ prison population has over 2.2 million people with
current statistics indicating that historically marginalized groups are
overrepresented in the incarcerated population.66 Proponents of AI in the
criminal justice system (including prosecutors, academics, law professors,
law enforcement officers, and software developers) assert that algorithms are
an ideal solution for reducing high levels of incarceration and maintaining
consistent prison sentencing across all racial groups because of AI’s accurate
and efficient predictive modeling capabilities.67 For example, former
prosecutor and New Jersey Attorney General Anne Milgram favors
algorithms in the criminal justice system because she believes data and
analytics can be useful in determining risk-levels for pre-trial holding and
sentence durations in a way that would reduce prison populations by allowing
low-risk offenders to be released.68 Another proponent, Richard Berk, a
criminology and statistics professor at the University of Pennsylvania, has
designed various algorithms currently used in Pennsylvania, which Berk says
are more accurate, fair, and transparent than judicial discretion subject to
unconscious bias.69
Criminal
Justice
Facts,
THE
SENTENCING
PROJECT,
https://www.sentencingproject.org/criminal-justice-facts/ (last visited Jul.
20, 2020) (Noting that White men have a 1 in 17 probability of
imprisonment, while Black men and Latino men have a 1 in 3 and 1 in 6
chance, respectively. Similarly, White women have a 1 in 111 probability
of imprisonment, while Black women and Latino women have a 1 in 18
and 1 in 45 chance, respectively).
67
See, e.g., SLOBOGIN, supra note 30; Coglianese & Dor, supra note 30
(arguing that utilizing artificial intelligence in the administrative context
has the same benefits as use in the private sector, namely accuracy and
efficiency, that allow for more accurate forecasts for governmental
decision-making); Sam Corbett-Davies, Sharad Goel, & Sandra GonzálezBailón, Even Imperfect Algorithms Can Improve the Criminal Justice
System,
N.Y.
TIMES
(Dec.
20,
2017),
https://www.nytimes.com/2017/12/20/upshot/algorithms-bail-criminaljustice-system.html (noting that in New York, algorithmic risk scores
allow for more consistency to combat disparities resulting from individual
judge preference where stricter judges impose bail twice as often lenient
judges).
68
Anne Milgram, Why smart statistics are the key to fighting crime, TED
(Oct.
2013),
https://www.ted.com/talks/cathy_o_neil_the_era_of_blind_faith_in_big_
data_must_end?language=en (Ms. Milgram later accepted a role at the
Arnold Foundation and led a team of researchers and statisticians to build
a universal risk assessment tool to predict whether an individual is likely
to commit an act of violence if released. Ms. Milgram hopes that this
universal risk assessment tool will eventually be used by every judge in
the United States.).
69
Dana Casadei, Predicting Prison Terms and Parole, DOWNTOWN
PUBLICATIONS
(Mar.
24,
2020),
66

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

15

Given this apparent support, court systems have continued to adopt
algorithms for criminal justice purposes. As of August 2021, every state
except North Dakota has adopted some form of algorithm for making risk
assessment determinations for different proceedings within the criminal
justice system.70 More local jurisdictions within each state71 are adopting
forensic algorithms for additional purposes beyond risk assessment, including
facial recognition,72 bail determination,73 criminal sentencing,74 and DNA
matching.75
Contrary to state practices, the federal criminal justice system has not
readily adopted risk assessments. Indeed, federal risk assessments virtually
disappeared from federal sentencing when formal guidelines were instituted
https://www.downtownpublications.com/singlepost/2020/03/24/Predicting-prison-terms-and-parole.
70
Algorithms in the Criminal Justice System: Pre-Trial Risk Assessment
Tools, EPIC.ORG, https://epic.org/algorithmic-transparency/crim-justice/
(last visited Sep. 9, 2021); PUBLIC SAFETY ASSESSMENT,
https://www.psapretrial.org/about (last visited March 20, 2020); see also
Rhys Dipshan, Victoria Hudgins, & Frank Ready, The United States of
Risk Assessment: The Machines Influencing Criminal Justice Decisions,
LAW.COM
(Jul.
13,
2020,
07:00),
https://www.law.com/legaltechnews/2020/07/13/the-united-states-ofrisk-assessment-the-machines-influencing-criminal-justicedecisions/?slreturn=20200816100945.
71
Coglianese & Dor, supra note 30 (based on researchers’ telephone and
email exchanges with the National Center for state Courts, there are
approximately 15,000 to 17,000 different state and municipal courts,
which range based on changes in size and organization of the latter); see
also
State
Court
Structure
Chart,
COURTSTATISTICS.ORG,
http://www.courtstatistics.org/state_court_structure_charts?SQ_VARIAT
ION_28850=0 (last visited Sep. 17, 2020).
72
See, e.g., Hill, supra note 57 (detailing the story of a wrongfully arrested
Black man based on a flawed match from a facial recognition algorithm).
73
See, e.g., Holland v. Rosen, 277 F. Supp. 3d 707, 718 (D.N.J. 2017)
(allowing for the use of a public safety assessment algorithm to weigh nine
objective factors, excluding race and gender considerations, to influence
bail determinations based on statistical likelihood of failure to appear, new
criminal activity, and new violent criminal activity).
74
See, e.g., State v. Loomis, 881 N.W.2d 749, 769-70 (Wis. 2016)
(allowing for input from an algorithm predicting the risk of a recidivism
for making a sentencing determination).
75
Computers Are Helping Justice, CYBERGENETICS (Jun. 16, 2017),
https://www.cybgen.com/information/newsroom/2017/jun/Cybergenetics
-to-New-York-Times-Computers-are-helping-justice.shtml (In the case
against Darryl Pinkins, who was misidentified and wrongfully convicted
of rape and robbery, TrueAllele led to release and exoneration of Pinkins
after twenty-seven years of incarceration. TrueAllele was able to identify
additional DNA samples that FBI interpretation had initially failed to
recognize.).

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

16

to require judges to issue backward-looking sentences based on culpability
instead of forward-looking sentences considering risk of future crime.76
Congressional and Executive delays in adopting algorithm-based risk
assessments may also be attributed to constitutional limitations77 and
federalism.78 Additionally, federal statutes are subject to jurisdictional
restraints, unlike state legislatures, which are able to exercise plenary police
powers without being limited to enumerated powers contained in the
Constitution.79 Despite these limitations, Congress may nevertheless enact
substantive and procedural criminal laws under the Necessary and Proper
Clause when such a law is reasonably related to other constitutionally
enumerated powers.80
Though federal courts and prison systems have not been as quick to adopt
algorithmic tools (concrete adoption statistics do not appear to be readily
ascertainable), the trend may be changing. For example, on December 21,
2018, President Trump—with bipartisan support—signed the First Step Act
(“FSA”) into law.81 The FSA is focused on reducing recidivism through
76

Brandon L. Garrett, Federal Criminal Risk Assessment, 41 CARDOZO L.
REV. 121, 123-24 (Oct. 2019).
77
Reid v. Covert, 354 U.S. 1, 5-6 (1957) (“The United States is entirely a
creature of the Constitution. Its power and authority have no other source.
It can only act in accordance with all the limitations imposed by the
Constitution”); U.S. CONST. ART. I, §1 (“All legislative Powers herein
granted shall be vested in a Congress of the United States ...”); U.S.
CONST. ART. I, §8, cl. 6 (granting Congress the power to provide for
punishment for counterfeiting; U.S. CONST. ART. I, §8, cl. 10 (allowing
Congress to define and punish piracies and felonies committed on the high
seas, and offenses against Laws of Nations); U.S. CONST. ART. III, §3
(empowering Congress to set punishment requirements for treason).
78
U.S. CONST. amend. X (“The powers not delegated to the United States
by the Constitution, nor prohibited by it to the States, are reserved to the
States respectively, or to the people.”).
79
United States v. Lopez, 514 U. S. 549, 567 (1995); see also Engle v.
Isaac, 456 U.S. 107, 128 (1982) (“The states possess primary authority for
defining and enforcing the criminal law. In criminal trials they also hold
the initial responsibility for vindicating constitutional rights. Federal
intrusions into state criminal trials frustrate both the states' sovereign
power to punish offenders and their good-faith attempts to honor
constitutional rights.”).
80
U.S. CONST. ART. I, §8, cl. 18 (“The Congress shall have Power To make
all Laws which shall be necessary and proper for carrying into Execution
the foregoing Powers, and all other Powers vested by this Constitution in
the Government of the United States…”).
81
U.S. Department of Justice, The First Step Act of 2018: Risk and Needs
Assessment System, NATIONAL INSTITUTE OF JUSTICE (2019),
https://nij.ojp.gov/sites/g/files/xyckuh171/files/media/document/the-firststep-act-of-2018-risk-and-needs-assessment-system_1.pdf; First Step Act
of 2018, Pub. L. No. 115-391, sec 101, § 3635(6), 132 Stat. 5194, 5208–
10; see also Coglianese & Dor, supra note 30.

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

17

prison reform and allows for governmental contracting to develop the
Prisoner Assessment Tool Targeting Estimated Risk and Needs
(“PATTERN”), an algorithmic instrument that predicts the likelihood of
recidivism for parolees within three years of federal prison release.82 FSA and
PATTERN were enacted and developed in response to federal legislation
calling for risk assessments in federal parole decisions.83 Development and
finalization of PATTERN is still ongoing,84 but federal enactment of both
FSA and PATTERN seem to be indicative of expanding predictive algorithm
use in the federal criminal justice system.
It is worth noting at this juncture that when we speak broadly about “the
criminal justice system,” there are differences between federal and state
agencies and practices. Thus, there is no unified system, and this inevitably
leads to inconsistencies and wide-ranging, divergent practices and policies
(even within states).85

82

U.S. Department of Justice, supra note 81 (The FSA report states
that the PATTERN instrument’s model prediction algorithm based on
static risk factors and “dynamic items that are associated with either an
increase or a reduction in risk” comply with FSA requirements); see also
Brandon Garrett & John Monahan, Assessing Risk: The Use of Risk
Assessment in Sentencing, JUDICATURE (Summer 2019),
https://judicature.duke.edu/articles/assessing-risk-the-use-of-riskassessment-in-sentencing/ (noting the FSA, “perhaps the most farreaching federal sentencing reform in a generation, mentions risk no less
than 100 times and relies on risk assessments to allocate prison
programming and prisoner release”).
83
U.S. Department of Justice, supra note 81; First Step Act of 2018, Pub.
L. No. 115-391, sec 101, § 3635(6), 132 Stat. 5194, 5208–10.
84
U.S. Department of Justice, supra note 81 (discussing ongoing changes
and modifications to PATTERN as a result of independent review
committees and stakeholder meetings).
85
Trevor G. Gardner & Lisa L. Miller, Criminal Justice, CENTER FOR THE
STUDY
OF
FEDERALISM
(May
2018),
http://encyclopedia.federalism.org/index.php/Criminal_Justice
(describing how the array of federal, state, and local criminal justice
systems constitute America’s decentralized approach to criminal justice
and often results in differing policies surrounding the same issue, such as
capital punishment).

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

18

B. Procurement of Algorithms
Procurement is the process by which government agencies may enter into
contracts for the “principal purpose [… of] furnish[ing] services in the United
States through the use of service employees.”86 There are separate federal and
state procurement systems. State adoptions of technologies for use in the
criminal justice system are possible through licensing procurement based on
broad legislative permissions, which typically require adherence to only
minimal standards of justice.87 Beyond that, states have discretion to negotiate
and enter into terms and agreements with vendors.88 For example, as a
condition for utilizing risk recidivism algorithms, developers will often
require the user—including law enforcement and judiciaries—to sign a
Memorandum of Understanding (“MOU”) prior to implementation to prohibit
disclosure of algorithms deemed to be trade secrets.89 How states choose to
respond to MOU requirements and handle implementation is largely based on
principles of federalism in that there is no uniform Federal requirement
equally applicable to the states.90 In Florida, for example, private parties must
expressly designate what information is considered a trade secret, otherwise
confidentiality is considered waived.91
Government agents responsible for procurement generally do not have
guidance on best practices when comparing and evaluating different artificial
intelligence tools, which sometimes results in premature algorithm
29 C.F.R. § 10.2; see also 41 U.S.C. § 6701(1)(A) (defining a service
employee as an individual performing duties under a government contract
for the benefit of the United States).
87
Christopher Bavitz & Kita Hessekiel, Examining the Role of the State in
the Development and Deployment of Algorithmic Technologies, BERKMAN
KLEIN CENTER FOR INTERNET & SOCIETY AT HARVARD UNIVERSITY (Jul.
11,
2018),
https://cyber.harvard.edu/story/2018-07/algorithms-andjustice.
88
See, e.g., Livescan Device Vendors and Livescan Service Providers,
Florida
Dep’t
of
Law
Enforcement,
https://www.fdle.state.fl.us/CJIS/Livescan-Contracts (last visited Sep. 8,
2021) (listing the current Florida law enforcement contracts with biometric
companies); EXEC. ORDER NO. 14,006 (Jan. 26, 2021).
89
Brauneis & Goodman, supra note 30, at 139 (Developers often try to
obtain written confidentiality agreements or MOUs for algorithm use, even
in the criminal justice context).
90
U.S. CONST. amend. X (“The powers not delegated to the United States
by the Constitution, nor prohibited by it to the States, are reserved to the
States respectively, or to the people.”).
91
See e.g., MOU Between the Laura and John Arnold Foundation and the
Seventh Judicial Circuit of the State of Florida 3 (2015),
http://cdn.muckrock.com/foia_files/2016/12/15/Memorandum_of_Under
standing.pdf [http://perma.cc/4QAE-NUCP] (detailing the Foundation’s
obligation to specifically designate any information it considers to be trade
secret).
86

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

19

deployment.92 Lack of implementation guidelines have been a strong point of
criticism. Harvard’s Criminal Justice Policy Program argues that without
proper calibration to reflect the relevant jurisdiction, any potential benefits
from the algorithm may be undermined by disparities or displacement of other
considerations.93 Further, court adoption of algorithms does not typically
require judicial training. Therefore, for risk assessments, judges may not
know how the software works, what exactly the numerical risk score means,
or whether the risk score relies solely on populational outcomes derived from
data based on individuals with certain characteristics that the developer
arbitrarily chose.94 In other words, the algorithm may be accurate for group
averages, but not for specified individuals within the group, which poses
significant problems if judges erroneously give undue weight to algorithmicbased risk scores. If proper validation and review are not in place, these
problems may not be discovered until well after the predictive algorithm has
been in use.
1. Exemplar Developers
Although they vary, state court systems have adopted algorithms
developed by for-profit companies, non-profit organizations, or government
units, to determine bail, pre-trial detention, sentencing, prison management,
and parole.95 Northpointe (now Equivant), a private for-profit company
established in 1989, developed one of the most widely used algorithms for
risk assessment: Correctional Offender Management Profiling for Alternative
Sanctions (“COMPAS”).96 Equivant’s COMPAS differentiates between risk
scales and needs scales.97 The COMPAS risk scales attempt to predict
recidivism, with the purpose of “discriminat[ing] between offenders who will
and will not recidivate,” whereas the needs scales attempt to “describe the
Bavitz & Hessekiel, supra note 87.
Moving Beyond Money: A Primer on Bail Reform, HARVARD LAW
SCHOOL CRIMINAL JUSTICE POLICY PROGRAM (Oct. 2016),
http://cjpp.law.harvard.edu/assets/FINAL-Primer-on-Bail-Reform.pdf.
94
Id. (These issues are especially prominent when algorithms are
developed by private companies. For example, when risk assessments
characterize certain risks as “high,” “moderate,” or “low” based on a
policy assessment as opposed to true statistics, judges may give undue
weight to deciding in favor of a specific outcome).
95
Algorithms in the Criminal Justice System, supra note 70; PUBLIC
SAFETY ASSESSMENT, supra note 70.
96
Northpointe
Software
Suite,
NORTHPOINTE,
http://www.northpointeinc.com/files/downloads/Northpointe_Suite.pdf
(last visited Jul. 1, 2020).
97
Practitioner’s Guide to COMPAS Core, EQUIVANT (Apr. 4, 2019),
http://www.equivant.com/wp-content/uploads/Practitioners-Guide-toCOMPAS-Core-040419.pdf; see generally Nathan James, Risk and Needs
Assessment in the Federal Prison System, CONGRESSIONAL RESEARCH
SERVICE (2018), available at https://sgp.fas.org/crs/misc/R44087.pdf
92
93

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

20

offender.”98 According to Equivant, COMPAS is comprised of forty-three
scales that are user-configurable at different decision points based on
populational and local criminal justice system needs.99
In a study by ProPublica, researchers discerned that the COMPAS
algorithm relies on scores derived from various combinations of the 137
questions answered by either the defendant or criminal records, though the
calculations used to determine the risk score are not publicly disclosed due to
trade secrecy assertions.100 COMPAS and other algorithms serve as pre-trial
risk assessments, though courts use the scores at varying stages of criminal
proceedings. As of 2016, at least nine states allow judges to consider
COMPAS scores when making sentencing determinations.101
On the non-profit side, the Arnold Foundation has been a leading
developer of risk assessment software aiming to improve the criminal justice
decision-making process by developing safer, fairer, and cost-effective datadriven risk assessment tools.102 The Foundation developed an automated
Public Safety Assessment (“PSA”) based on existing Kentucky data from
criminal defendant interviews because it viewed Kentucky as a data collection
leader in the pretrial field. Early versions of the PSA evaluated nine criminal
history factors and three questions from defendant interviews, but had a goal
of removing the interview-dependent questions.103 The Foundation
considered “hundreds of risk factors,” including those related to prior arrests,
prior convictions, drug and alcohol use, mental health, familial status, and
employment, and determined that the algorithm alone (without the defendant
interview) was highly effective in its predictions.104 Later versions of the PSA
provide a judicial dashboard with scores for “risk of violence” and “failure to

Id. at 7.
Id. at 2.
100
Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner, Machine
Bias,
PROPUBLICA
(May
23,
2016),
https://www.propublica.org/article/machine-bias-risk-assessments-incriminal-sentencing.
98
99

101

Id. (states allowing COMPAS score consideration during sentencing
include Arizona, Colorado, Delaware, Kentucky, Louisiana, Oklahoma,
Virginia, Washington and Wisconsin).
102
Research Summary: Developing a National Model for Pretrial Risk
Assessment, LAURA & JOHN ARNOLD FOUND. (Nov. 2013),
http://www.arnoldfoundation.org/wp-content/uploads/2014/02/LJAFresearch-summary_PSA-Court_4_1.pdf; see also Casadei, supra note 69
(other common algorithms include the Risk Assessment System-Pretrial
Assessment Tool (ORAS-PAT), Virginia Pretrial Risk Assessment
Instrument (VPRAI), and Virginia Pretrial Risk Assessment InstrumentRevised (VPRAI-R)).
103
Research Summary: Developing a National Model for Pretrial Risk
Assessment, supra note 102.
104
Id.

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

21

appear.”105 In an independent study conducted by filing forty-two open
records requests in twenty-three states, researchers discerned that the Arnold
Foundation created its risk assessment algorithm by analyzing data in 750,000
cases; however, there was no information about how the data was analyzed,
alternatives, or how those alternatives compared to the algorithms that were
eventually implemented.106 The authors concluded that secrecy surrounding
the PSA was the result of insufficient judicial and legislative insistence on
disclosure practices, and deference to overbroad trade secret assertions.107
2. Algorithms Need Data
Algorithms cannot operate effectively without sufficient data.108
Traditional statistical tools require human intervention to choose specific
variables and to select the precise mathematical relationships between the
variables; once machine learning is involved, artificial intelligence allows
algorithms to discover correlations “on their own” after they have been
programmed to do so.109 In the criminal justice context, risk assessment
software considers factors such as socioeconomic status, family background,
neighborhood crime, employment status, education, employment history, and
demographic information to generate a high or low score with specific
percentages based on an individual’s criminal risk.110 While developers are
often reluctant to explicitly include race in these algorithms, Professor
Deborah Hellman has argued that it is legally permissible to do so and that
doing so may actually improve fairness.111
Statisticians use these demographics, along with sentencing data and
historical recidivism rates, to identify which variables occur in the most
relevant cases, and those data points are then used to create predictive
models.112 Statisticians then reverse the process to attempt to locate the
Milgram, supra note 68.
Brauneis & Goodman, supra note 30, at 139.
107
Id. at 110.
108
See generally Willem Sundblad, Data Is the Foundation for Artificial
Intelligence and Machine Learning, FORBES (Oct. 18, 2018, 10:30),
https://www.forbes.com/sites/willemsundbladeurope/2018/10/18/data-isthe-foundation-for-artificial-intelligence-and-machinelearning/#4bd8c64051b4 (“[D]ata is both the most underutilized asset of
manufacturers and the foundational element that makes AI so powerful.”).
109
Coglianese & Dor, supra note 30.
110
Algorithms in the Criminal Justice System: Pre-Trial Risk Assessment
Tools, supra note 70.
111
Deborah Hellman, Measuring Algorithmic Fairness, 106 VA. L. REV.
811, 846 (2020).
112
Angele Christin, Alex Rosenblat, & Danah Boyd, Courts and Predictive
Algorithms, DATA & CIVIL RIGHTS: A NEW ERA OF POLICING AND JUSTICE
(Oct.
27,
2015),
105
106

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

22

selected variables in new cases, which if successful, may be applied to active
cases to generate recidivism risk scores.113 Like all statistical models, the
quality of the algorithm being used depends on many factors, including
sample size, testing duration, record completeness, and modeling strategy.114
More advanced programs, such as COMPAS, analyze a large series of
dataset inputs (e.g., criminal history, age, education level) that are obtained
from public records or defendant answers to reveal which correlations best fit
the data relationship between the inputs and outcomes.115 In essence, each
factor is weighed against the algorithms’ associated risk factor input to
generate a risk score. To measure the algorithmic model’s performance, data
scientists compare false positives and false negatives that the algorithm
generates for random guesses based on group data.116 This graphical
representation creates what is known as a “receiver operating characteristic”
(“ROC”) curve that generates numerical values between 0.5 and 1 to compare
algorithm accuracy to random guesses; however, this approach only works
when the algorithm’s outputs are ranked as least-likely to most-likely to be
associated with a given outcome.117
Readers should note that the above descriptions are merely general
illustrations rather than being complete descriptions of any specific predictive
algorithm. Indeed, software developers often attempt to maintain many
operative features of how specific algorithms work as trade secrets (e.g.,
which factors are considered and how heavily these factors are weighed in the
final risk score calculation). Furthermore, in the context of predictive
algorithms and transparency, it is worth clarifying that scholars are mostly
concerned about understanding the model -- i.e., what variables are used and
how they are processed to result in a prediction -- and about understanding
the validity of the model -- to what degree is it accurate, biased, etc. A given
model might be implemented in computer code in different ways, so it is the
model, rather than how it is implemented in code, that could be revealing.
Thus, in many cases, disclosure of source code does not allow one to
understand the model, because, for example, the variables (e.g., “variable 1”)
may not be named in any way that helps to understand what they represent.
Moreover, even if the code aided in understanding the model, mere code
disclosure would not necessarily help to assess the validity of the model.118
https://www.law.nyu.edu/sites/default/files/upload_documents/Angele%
20Christin.pdf.
113
Id.
114
David Steinhard, Juvenile Detention Risk Assessment: A Practice
Guide to Juvenile Detention Reform, THE ANNIE E. CASEY FOUNDATION
(2006),
p.52,
https://www.aecf.org/m/resourcedoc/aecfjuveniledetentionriskassessment1-2006.pdf.
115
Moving Beyond Money: A Primer on Bail Reform, supra note 93.
116
Brauneis & Goodman, supra note 30, at 121.
117
Id.
118
See, e.g., SLOBOGIN, supra note 30, at 108 (discussing the Congressional

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

23

3. Inconsistent Testing and Implementation.
Due to the technical skill and financial costs associated with developing
predictive algorithms, many states are likely to require third-party contracting
for software services. Absent express state requirements for trade secret or
confidentiality designations,119 these contracts likely include default terms
requiring stringent confidentially benefiting the service provider while
simultaneously failing to provide for clear validation standards.
The below examples are only a few publicly available instances of
problematic outcomes associated with improper algorithm testing, and some
occurrences may never be publicized. A 2013 study examined nineteen
different risk methodologies and found that software developers only
examined algorithm validity in one or two studies.120 There were no external
independent reviews.121 Because states’ have the ability to exercise plenary
police powers, there is undoubtedly variability among jurisdictional uses of
algorithms, including possession rights, which makes any efforts for
establishing consistency difficult to achieve.122 This is especially true when
proper testing is not conducted for algorithms, with serious implications, such
as higher bail requirements or harsher prison sentences.
Indeed, new algorithm technologies are often prematurely adopted in the
criminal justice system without proper validation studies. For example, New
York adopted an algorithmic-based probationary pilot program which was
expanded to the entire state probation department in 2010, yet a
comprehensive statistical evaluation was not published until 2012.123 In
Michigan, Detroit police relied on DataWorks Plus’s faulty algorithmic
identification of a grainy surveillance video still to arrest a Black man for a
crime he did not commit in what was likely the first publicized faulty facial

decision to require public disclosure of only the PATTERN algorithm, not
the underlying validation studies or data).
119
See, e.g., the MOU between the Arnold Foundation and a Florida Court
requiring trade secret designation, supra note 91.
120
See Jeff Larson, Surya Mattu, Lauren Kirchner & Julia Angwin, How
We Analyzed the COMPAS Recidivism Algorithm, PROPUBLICA (May 23,
2016), https://www.propublica.org/article/how-we-analyzed-the-compasrecidivism-algorithm.
121
See id.
122
See Katyal, supra note 30, at 1244-45 (2019) (discussing limitations on
discovery orders in terms of “information within the custody possession,
or control by the State,” for which States can choose to allow code
developers to maintain possession of algorithmic source code and thus
evade the scope of discovery).
123
See Angwin, et al., supra note 100 (New York City was initially
excluded from the pilot program).

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

24

recognition case.124 Although DataWorks Plus was developed in 2005 and has
had significant time for testing, the company’s general manager confirmed
that scientific testing to formally measure accuracy or bias has not been
performed.125
As it turns out, COMPAS validity measurements take advantage of the
industry accepted standard of meeting only a 70% probability requirement
that a randomly chosen defendant is classified correctly, regardless of whether
that classification is high or low-risk of recidivism.126 This standard
essentially allows for a 30% chance that a low-risk defendant will be ranked
as high-risk and receive a longer or harsher sentence than the defendant
otherwise would have if the algorithm had predicted correctly.127
In Florida, state testing deficiencies and preliminary indications of
racially disparate impacts motivated ProPublica to obtain risk scores for more
than 7,000 people arrested in Broward County between 2013 and 2014 to
determine how many individuals were charged with a new crime over a twoyear period following their arrest—the same standard used by COMPAS
developers.128 The 2016 ProPublica Study found the risk scores to be highly
unreliable. Of the total number of people predicted to commit a subsequent
violent crime, only 20% did so.129 For those deemed likely to commit any
future crime, only 61% were arrested within a two-year period.
Some states have been more forthcoming than others with their data and
predictive algorithm implementation processes. For instance, Pennsylvania
provides predictive algorithm information on its website. However, this is
likely due to the fact that the Pennsylvania Commission on Sentencing
developed its own algorithm as opposed to contracting with a service
provider.130

124

See Hill, supra note 57 (If police had conducted further
investigation before relying so heavily on the facial recognition algorithm,
they would have discovered that the suspect had posted an Instagram video
during the time of the robbery, which showed him driving home from
work).
125
See id,
126
See John Lightbourne, Damned Lies & Criminal Sentencing Using
Evidence-Based Tools, 15 DUKE L. & TECH. REV. 327, 336 (2017).
127
See id.
128
See Angwin, et al., supra note 100.
129
See Sentencing Risk Assessment Instrument, PENNSYLVANIA
COMMISSION ON SENTENCING, http://pcs.la.psu.edu/guidelines/sentencerisk-assessment-instrument (last visited Jul. 11, 2020); see also Stephanie
Wykstra, Just How Transparent Can a Criminal Justice Algorithm Be?,
SLATE
(Jul.
3,
2018,
08:00),
https://slate.com/technology/2018/07/pennsylvania-commission-onsentencing-is-trying-to-make-its-algorithm-transparent.html
130
Id.

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

25

C. Legal Challenges & Considerations
The law always seems to trail technological advancements, and predictive
algorithms are no different, as courts have only just begun tackling the
associated legal implications of artificial intelligence. Of the cases that have
addressed challenges to predictive algorithms, little is known about how much
weight trial court judges ascribe to such risk assessments, especially in
determining whether a risk score was a dispositive factor.131 Although risk
scores, such as those provided by COMPAS, are intended to be used only as
advisory guidance, judges have cited risk scores in their sentencing
decisions.132 Despite the large impact that risk scores may have, courts have
generally been unwilling to provide defendants with access to the algorithms
to ensure proper computations. If considered persuasive precedent, these
decisions may have widespread implications in the form of algorithmic access
if other jurisdictions choose to adopt similar reasoning.
In Wisconsin, for instance, criminal justice leaders appear to favor
algorithm use. The Wisconsin Department of Corrections has adopted
algorithms in each step of criminal proceedings, and judges have cited risk
scores as factors in issuing a criminal sentence.133 For example, Paul Zilly,
convicted of stealing a push lawn mower and tools, reached a plea agreement
in which the prosecutor would recommend a year in county jail; however, the
judge reviewed Zilly’s COMPAS risk score and instead imposed a two-year
sentence in state prison followed by three years of supervision.134 After
Northpointe’s founder testified that COMPAS was not originally designed to
be used for sentencing, the judge (on appeal) reduced Zilly’s prison sentence
to 18 months.135
In State v. Loomis, the Wisconsin Supreme Court held that COMPAS
scores, when based on accurate input information, did not violate due process
protections for individual sentencing.136 The defendant challenged the risk
score, arguing that he was unable to ascertain how his risk score was
determined; it was unclear how specific factors were weighed; and that the
131

See Richard Berk, An Impact Assessment of Machine Learning Risk
Forecasts on Parole Board Decisions and Recidivism, 13 J. EXP.
CRIMINOL. 193, 193 (2017) (noting that the public has “scant information
about how actuarial risk assessments have affected practices and
outcomes”).
132
Angwin, et al., supra note 100.
133
See Julia Angwin et al., Risk Scores Attached to Defendants Unreliable,
Racially Biased, MILWAUKEE J. SENTINEL (May 30, 2016),
http://archive.jsonline.com/news/crime/risk-scores-attached-todefendants-unreliable-racially-biased-b99732973z1-381306991.html.
134
See id.
135
Id.
136
See State v. Loomis, 881 N.W.2d 749, 749, 767-68 (Wis. 2016) (the
Wisconsin Supreme Court affirmed an eight-year sentence based on the
defendant’s plea to two lesser charges associated with a drive-by shooting
and a Pre-Sentence Report (“PSR”).).

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

26

developer’s trade secret assertion amounted to withholding information
considered for sentencing purposes.137 The Court rejected this challenge by
reasoning that the PSR was accompanied by additional factors such that
courts could sufficiently assess score accuracy, and that risk scores would
only be a due process violation if used as the exclusive determinative factor.138
Another source code example (not involving a predictive tool, but DNA
analytics) comes from California. In an unpublished case (“Chubbs”), the
Court of Appeals denied the defendant’s request for algorithmic source code
used to calculate the likelihood of the presence of defendant’s DNA at the
crime scene based on a complex DNA sample.139 The Court reasoned that the
source code was protected under California’s trade secret privilege140—likely
the first decision of its kind to extend evidentiary privileges for trade secret
protection in the criminal context.141 In requiring a particularized showing that
the algorithm is necessary to the defendant’s defense, the Chubbs Court
heightened the discovery burden from the standard good cause showing.142
Instead of simply requiring a protective order, the Court withheld potentially
critical information in its entirety from the defendant.143
Some courts outside of California144 are citing Chubbs in criminal
proceedings to justify trade secret exemptions for algorithm disclosure,
including that of TrueAllele.145 Other courts, especially in Wisconsin, have
cited Loomis for at least two propositions. First, courts may consider
COMPAS risk scores without algorithmic model disclosure so long as the
score is not determinative,146 and second, COMPAS scores constitute a proper
Id. at 761.
Id. at 763-64, 768, 771.
139
See People v. Superior Court (Chubbs), No. B258569, 2015 WL
139069 (Cal. Ct. App. Jan. 9, 2015).
140
Id.
141
See Wexler, supra note 30, at 1359-60.
142
Chubbs, 2015 WL 139069.
143
Id.
144
Chubbs cannot be cited in California because it is an unpublished
opinion. See CAL.R.CT.8-1115(a) (“[A]n opinion of a California Court of
Appeal . . . that is not certified for publication or ordered published must
not be cited or relied on by a court or a party in any other action.”).
145
See Wexler, supra note 30, at 1360 (Wexler notes that both State v. Fair,
No. 10-1-09274-5 SEA, slip op. at 3 n.1 (Wash. Super. Ct. King Cty. Jan.
12, 2017) (denying a motion to compel TrueAllele’s source code) and the
Letter Regarding Motion to Quash at 2, United States v. Johnson, No.
1:15-cr-00565-VEC (S.D.N.Y. June 15, 2016) reference Chubbs to
support trade secret exemptions to disclosure).
146
See, e.g., State v. Belen, No. 2017AP293-CRNM, 2018 Wisc. App.
LEXIS 191 (Wisc. Ct. App. Feb. 14, 2018) (allowing the sentencing court
to consider the defendant’s COMPAS score following conviction of child
neglect resulting in death because the score was only reviewed, not
discussed); State v. Parrish, No. 2017AP1442-CRNM, 2018 Wisc. App.
137
138

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

27

factor within the scope of judicial discretion in sentencing.147 At least where
horizontal stare decisis is concerned, proponents of algorithmic nondisclosure suggest that the trade secret issue has already been decided such
that courts can consider risk recidivism scores while withholding certain
elements from defendants.148
Despite the apparent trend favoring increased predictive algorithm use,
some judges and civil rights groups have expressed skepticism about
algorithmic accuracy. For example, in Loomis, Judge Abrahamson’s
concurring opinion notes that COMPAS may serve as a useful tool for
sentencing considerations, but that the “court's lack of understanding of
COMPAS was a significant problem in the instant case” because few
questions could be answered about how the algorithm worked and it was
unclear as to whether the algorithm made a true individualistic
determination.149 Additionally, the Leadership Conference on Civil and
Human Rights has also suggested that algorithms can be beneficial to the
criminal justice system, but only if the models (1) are independently
validated; (2) can be challenged by defendants; and (3) are available for public
scrutiny in terms of design, structure, and accuracy.150
The criminal justice system in the United States is based on the premise
that “it is far worse to convict an innocent man than to let a guilty man go
free.”151 Allowing for sentencing decisions based on algorithms that may
falsely identify the risk level of defendants in up to 30% of cases seems
directly contrary to this notion of justice. As is evident in Loomis, courts do
consider algorithms when issuing sentencing decisions. Moreover, predictive
policing algorithms used before and during crime investigation have the
potential to contribute to the confirmation bias of unquestioned algorithm
reliability. Academics have also challenged location-based predictive
policing tools that break cities into block-by-block districts to predict when
LEXIS 316 (Wisc. Ct. App. Mar. 5, 2018) (affirming the trial court’s
decision to follow the presentence investigate report’s sentencing
recommendation because the report was not a determinative factor).
147
State v. Villanueva, 2018 WI App 28, 381 Wis. 2d 471, 915 N.W.2d
455 (Wisc. Ct. App. Mar. 6, 2018) (noting that courts can consider a wide
range of sentencing factors pertinent to the defendant, and that such
discretion is abused only if the sentence is based on improper factors).
148
See, e.g., Cary Coglianese & David Lehr, Transparency and
Algorithmic Governance, 71 ADMIN. L. REV. 1, 34 (2019) (Arguing that
full algorithm disclosure is not required—even when algorithms are
outcome determinative—because true transparency is satisfied by
contractors’ explanation of the algorithm’s purpose, design, and basic
functioning.).
149
State v. Loomis, 881 N.W.2d 749, 749, 774-75 (Wis. 2016).
150
See Greg Chaney, The Criminal Justice System’s Algorithms Need
Transparency,
LAW 360
(Mar.
31,
2019,
8:02
PM),
https://www.law360.com/articles/1143086/the-criminal-justice-system-salgorithms-need-transparency.
151
In re Winship, 397 U.S. 358, 372 (1970) (Harlan, J., concurring).

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

28

crimes might occur based on historical data.152 And investigative reporters
are still finding new uses of predictive policing tools that have not been
publicly disclosed.153 Given the level of importance placed on such
algorithms, it should be apparent that statistical accuracy should be an
important consideration prior to algorithm implementation, otherwise such
decisions will only reinforce public confidence in risk scores that have not
been fully validated and are based on standards well below scientifically
accepted confidence intervals.154
1. Access Denied
In Loomis and Chubbs, the Courts considered whether to permit
disclosure of algorithmic models.155 Many algorithmic models are considered
“black boxes,” or systems whose inputs and outputs may be known, but whose
internal workings are unknown.156 These models use information in an
unknown manner to produce results or predictions that appear facially neutral
but may actually yield discriminatory results.157
Aside from the black box algorithmic models, the inputs and outputs of
data used by the algorithmic models can also be problematic. Algorithms are
152

The most common locational algorithm is PredPol, which updates its
predictions throughout the day. See Will Douglas Heaven, Predictive
Policing Algorithms Are Racist. They Need to Be Dismantled.,
MIT TECH . REV . (July 17, 2020),
https://www.technologyreview.com/2020/07/17/1005396/predictivepolicing-algorithms-racist-dismantled-machine-learning-biascriminal-justice/.
153
See, e.g., Ali Winston, Palantir Has Secretly Been Using New Orleans
To Test Its Predictive Policing Technology, THE VERGE (Feb. 27, 2018,
3:25 PM), https://www.theverge.com/2018/2/27/17054740/palantirpredictive-policing-tool-new-orleans-nopd (discussing the New Orleans
Police Department’s use of Palantir for data mining to predict which
individuals may commit violent crimes—a use City Council members
were not even aware of).
154
See supra Part I.B.3.
155
See State v. Loomis, 881 N.W.2d 749, 763-64, 768, 771 (Wis. 2016)
(finding that the COMPAS model did not have to be disclosed because it
was not the sole factor in sentencing); People v. Superior Court (Chubbs),
No. B258569, 2015 WL 139069 (Cal. Ct. App. Jan. 9, 2015) (determining
that requiring algorithm disclosure would be a violation of California’s
trade secret law).
156
The issue of the “black box” has been ongoing for over a decade. See,
e.g., Elizabeth A. Rowe, Striking a Balance: When Should Trade-Secret
Law Shield Disclosures to the Government?, 96 IOWA L. REV. 791, 82635 (2010) (addressing when the government can request disclosure of
“black box” algorithms).
157
See Anupam Chandler, The Racist Algorithm?, 115 MICH. L. REV.
1023, 1024-25 (2017).

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

29

trained to operate based on real-world facts. If the inputs an algorithm relies
on are inherently biased, then the resulting outputs are also likely to be
biased.158 Because algorithms often rely on established data and statisticiandetermined inputs,159 the inputs have potential to exacerbate racial disparities
if statisticians fail to revise underlying algorithmic models reliant upon
existing criminal statistics. Thus, the algorithmic models may be facially
neutral but still generate even unintentional race-based discrepancies.160
Ultimately, criminal defendants are unable to challenge what appear to be
algorithmic model defects without some form of access to the underlying
information. Advocates for opening algorithmic black boxes to partial
disclosure cite studies indicating patterns of bias, inaccurate predictions, and
discrimination.161 Data scientist Cathy O’Neil has continuously found that
mathematical models are not free of bias, but instead reinforce discrimination,
especially where race and lower socioeconomic status are concerned, because
algorithms are designed for the masses instead of tailored to individual
characteristics.162 For example, O’Neil notes that algorithm developers can
influence the concentration of law enforcement officers in minority
neighborhoods by using arrest data from specific areas instead of the relevant
jurisdiction as a whole.163 This may seem unlikely at first glance, but ZIP code
reliance can provide significant information that algorithms rely on for
predictive outcomes.164
Whether intentional or not, relying on such data in the criminal justice
system without proper testing and verification procedures may create biased

Id.
See generally supra Part I.B for a discussion on how algorithms are
developed.
160
For example, as discussed in Part I data from certain ZIP codes may
serve as a proxy for racial discrimination.
161
See also Part I.B.3.
162
CATHY O’NEIL, WEAPONS OF MATH DESTRUCTION: HOW BIG DATA
INCREASES INEQUALITY AND THREATENS DEMOCRACY 9 (2016).
163
Cathy O'Neil, The Era in Blind Faith in Big Data Must End, TED (Apr.
2017),
https://www.ted.com/talks/cathy_o_neil_the_era_of_blind_faith_in_big_
data_must_end?language=en.
164
For instance, in the marketing context, by using propensity models
to determine the likelihood of certain outcomes, ZIP codes allow
algorithms to make determinations about income, education level, family
composition, and lifestyle to such an extent that vendors can alter online
pricing and availability based on shopper locations. See Katherine Noyes,
Will Big Data Help End Discrimination—or Make it Worse?, FORTUNE
(Jan. 15, 2015, 3:16 PM), https://fortune.com/2015/01/15/will-big-datahelp-end-discrimination-or-make-it-worse/ (ZIP codes often serve as
proxies for advertising tactics that provide numerous insights beyond
location).
158
159

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

30

and discriminatory outcome predictions,165 and in some instances, erroneous
results.166 Indeed, the ProPublica Study, even when criminal history,
recidivism, gender, and age were isolated, confirmed bias and discrimination
in the COMPAS algorithm in finding that Black defendants were 77% more
likely than Whites to be identified as high-risk for likelihood of committing a
future violent crime.167 Even if the COMPAS algorithmic model does not
explicitly consider race, it may consider ZIP code, thus providing a possible
explanation for the racial disparities.
Another study conducted by Megan Stevenson found that Kentucky’s
implementation of mandatory algorithm review before judges decide whether
to hold a criminal defendant in jail before trial resulted in an increase in the
number of White defendants being released prior to trial, while the percentage
of Black defendants released prior to trial remained the same—a change that
effectively created new inequities that were not present in the Kentucky bail
system prior to the algorithm.168 One possible explanation for this change
could be attributed to population density such that rural area judges granted
release without bail more often than urban communities, the latter of which
contain more diverse populations.169 A different explanation based on a
formal academic study, however, indicated that judges were more likely to
impose bail as a condition for release for moderate-risk defendants who were
Black and of lower socioeconomic status, despite the default recommendation

See Criminal Justice Facts, THE SENTENCING PROJECT,
https://www.sentencingproject.org/criminal-justice-facts/ (last visited Jul.
20, 2020) (Noting that White men have a 1 in 17 probability of
imprisonment, while Black men and Latino men have a 1 in 3 and 1 in 6
chance, respectively. Similarly, White women have a 1 in 111 probability
of imprisonment, while Black women and Latino women have a 1 in 18
and 1 in 45 chance, respectively.).
166
See, e.g., New Orleans Metropolitan Crime Commission Calls Arnold
Foundation Public Safety Assessment “Flawed” as the Pretrial Justice
Institute Attempts to Pivot Again, AMERICAN BAIL COALITION (Jul. 9,
2019),
https://ambailcoalition.org/new-orleans-metropolitan-crimecommission-calls-arnold-foundation-public-safety-assessment-flawed-asthe-pretrial-justice-institute-attempts-to-pivot-again/ (detailing the New
Orleans Metropolitan Crime Commission’s findings that the Arnold
Foundation’s public safety risk assessment algorithm recommended free
bond release for 75% of violent felony suspects and 93% of weapons
felony suspects).
167
See Angwin et al., supra note 63.
168
See Megan Stevenson, Assessing Risk Assessment in Action, 103 MINN.
L. REV. 304 (Nov. 2018).
169
See Tim Simonite, Algorithms Should’ve Made Courts More Fair. What
Went
Wrong?,
WIRED
(Sep.
5,
2019,
7:00
AM),
https://www.wired.com/story/algorithms-shouldve-made-courts-morefair-what-went-wrong/.
165

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

31

that bond be waived.170 This study asked 340 judges to decide sentences for
hypothetical defendants based on drug charges, with half of the cases
including medium to high risk scores for rearrest.171 Even after controlling for
gender and race, likelihood of incarceration was higher for poorer defendants,
which may correlate with race.172
These studies are troubling because criminal justice algorithms,
which are meant to ensure a fairer system, can create underlying bias and
discrimination absent proper disclosure and testing in at least two ways, with
the potential for overlap. First, if the algorithm is based on data that is not
publicly available for bias testing and is subject to trade secret protection,
sentencing judges may be overly relying on algorithmic data predictions that
have built in discriminatory factors. Second, risk assessment scores can alter
judicial discretion in the criminal sentencing context to make disparities
worse, especially for Black defendants and defendants of lower socioeconomic status. To combat these problems, researchers suggest allowing for
data integrity checks to account for bias and reconsidering definitions of
success to include occurrences outside of the status quo.173 But to date, a
significant number of jurisdictions continue to adopt algorithms without
taking these precautionary steps.

2. State v. Pickett
Most recently, a New Jersey state court recognized these values and
concerns. In the closely-watched case of State v. Pickett,174 the New Jersey
Superior Court considered a request by a criminal defendant accused of
murder to gain access to the TrueAllele source code used by the state’s expert
in rendering testimony concerning the likelihood that the defendant’s DNA
had been present at the scene of the murder. The defendant expressed concern
that the computer program and underlying methodology the state’s expert
used was untested, potentially unreliable, and that “peer reviewed” articles
offered to support the program were authored or funded by the expert or his
organization.175 At a Frye hearing (similar to a Daubert hearing), the trial
170

See Jennifer Skeem, Nicholas Scurich & John Monahan, Impact of Risk
Assessment on Judges' Fairness in Sentencing Relatively Poor Defendants,
44 L. & HUM. BEHAV. 51, 53 (2020).
171
Id. at 53.
172
Id. at 56; see also Real Median Household Income by Race and
Hispanic Origin: 1967 to 2017, U.S. CENSUS BUREAU,
https://www.census.gov/content/dam/Census/library/visualizations/2018/
demo/p60-263/figure1.pdf (last visited Jul. 29, 2020) (noting that the
median household income for all races combined is just over $60,000 per
year, but for Blacks is just over $40,000 per year).
173
O'Neil, supra note 163.
174
246 A.3d 279 (N.J. Sup. Ct. App. Div. 2021).
175
Id. at 305.

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

32

court denied the defendant access to the software’s source code and related
documentation.
As relevant to our negotiated approach, outlined infra in Part III.D,
the parties in Pickett reached an impasse when trying to negotiate the terms
of a protective order for source code.176 The defendant agreed to a prohibition
on disclosure to any individual with “any direct or indirect commercial or
employment interest in competing software products” and to certain other
safeguards.177 However, the prosecution insisted on additional and more
expansive protections including a requirement that the software be reviewed
only at the prosecutor’s office in a supervised inspection, permitting only
handwritten notes on the 170,000 lines of code (a process which was
estimated to take eight years to enable understanding the code).178 Then there
were the financial terms: a $1,000,000 automatic civil liability “in the event
that the proprietary materials are improperly handled, negligently or
otherwise,” and the $3,000,000 defense liability coverage.179
On appeal, the defendant contended that the source code and
documentation were necessary to his defense and should be discoverable
notwithstanding a claim that they contained trade secrets.180 The parties and
amici including the ACLU, the Innocence Project, the Legal Aid Society,
various bar associations and other interested groups, submitted extensive
briefing.181The appellate court held, consistent with some rulings from other
jurisdictions, that if the state chooses to use an expert who relies on particular
novel software to develop or support its conclusions to be offered at trial, then,
upon a showing of particularized need, the defendant is entitled, under a
suitable protective order, to access the software’s source code.182 In addition,
supporting software development and related documentation (including
documentation pertaining to testing, design, bug reporting, change logs, and
program requirements) was also needed to challenge the reliability of the
software.183 According to the court, “[a] criminal trial where the defendant
does not have ‘access to the raw materials integral to the building of an
effective defense’ is fundamentally unfair.”184
While recognizing that the owner of a trade secret who establishes
that the requested information is in fact a trade secret may refuse to disclose
it, the court found that this privilege is not absolute.185 The burden fell on the
defendant seeking access to show “(1) a rational basis for ordering production
of the information sought, (2) the specificity of the information sought, (3)
Id. at 289.
Id. at 288.
178
Id.
179
Id. at 319-20.
180
Id. at 291-92.
181
Id. at 292-99.
182
Id. at 284.
183
Id.
184
Id. at 299.
185
Id. at 300.
176
177

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

33

the available means of safeguarding the trade secret owner’s intellectual
property, such as the issuance of a protective order; and (4) any other relevant
factors unique to the facts of the case.”186 The court found that defendant had
satisfied that burden.187 The case was remanded, finding that “anything less
than full access contravenes fundamental principles of fairness, which
indubitably compromises a defendant’s right to present a complete
defense.”188 The trial court was directed to compel disclosure of the source
code and related materials “pursuant to an appropriate protective order.”189

3. Special Constitutional Concerns for Criminal Justice?
Scholars have raised concerns about possible due process violations as
algorithms have moved into virtually every sector of government decisionmaking.190 The Fifth and Fourteenth Amendments provide that no person can
be deprived “of life, liberty, or property without due process of law” by the
federal government or state governments, respectively.191 As an extension of
the due process clause, the Sixth Amendment entitles a criminal defendant to
the right to confront witnesses against him,192 which is applicable to states as
a result of ratification of the Fourteenth Amendment193 and the Supreme
Court’s adoption of a selective incorporation approach where the Bill of
Id. at 284.
Id.
188
Id. at 324.
189
Id.
190
See, e.g., Brauneis & Goodman, supra note 30, at 103-04; Danielle
Keats Citron, Technological Due Process, 85 WASH. U. L. REV. 1249,
1294 (2008); Ryan Calo & Danielle Citron, The Automated Administrative
State: A Crisis of Legitimacy, 70 EMORY L. J. 797, 816-17 (2021); John
Villasenor & Virginia Foggo, Artificial Intelligence, Due Process, and
Criminal Sentencing, 2020 MICH. ST. L. REV. 295 (2020); Wexler, supra
note 30; SLOBOGIN, supra note 30; Graves & Katyal, supra note 30.
191
U.S. CONST. amends. V, XIV.
192
U.S. CONST. amend. VI (“In all criminal prosecutions, the accused shall
enjoy the right […] to be […] confronted with the witnesses against him”)
(emphasis added).
193
U.S. CONST. amend. XIV; see Duncan v. Louisiana, 391 U.S. 145, 14849 (1968) (citing Powell v. Alabama, 287 U.S. 45, 67 (1932), In re Oliver,
333 U.S. 257, 273 (1948), and Gideon v. Wainwright, 372 U.S. 335, 34344 (1963) to note that the question of whether a right contained within the
Bill of Rights shall be incorporated to be applicable to States depends upon
whether the right is among those “fundamental principles of liberty and
justice which lie at the base of all our civil and political institutions;”
whether the right is “basic in our system of jurisprudence;” and whether
the right is "a fundamental right, essential to a fair trial”).
186
187

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

34

Rights is concerned.194 In general, unless a declarant is unavailable and the
defendant had a prior opportunity to cross-examine the declarant, the
Supreme Court has held that testimonial statements of witnesses absent from
trial must be excluded from evidence because of the defendant’s rights under
the confrontation clause.195
Nevertheless, whether the confrontation right applies to algorithms and
outside of a trial196 (e.g., pre-trial or post-trial) is a very complex issue. Courts
have held that machines do not count as hearsay declarants, and thus the
confrontation right does not attach. As one commentor notes, “[u]nder
the machine-generated testimony doctrine, courts across the nation have held
that machine-generated data does not trigger the Confrontation Clause
because it is the machines--not the analysts operating them--that make the
statements at issue, and machines are not “witnesses” within the meaning of
the Confrontation Clause.”197
As studies have suggested, algorithmic use in the criminal justice
system has disproportionately impacted racial minorities and those of lowersocioeconomic status,198 which some argue should be considered a due
process violation since they rely on generalized information, not tailored to
the individual’s life and characteristics. For instance, Sonja B. Starr has
argued that statistical sentencing based on specific characteristics is
unconstitutional because use of group tendencies as a proxy for individual
characteristics should not be constitutionally permissible. 199 That does not,
however, appear to be a widely adopted view, as generalized information is
routinely used for forensics and sentencing.200
While similar due process arguments have been made with respect to the
disclosure issue, many defendants have been unsuccessful in challenging

194

Pointer v. Texas, 380 U.S. 400, 403 (1965) (holding that “the Sixth
Amendment’s right of an accused to confront the witnesses against him is
likewise a fundamental right and is made obligatory on the States by the
Fourteenth Amendment”).
195
Crawford v. Washington, 541 U.S. 36, 59 (2004). But see FED. R. EVID.
804(b) (detailing exceptions to the Rule against Hearsay).
196
See 2 Wharton's Criminal Evidence § 6:10 (15th ed.).
197
Brian Sites, Rise of the Machines: Machine-Generated Data and the
Confrontation Clause, 16 COLUM. SCI. & TECH. L. REV. 36, 51 (2014).
198
See Angwin et al., supra note 100 (finding that even even when criminal
history, recidivism, gender, and age were isolated, from the COMPAS
study, Black defendants were still 77% more likely to be flagged for higher
risk of future violent crime); Stevenson, supra note 168 (discussing the
racial inequalities of pre-trial release for Black defendants compared to
White defendants); Simonite, supra note 169 (noting judicial discretion
results in harsher rates of incarceration for defendants of lower socioeconomic status).
199
Starr, supra note 61, at 827.
200
See, e.g., Rebecca Foxwell, Risk Assessments and Gender for Smarter
Sentencing, 3 VA. J. CRIM. L. 435, 454 (2015).

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

35

forensic algorithmic use without disclosure as a due process violation.201 The
Supreme Court has cautioned that “[a]ssurances of secrecy are conducive to
the transmission of confidences which may bear no closer relation to fact than
the average rumor or item of gossip, and may imply a pledge not to attempt
independent verification of the information received. The risk that some of
the information accepted in confidence may be erroneous, or may be
misinterpreted, by the investigator or by the sentencing judge, is manifest.”202
Nonetheless, courts seem to be unwilling to resolve this issue in favor of
disclosure.203 Further, courts have not fully discussed the distinction between
reviewing individual pieces of information fed into an algorithm as opposed
to actual review of how the score itself was calculated, which may be relevant
when trying to demonstrate the level of judicial reliance on a given risk
See e.g., State v. Loomis, 881 N.W.2d 749, 764-65 (Wis. 2016) (denying
the defendant’s due process challenge to the court’s failure to disclose the
COMPAS algorithm because the risk assessment score was not the
determinative factor regarding whether the defendant received an
individualized sentence, and COMPAS has the potential to provide courts
with more information) (emphasis added); People v. Wakefield, 107
N.Y.S.3d 487, 494, 497-98 (N.Y. App. Div. 2019) (denying defendant’s
due process challenge to a confrontation clause violation as a result of a
DNA matching algorithm source code being withheld because, under the
specific facts of the case, the source code was not a declarant since there
was human input when utilizing the algorithm and creator of the source
code testified in court. Despite this holding, the court noted that
algorithmic source code reliance can raise legitimate questions concerning
due process and artificial intelligence.); ACLU Brief Challenges the
Constitutionality of Virginia’s Sex Offender Risk Assessment Guidelines,
AMERICAN CIVIL LIBERTIES UNION OF VIRGINIA (Oct. 28, 2003)
https://acluva.org/en/press-releases/aclu-brief-challengesconstitutionality-virginias-sex-offender-risk-assessment;
Brooks
v.
Commonwealth, No. 2540-02-3, 2004 Va. App. LEXIS 29 (Va. Ct. App.
Jan. 28, 2004) (dismissing the Virginia ACLU’s challenge to the risk
assessment tool because the algorithm was only advisory in nature).
202
See Gardner v. Florida, 430 U.S. 360 (1977); see also Villasenor &
Foggo, supra note 190, at 324.
203
See Flores v. Stanford, No. 18 CV 2468 (VB), 2019 U.S. Dist. LEXIS
160992, at 29-30 (S.D.N.Y. Sep. 20, 2019) (denying defendant
Chairwoman of the New York State Board of Parole’s motion to dismiss
plaintiff’s class action claim that parole procedures for juveniles
potentially serving life sentences violate due process because if the
plaintiff’s allegations that the Parole Board do not review individual files
before making parole determinations, which are based at least in part on
risk assessment algorithms, then there is plausible evidence that the Parole
Board does not provide juveniles with procedurally adequate opportunities
for release, such that the defendant’s motion cannot be granted as a matter
of law. This issued has not yet been ruled upon. On April 10, 2020, the
deadline for discovery completion was extended to November 20, 2020,
and a case management conference is scheduled for November 23, 2020).
201

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

36

assessment score.204
4. FOIA Disclosures Unlikely
For software developers creating algorithms for various functions
within the criminal justice system, meeting the requirements of the Freedom
of Information Act’s (“FOIA”) Exemption 4 is of critical importance because
the trade secret exemption often serves as a primary mechanism for
preventing public disclosure of algorithmic models in response to FOIA
requests.205 Because courts are often unwilling or unable to fully disclose
information pertaining to algorithms, criminal defendants may try to obtain
information through FOIA. However, our review and analysis of the cases
revealed that these attempts have been largely unsuccessful.206
In fact, court decisions have upheld plea agreements where
defendants have waived their rights to request information pertaining to their
case under open government laws,207 which seems contrary to FOIA’s
purposes, particularly if defendants are unaware that risk-assessment
algorithms were used in making a guilt determination (e.g., based on
See Danielle Kehl, Priscilla Guo, & Samuel Kessler, Algorithms in the
Criminal Justice System: Assessing the Use of Risk Assessments in
Sentencing, RESPONSIVE COMMUNITIES (Jul. 2017), p.22-23,
https://dash.harvard.edu/bitstream/handle/1/33746041/201707_responsivecommunities_2.pdf?sequence=1&isAllowed=y.
205
5 U.S.C. § 552(b)(4) (2020) (Exemption 4 provides protection for
information submitted to the government that is classified as a trade secret,
confidential commercial information, or confidential financial
information). For more background on FOIA and policy implications, see
Mark Fenster, The Transparency Fix: Advocating Legal Rights and Their
Alternatives in the Pursuit of a Visible State, 73 U. PITT. L. REV. 443
(2002).
206
The authors reviewed a sample size of nineteen cases from 2017 to
2019, and prior to the adoption of the Argus Leader Media (Food Mktg.
Inst. v. Argus Leader Media, 139 S. Ct. 2356 (2019)) standard. It revealed
that 68% of FOIA requests were denied based on judicial rulings in favor
of nondisclosure under Exemption 4. The sample was obtained by
performing a Lexis Advance search for “FOIA” and “trade secrets” from
January 1, 2017, through June 23, 2019, forty-three results were returned,
with twenty-four being excluded from the analysis due to trade secret
disclosure not being a primary issue or being earlier decisions that were
further discussed on appeal (on file with authors).
207
See, e.g., Caston v. Exec. Office for the United States Attys., 572 F.
Supp. 2d 125, 129 (D.D.C. 2008) (denying a FOIA request because the
plaintiff voluntarily and intelligently waived his right to request
information from any United States department or agency in a plea
agreement); United States v. Lucas, 141 Fed. Appx. 169 (4th Cir. 2005),
certiorari denied 546 U.S. 1196, 126 S.Ct. 1391, 164 L. Ed. 2d 94 (2006)
(affirming denial of a FOIA request because the petitioner waived his right
to such requests in plea agreement).
204

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

37

predictive policing practices) or will be used during sentencing proceedings.
This lack of transparency can be detrimental during the sentencing process
because most risk assessment tools were originally designed for criminal
rehabilitation purposes, and if a defendant is deemed ineligible for alternative
treatment, incarceration results.208 The same risk assessment score is often
used to determine length of incarceration—with potential to significantly
differ based on changes in a single risk factor—which defendants can rarely
challenge.209 Apprehension about algorithmic transparency has increased
further after the Food Marketing decision, in which the Supreme Court held
that information is exempt from FOIA so long as the information is treated as
confidential and its owner has received assurance that the information will
remain confidential.210
Another practical limitation of obtaining records through FOIA is that
the government cannot give that which it does not possess. Therefore, to the
extent private vendors retain ownership, control, and possession of their
algorithmic models, records, and source codes they remain beyond the reach
of public records requests.211 There are also FOIA exemptions that protect law
enforcement and court records.212 Thus, even outside the FOIA context,
criminal defendants have been unsuccessful in obtaining algorithmic models
and other proprietary information during discovery because it was in
possession of the developer (not the government) and claimed as a trade
secret.213
II. THE TENSION BETWEEN TRADE SECRECY AND PUBLIC
TRANSPARENCY
It is axiomatic that trade secrecy is built for secrecy, not openness. It is
built for competitors in the private-sector environment to protect their private
property, not the public interest in having access to such information. In the
context of calls for greater public transparency, this tension between trade
secrecy and government transparency has been lamented by scholars who
note that trade secrecy has changed from protecting against a competitor’s
misappropriation to a shield protecting the proprietor from public
investigation.214 Nonetheless, the situation is more nuanced. We view the
trade secrecy framework as presenting both the problem and the solution
when it comes to disclosure. While trade secret law requires some degree of
secrecy it also permits alienability and sharing of secrets under contractual
terms structured (within limits) by the parties and designed to serve the unique
Angwin et al., supra note 100.
Id.
210
Food Mktg. Inst. v. Argus Leader Media, 139 S. Ct. 2356 (2019).
211
See Brauneis & Goodman, supra note 30, at 135.
212
5 U.S.C. § 552(b)(7); see generally Brauneis & Goodman, supra note
30, at 152.
213
See, e.g., State v. Kuhl, 741 N.W. 2d 701, 708 (Neb. Ct. App. 2007).
214
See, e.g., Katyal, supra note 30, at 1242.
208
209

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

38

needs of the parties.
For instance, trade secret litigation routinely occurs in courts while
observing the public right of access to court filings and proceedings and the
protection of the litigants’ trade secrets. The right of access is firmly
entrenched in the law throughout the United States.215 At the same time, there
are protections available under the federal Defend Trade Secrets Act
(DTSA)216 and the Uniform Trade Secrets Act (UTSA)217 to safeguard trade
secrets from disclosure during litigation, which serve as qualifications on the
public’s right of access to court proceedings. Thus, even in trade secret
misappropriation cases, there is tension between the qualified right of public
access and the litigants’ need to protect the confidentiality and value of their
trade secrets.
The qualified right to public access can be overcome “only by an
overriding interest based on findings that closure is essential to preserve
higher values and is narrowly tailored to serve that interest.”218 The protection
of trade secrets has long been recognized as one of these overriding interests
that justify an exception to this right. Indeed, the Supreme Court has
recognized that “sources of business information that might harm a litigant’s
competitive standing” are exempted from public disclosure.219
Trade secret rights potentially apply to the technologies discussed in this
Article and protect data, software, and algorithms in these technologies. In
general, trade secret rights cover operability and functionality of devices, and
algorithmic models are often within one of these categories.220 In fact, these
rights are sufficiently strong that putative trade secret owners may refuse to
reveal the protected information, even to the government.221 Developers are
also cautious because with trade secrecy, others may lawfully attempt to
reverse engineer the software, unless prohibited by contract. Further, trade
secret rights are destroyed if improperly disclosed, and trade secret owners
are required to take reasonable efforts to protect information that they deem a
trade secret; courts typically expect such efforts, at a minimum, will include

Press-Enterprise Co. v. Superior Court, 478 U.S. 1, 8 (1986).
18 U.S.C. § 1836, et seq.
217
Unif. Trade Secrets Act § 5 (amended 1985), 14 U.L.A. 437 (1990).
218
See Globe Newspaper Co. v. Superior Court, 457 U.S. 596, 606–07
(1982) (“here, as in the present case, the state attempts to deny the right of
access in order to inhibit the disclosure of sensitive information, it must be
shown that the denial is necessitated by a compelling governmental
interest, and is narrowly tailored to serve that interest.”).
219
Nixon v. Warner Communications, 435 U.S. 589, 598 (1978).
220
See LIFESCIENCE ALLEY LONG COMMENT REGARDING A PROPOSED
EXEMPTION UNDER 17 U.S.C. 1201 (MARCH 27, 2015), available at
https://www.copyright.gov/1201/2015/comments032715/class%2027/LifeScience_Alley_Class27_1201_2014.pdf.
221
See Elizabeth A. Rowe, When Should Trade Secret Law Shield
Disclosures to the Government?, 96 IOWA L. REV. 791 (2011).
215
216

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

39

nondisclosure agreements.222
A combination of trade secrecy and contract law through licensing
agreements can be a powerful combination for controlling proprietary
information.223 This is why developers and private vendors rely not only on
their property rights but often insist on contracts that contain confidentiality
and non-disclosure limitations. Accordingly, while it is not often
acknowledged, there are weighty rights and constitutional concerns on both
sides that make this trade secret “problem” a thorny conundrum, and why any
thoughtful solutions must recognize and wrestle with the legitimate
arguments and interests on both sides.
A. Built for Competitive Environment
As originally conceptualized (and in much of the world outside of the
United States), the protection of business secrets is grounded in the
maintenance of business ethics and the prevention of unfair competition.224
Along with the present day recognition of trade secrets as a form of intangible
property, the unfair competition aspects of trade secret law remain an
important part of its genetics, as evidenced by the elements of a trade secret
misappropriation claim.225 Further, most trade secret cases involve
misappropriation by former employees or business competitors.226 Strikingly,
the circumstances presented in these government-vendor cases, however, are
not that. Instead, they represent an attempt, between non-competitors, to keep
the nature of their business dealings secret from the public (even when the
public is paying for the service or product).227
B. For Developers, Algorithms and Data are
Property
For the purposes of this Article, it is important to bear in mind that it is
not only criminal defendants who have constitutional rights to be considered
in this debate, but so do trade secret owners. A case that is frequently cited
for the proposition that trade secrets are a form of private property is
Ruckelshaus v. Monsanto Co.228 In that case, the Supreme Court considered
whether certain provisions of the Federal Insecticide, Fungicide, and
Rodenticide Act were unconstitutional.229 Monsanto argued that the
See Elizabeth A. Rowe & Sharon K. Sandeen, Trade Secret Law: Cases
and Materials 199-201 (West Acad., 3rd ed. 2020).
223
See Elizabeth A. Rowe, Sharing Data, 104 IOWA L. REV. 287, 303
(2018).
224
See ROWE & SANDEEN, supra note 222, at 21.
225
Id. at 281-82.
226
Id. at 300.
227
See Katyal, supra note 30, at 1247.
228
467 U.S. 986 (1984).
229
Id. at 990.
222

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

40

provisions of the law that required it to disclose certain information and data
were unconstitutional because they amounted to a property taking without just
compensation in violation of the Fifth Amendment to the U.S. Constitution.230
To succeed on its claim, Monsanto had to first establish it had a property
interest in the information. In finding a property interest in Monsanto’s data,
the Court in Ruckelshaus explained: “Because of the intangible nature of a
trade secret, the extent of the property right therein is defined by the extent to
which the owner of the secret protects his interest from disclosure to
others…Information that is public knowledge or that is generally known in
an industry cannot be a trade secret… If an individual discloses his trade
secret to others who are under no obligation to protect the confidentiality of
the information or otherwise publicly discloses the secret, his property right
is extinguished.”231
Although it is clear from the language of Ruckelshaus that the Supreme
Court limited its holding to information that qualifies for trade secret
protection, some scholars, like Pamela Samuelson232 expressed concern that
the holding of Ruckelshaus might be used to claim property rights in lesser
forms of information. This view remains a critical issue today, especially in
the context relevant to this Article. It has become a particularly pressing issue
in FOIA litigation where, due to the Supreme Court’s decision in Food Mktg.
Inst. v. Argus Leader Media,233 businesses may believe that they have
property-like rights in any information they deem confidential.
Thus, whether information is characterized as property can have real
world consequences and when deciding whether information will be treated
as property, context matters. Information that meets the definition of a trade
secret is property to the extent it can be precisely defined and is maintained
within the exclusive control of the putative trade secret owner.234
Significantly, an emphasis on trade secrets as a property right leads to lesser
importance and weight on the public interest in governmental transparency.
This property rationale provides the underlying basis for the claim of
“ownership” over the technologies, their data, algorithms, and practically
anything else that can be captured by intellectual property and trade secrecy
even when they are serving government functions. As a result of the asserted
right to exclude (or restrict access and disclosure), ShotSpotter does not want
gunshot data disclosed,235 and the developer of Stingrays does not want police
Id. at 998-99.
Id. at 1007.
232
See Pamela Samuelson, Information as Property: Do Ruckelshaus and
Carpenter Signal a Changing Direction in Intellectual Property Law?, 38
CATH. U. L. REV. 365 (1989).
233
139 S. Ct. 2356 (2019).
234
See generally Ramon A. Klitzke, Trade Secrets: Important QuasiProperty Rights, JSTOR, www.jstor.org/stable/40686714 (last visited
Sep. 10, 2021).
235
See Hannah Bloch-Wehba, Access to Algorithms, 88 FORDHAM L. REV.
1265, 1283-84 (2020).
230
231

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

41

departments to report their use, or even courts to know about and review
them.236 Similarly, CMI, Inc. the developer of Intoxilyzer, a breathalyzer
device, refuses to disclose its source code.237 Unsurprisingly, each of these
developers argues that its source codes are protected by property rights under
a trade secret theory.
C. No Robust Role for Public Interest in
Governmental Transparency
The conception of trade secrets as property is fundamental to its design
and underlying legal framework. This makes it almost antithetical to
consideration of the public interest in governmental transparency.238 As
compared to the clarity of intellectual property rights, the “public interest”
generally is murky and unsettled.239 Indeed, for the purposes of this Article, it
should be noted plainly that there is no mechanism for robust consideration
of “the public interest” in the U.S. trade secret framework, except in some
rather limited circumstances, that themselves are under-developed.240 Other
than whistleblower protections,241 and some First Amendment242 exceptions,
public interest considerations most frequently arise (albeit in a relatively
cursory fashion) in the consideration of equitable principles243 for injunctive
relief in trade secret misappropriation cases. Thus, for the purposes of this
Article we operationalize it as the public interest in governmental
transparency. This public-secret tension is at the heart of any attempt to
understand and better balance private interests in intellectual property with

See Wexler, supra note 30, at 1366-67.
See Andrea Roth, Trial by Machine, 104 GEO. L.J. 1245, 1272 (2016).
238
In a forthcoming paper, one of the authors (Prof. Rowe) will explore the
broader questions about the public interest and trade secrecy. As pertains
to this context, a more comprehensive discussion is beyond the scope of
this Article.
239
See, e.g., Jonathan Zittrain, What the Publisher Can Teach the Patient:
Intellectual Property and Privacy in an Era of Trusted Privication, 52
STAN. L. REV. 1201, 1232–33 (2000).
240
See, e.g., Sharon K. Sandeen & Ulla-Maija Mylly, Trade Secrets and the
Right to Information: A Comparative Analysis of E.U. and U.S. Approaches
to Freedom of Expression and Whistle-blowing, 21 N.C. J.L. & TECH. 1, 55
(2020); Peter Menell, Tailoring A Public Policy Exception To Trade Secret
Protection, 105 CALIF. L. REV. 1 (2017).
241
18 U.S.C. § 1833.
242
See, e.g., Elizabeth A. Rowe, Trade Secret Litigation and Free Speech: Is
it Time to Restrain the Plaintiffs?, 50 B.C. L. REV. 1425, 1433 (2009); Pam
Samuelson, Principles for Resolving Conflicts Between Trade Secrets and the
First Amendment, 58 HASTINGS L.J. 777, 808-11 (2007).
243
See Elizabeth A. Rowe, Ebay, Permanent Injunctions, and Trade Secrets,
WASH. & LEE L. REV. 553, 567 (2020).
236
237

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

42

the public’s right to information.244 The tools used to effectuate that balance
in litigation involve carefully constructed protective orders, but outside of the
litigation context the contractual approach -- confidentiality agreements – as
championed in this Article, provide an ex ante tool for sharing trade secret
information.
As some scholars have argued, the struggle for transparency from secrecy
may be further exacerbated by developers’ overclaiming their trade secret
rights.245 This is also not unusual in IP, or unique to trade secrecy.246 However,
it is fundamentally the legal structure that provides this perceived thumb on
the scale in favor of IP owners and to the potential detriment of the public
interest in governmental transparency. In the case of algorithms in the
criminal justice system, the contractual non-disclosure agreements coupled
with asserted trade secret rights, reflect an example of intellectual property
laws providing greater protection than contract law alone would provide.
How we define the public interest and what constitutes an exception to
the weighty private property rights of trade secret owners is left open to debate
and circumstances. Several states have recognized, for instance, that it is
against the public interest to enter into settlement agreements that shield
information about dangers to the public’s health and safety.247 Similar public
policy carve-outs have also been made in the employment law area to permit
whistleblowing by employees despite their having signed confidentiality
agreements.248 No such exception exists for trade secrets related to
technologies in the criminal justice system, or even generally, technologies
acquired from private vendors by government agencies for public decision
making or critical public functions.
To be sure, the best (though not perfect) option here would be for
Congress to rule the kinds of contractual provisions that restrict disclosure in
this context void as a matter of public policy.249 According to the Second
Restatement of Contracts, a contract is unenforceable for public policy
reasons if either “legislation provides that it is unenforceable or the interest in
its enforcement is clearly outweighed in the circumstances by a public policy
See Sandeen & Mylly, supra note 240, at 55.
See, e.g., TANYA APLIN & SHARON K. SANDEEN, “Barriers to Data
Access in the US and EU: Trade Secrecy, Actual Secrecy and the Hype
Surrounding AI” in [book chapter forthcoming].
246
See Elizabeth A. Rowe, Patents, Genetically Modified Foods and IP
Overreaching, 64 S.M.U.L. REV 859, 883-84 (2011).
247
See Elizabeth E. Spainhour, Unsealing Settlements: Recent Efforts to
Expose Settlement Agreements That Conceal Public Hazards, 82 N.C. L.
REV. 2155, 2158-61 (2004) (discussing state laws, like Florida’s, that
declare private settlements that conceal public hazards void as a matter of
public policy).
248
See Employment Law § 8.9, at 262 (Mark A. Rothstein et al. eds., 2d
ed. 1999).
249
Until that happens, courts (both state and federal) could also find that
these contracts violate the public interest, if there were a coherent doctrine
on which to rely.
244
245

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

43

against the enforcement of such terms.”250 Thus, in this instance, a legislature
could deem that the public interest in promoting governmental transparency
could outweigh the developers’ interests in controlling the information made
available about its product.
For example, the last time Congress considered and debated the balance
between patent protection for seed producers and the public interest in
research it demonstrated a clear preference for and recognition of the
importance of research.251 In 1970, Congress expanded the intellectual
property protection afforded to plants by enacting the Plant Variety Protection
Act (PVPA).252 Prior to the enactment of the PVPA, hybrid-seed companies
enjoyed trade-secret protection over the plant varieties they developed. The
PVPA, through the issuance of a plant variety certificate, confers exclusive
rights to breeders of certain sexually reproduced or tuber propagated plant
varieties.253 Notably, though, the Act contains a research exemption, explicitly
providing that “[t]he use and reproduction of a protected variety for plant
breeding or other bona fide research shall not constitute an infringement of
the protection provided under this chapter.”254 Thus, it is possible that a
similar type of legislative balance could be achieved in this context.
Along those lines, there is a recent, directly applicable example of what a
legislator or legislature could do: Idaho Representative Greg Chaney
proposed H.B. 118, which was formally passed on March 28, 2019, making
Idaho the first state to completely remove trade secret protections within the
criminal justice system for pretrial risk assessment tools, and also requiring
algorithmic transparency and open access to the public for “inspection,
auditing, and testing” of those tools.255 While a bold step, this legislation is
nevertheless narrow in that it is limited to pretrial risk assessment tools. It
therefore does not apply to many other algorithmic models such as evidencegenerating software like ShotSpotter, DNA analysis software, or face
recognition software. There has also been some movement toward legislation
at the federal level. Congressman Takano has proposed the Justice in Forensic
Algorithms Act. It aims to eliminate the trade secret evidentiary privilege in
criminal proceedings, as well as create standards for algorithms used for
evidence,256 but reading political tea leaves one might expect it is unlikely to
become law any time soon.257
Restatement (Second) of Contracts § 178 (1981).
See Rowe (Patents), supra note 246, at 865.
252
Plant Variety Protection Act, Pub. L. No. 91-577, 84 Stat. 1542 (1970)
(codified as amended in scattered sections of 7 and 28 U.S.C.).
253
7 U.S.C. § 2402(a) (2006).
254
7 U.S.C. § 2544 (2000).
255
IDAHO CODE § 19-1910 (2020).
256
See Press Release, supra note 42.
257
See,
e.g.,
Lexis
Legislative
Outlook,
available
at
https://plus.lexis.com/document/documentlink/?pdmfid=1530671&crid=
28f14df7-3387-45ba-9942250
251

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

44

III. THE FIT: PROCUREMENT POLICIES & CONTRACTING
With all these tensions, is there any possible fit or overlap between trade
secrecy and governmental transparency? This Article answers in the
affirmative: procurement and contracting. Just as the mismatch between trade
secrecy and government-private ventures present obstacles to public
disclosure, the trade secrecy framework also supports a potential solution.
Trade secret law is not designed to foster absolute secrecy. To the contrary,
trade secret law is built to solve Arrow’s information paradox by facilitating
the sharing of information in a manner that does not result in loss of the value
of the information to its owner.258 Importantly, the trade secret framework
provides that the person now in possession of the owner’s secret is still
(contractually) restrained in their ability to use and to disseminate the
information further.
Contract law is thus central to our proposal. Contracts can balance the
competing interests of secrecy and disclosure, and contractual negotiations
between government agencies and private vendors are the means for
achieving such balance on a transaction-by-transaction basis. Contracts are
routinely used with trade secrecy to assure confidentiality and non-disclosure.
At the same time, contractual provisions can also be used to set out the terms
and conditions of any permissible disclosure. This may be the problem and a
solution since trade secret owners receive promises of confidentiality in the
procurement contracts. Therefore, until there are legislative pronouncements
that express public policy goals and interests regarding the disclosure of
algorithms in the public sphere, private contracting (consistent with
government procurement principles) could be used to address the problem.
We contend that it is possible to envision contract law as a means to
simultaneously support greater sharing in this context, while also protecting
the rights of vendors.259
Government procurement is a necessity for societal functioning. At every
level of government, officials and agencies contract for goods and services
that cannot be provided in-house, whether due to employee shortages or
limitations on requisite skill sets. As aptly characterized by Danielle Conway,

02ae22a38a08&pddocfullpath=%2Fshared%2Fdocument%2Fstatuteslegislation%2Furn%3AcontentItem%3A62DF-5H01-JSXV-G38W0000000&pdcontentcomponentid=133053&pdproductcontenttypeid=undefined
&pdiskwicview=false&pdpinpoint=&ecomp=7gktk.
258
See Elizabeth A. Rowe & Sharon K. Sandeen, Trade Secrecy and
International Transactions: Law and Practice 66 (2015).
259
See Matwyshyn, supra note 47, at 5 (arguing that contract law can be
used as a means to protect consumer privacy).

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

45

however, procurement law is complex and dynamic.260 Governmental
authority to enter into contracts is derived from the Engagements Clause of
the Constitution,261 which the Supreme Court has held to be a valid exercise
of constitutional authority.262 Government procurement increases access to
marketplace services with industry specialists, and often reduces costs while
simultaneously improving quality through the creation of competition.263 For
many decades, federal and local government agencies have been major
purchasers of goods and services such as weapons, airplanes, office supplies,
and infrastructure construction. Indeed, the government is the largest
purchaser of goods and services. In 2020 the federal government alone spent
over $665 billion, and of that over $14 billion was spent on technology
products and services.264
Because public funds pay for government contracts, government
procurement is subject to additional regulations beyond what is required in
private contracts. For example, unlike private agreements where principals
and agents may enter into contracts, only designated officers can legally bind
the Government through procurement efforts. Federal law requires agency
leaders to establish and maintain procurement management programs to
select, appoint, and terminate contracting officers.265 Once selected or
appointed, contracting officers have federal authority to “enter into,
administer, and terminate contracts,” which bind the Government, but only to
the extent of the particular officer’s authority.266 Additionally, before entering
into a contract, the government must publicize contract actions, provide for
full and open competition, verify contractor qualifications, describe agency
needs, and maintain specific records.267
Danielle M. Conway, State and Local Government Procurement xiii
(2012).
261
U.S. CONST. ART. VI, cl. 1.
262
United States v. Tingey, 30 U.S. 115, 128 (1831) (“[W]e are of opinion
that the United States have such a capacity to enter into contracts.”).
263
See generally Kate M. Manuel et al., The Federal Acquisition
Regulation (FAR): Answers to Frequently Asked Questions,
CONGRESSIONAL
RESEARCH
SERVICE
(Feb.
3,
2015),
https://fas.org/sgp/crs/misc/R42826.pdf.
264
A Snapshot of Government-Wide Contracting for FY 2020,
https://www.gao.gov/blog/snapshot-government-wide-contracting-fy2020-infographic.
265
41 U.S.C. §§ 1702(b)(3)(G), 3102; 48 C.F.R. § 1.603-1.
266
48 C.F.R. § 1.602-1.
267
See 41 U.S.C. § 1710 (detailing the process and requirements for
transitioning from an agency performed function to contractor
performance); 41 U.S.C. § 1712 (describing record requirements where
contracting and procurement are concerned); 41 U.S.C. § 1303 (explaining
the Federal Acquisition Regulation); 48 C.F.R. §§ 1–53, (Further detailing
the Federal Acquisition Regulation, which sets forth the federal
procurement process, best practices, procedures, and requirements for
agencies, and provides standard clauses and forms.).
260

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

46

Government agencies’ use of private vendors to perform government
functions, for better or worse, has become commonplace.268 Public
contracting for agency management services related to policy, rulemaking,
and decision-making processes are exempted from standard administrative
procedures that would otherwise govern decisions of policy; general rule
making procedures require publication in Federal Register, notice and
comment period, statement of authority but contracting does not require these
things.269 Therefore, existing agency practices of adopting AI frequently
remain concealed and are not subject to public review.270 This is due to the
accompanying practices that are integral to the private commercial
marketplace, including contracting and trade secrecy protective measures, but
which have raised a host of concerns to scholars.271 For instance, Danielle
Cintron notes that “the public [and] government actors are unable to influence
policy when it is shrouded in closed code."272
A. Contracting for Algorithms & AI
Similar to private entities, government agencies are also consumers of
technology, albeit on a larger scale. Indeed, at nearly every level of
government, agencies purchase or contract for AI services that rely on
patterns and remove official or agent discretion from decision-making
processes.273 Proponents of AI procurement argue that government agencies
often lack staff with the requisite capabilities to participate in AI research and
development.274 While this is likely a realistic assessment of lacking
government capacity to develop complex algorithms, public policy warrants
recognition of the fact that developers often keep relevant code secret, and
See, e.g., Deepa Varadarajan, Business Secrecy Expansion and FOIA,
68 UCLA L. REV. 462, 466 (2021); JODY FREEMAN & MARTHA MINOW,
GOVERNMENT BY CONTRACT: OUTSOURCING AND AMERICAN DEMORACY
( 2009); Gillian E. Metzger, Privatization as Delegation, 103 COLUM. L.
REV. 1367, 1369 (2003).
269
See 5 U.S.C. § 553(a)(2) for exclusion; 5 U.S.C. § 553(b) for default
agency rules.
270
See Deirdre K. Mulligan & Kenneth A. Bamberger, Procurement as
Policy: Administrative Process for Machine Learning, 34 BERKELEY
TECH. L.J. 773, 780 (2019).
271
See, e.g., Varadarajan, supra note 268, at 466; David S. Levine, The
Impact of Trade Secrecy on Public Transparency, in The Law and Theory
of Trade Secrecy, at 407, available at https://ssrn.com/abstract=1373536;
FRANK PASQUALE, THE BLACK BOX SOCIETY: THE SECRET ALGORITHMS
THAT CONTROL MONEY AND INFORMATION (2016); Danielle Keats Citron,
Open Code Governance, U. CHI. LEGAL F. 355, 356-57 (2008).
272
Citron (Tech. Due Process), supra note 190, at 1290–91.
273
See Mulligan & Bamberger, supra note 270, at 788.
274
See Jenna Burrell, How the machine ‘thinks’: Understanding opacity in
machine learning algorithms, BIG DATA & SOC’Y 3-4 (2016), available at
https://journals.sagepub.com/doi/pdf/10.1177/2053951715622512.
268

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

47

agency staff and officials are likely unable to assess technological design for
the same reasons that they cannot develop AI.275 Additionally, officials in
favor of procurement often view AI technologies as a new way to fulfill
agency missions through what is viewed as administrative technologies.276
Despite what appears to be an increased preference for predictive
algorithms, federal regulation of AI has largely been focused on self-driving
autonomous vehicles, aviation, and war-related functions,277 which suggests
that AI contracts are subject to arguably outdated—or nonexistent—laws.
Even on the state and local level, AI regulation has been limited and slow to
keep up with government procurement needs. For instance, Robert Brauneis
and Ellen Goodman’s study found that many government agencies did not
have significant records about the creation and implementation of algorithms
and AI already in use.278 They attributed the lack of records to agency failure
to generate documents, and contractor failure to provide documentation to
governmental clients.279
Though AI contracting has been expeditious and regulation unhurried,
some cities have taken an affirmative approach. For example, New York City
was the first city to establish a task force to examine AI systems prior to
adoption;280 Oakland established a privacy commission to recommend best
practices for AI adoption;281 Seattle has allowed for public comment on AI
surveillance technologies;282 and Portland adopted Smart City PDX to protect
citizen privacy when AI is used.283 Nevertheless, these examples serve as
exceptions, not the current norm.
Perhaps one reason government agencies do not focus on
transparency concerns is because of the proprietary interests of AI developers
and the benefits that such programs provide. When AI produces the results
See id.
See Mulligan & Bamberger, supra note 270, at 789.
277
See Regulation of Artificial Intelligence in Selected Jurisdictions, LAW
LIBRARY
OF
CONGRESS
27-30
(Jan.
2019),
https://www.loc.gov/law/help/artificial-intelligence/regulation-artificialintelligence.pdf.
278
Brauneis & Goodman, supra note 30, at 152.
279
Id.
280
See Mayor de Blasio Announces First-In-Nation Task Force To Examine
Automated Decision Systems Used By The City, NYC.GOV (MAY 16, 2018),
HTTPS://WWW1.NYC.GOV/OFFICE-OF-THE-MAYOR/NEWS/251-18/MAYOR-DEBLASIO-FIRST-IN-NATION-TASK-FORCE-EXAMINE-AUTOMATED-DECISIONSYSTEMS-USED-BY.
281
See Privacy Advisory Commission, CITY OF OAKLAND,
http://www2.oaklandnet.com/government/o/CityAdministration/d/Privac
yAdvisoryCommission/index.htm (last visited Oct. 11, 2020).
282
See Surveillance Technologies Under Review, SEATTLE INFO. TECH.,
https://www.seattle.gov/tech/initiatives/privacy/surveillance-technologies
(last visited Oct. 11, 2020).
283
See Using Data and Technology to Improve People’s Lives, SMART
CITY PDX, https://www.smartcitypdx.com/ (last visited Oct. 11, 2020).
275
276

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

48

that governmental officials want, such as fingerprinting, image identification,
or DNA matching, few questions are likely to be raised about the validity of
the methodology.284 Indeed, AI developers who provide algorithms for the
criminal justice system have a limited market: police investigators and
government attorneys. This may put pressure on developers to produce results
that law enforcement agencies favor and prevent disclosure of their AI
software, which might then result in independent third-party review.285 (As
we discuss later, the limited market also increases the governments’
bargaining power to negotiate its terms). Further, after an agency has adopted
an AI tool deemed to be valid, and better yet, that produces the desired results,
there is little incentive for the agency to question the tool, when doing so
could provide evidence of algorithmic flaws.286
B. Existing Mechanisms for AI Review Are Not
Sufficient
As the above processes reveal, government procurement of AI
appears lacking in independent testing and verification prior to
implementation. With facial recognition technology, for instance, some states
and local governments have implemented their own laws,287 leaving
government procurement of AI highly discretionary and lacking in
uniformity. Even at the federal level, government agencies are not subject to
standard administrative rule-making procedures when contracting is
involved.288
Congressional and state regulation of government AI procurement does
not have to be an all or nothing approach. As suggested by the Leadership
Conference on Civil and Human Rights, AI can be beneficial for government
See Murphy, supra note 50, at 745-47.
Id.
286
See id. at 746 (Noting that “So long as [police and prosecutors] remain
satisfied, the [forensic method] laboratories need not engage in any new
development or self-criticism.”).
287
See, e.g., Oakland’s City Council’s Resolution, available at
https://www.eff.org/files/2018/05/17/oaklandccops.pdf;
See
Shirin
Ghaffary, San Francisco’s Facial Recognition Technology Ban,
Explained,
VOX
(May
14,
2019,
19:06),
https://www.vox.com/recode/2019/5/14/18623897/san-francisco-facialrecognition-ban-explained; See Nik DeCosta-Klipa, Boston City Council
Unanimously Passes Ban on Facial Recognition Technology,
BOSTON.COM (Jun. 24, 2020), https://www.boston.com/news/localnews/2020/06/24/boston-face-recognition-technology-ban; see also
Taylor Hatmaker, Portland Passes Expansive City Ban on Facial
Recognition Tech, TECH CRUNCH (Sep. 9, 2020. 20:45),
https://techcrunch.com/2020/09/09/facial-recognition-ban-portlandoregon/.
288
5 U.S.C. §§ 553(a)(2), 553(b).
284
285

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

49

agencies, including the criminal justice system, if certain standard
requirements are in place.289 For instance, prior to AI implementation, the
Leadership Conferences suggest independent validation, opportunities for
defendant challenges, and public accessibility to AI design, structure, and
accuracy tests to ensure accountability.290 Because trade secrecy has been a
limiting factor for public disclosure,291 additional proposals could be
considered as an attempt to balance AI developers’ interests with public
disclosure. In addition to the procurement approach outlined in this Article,292
some scholars have suggested exclusive government contracts, for which
transparency could be a condition precedent; tax incentives; or an amendment
to the exception for non-disclosure where government procurement is
involved.293
C. Proposal Consistent with Procurement Policies
Our transaction-by-transaction procurement approach is consistent not
only with trade secrecy law, but with existing federal procurement law and
policy. While procurement tends to conjure government transactions based on
awards to the lowest bidder, it is important to understand that procurement
law and policies are fundamentally about more than just price.294 Accordingly,
these policies allow for the kind of flexibility that would permit government
agencies to negotiate and contract for the kinds of terms (discussed below) to
See Chaney, supra note 150.
See id.
291
Algorithms and AI are generally not considered patentable material, so
software developers often opt for trade secret protection. See Alice Corp.
Pty. Ltd. v. CLS Bank Int'l, 573 U.S. 208 (2014) (holding that algorithms
are abstract ideas ineligible for patent protection because using the claims
on a computer is not enough to transform the algorithm into patentable
subject matter); Diamond v. Diehr, 450 U.S. 175 (1981) (holding that
mathematical formulas in the abstract are not proper subjects eligible for
patent protection). Public disclosure of trade secrets results in loss of trade
secrecy, which is why developers are often hesitant to disclose source
code. See 18 U.S.C. § 1839(3) (2020), but note that the DTSA does not
preempt State law. Cf. Rivendell Forest Prods. v. Ga.-Pacific Corp., 28
F.3d 1042, 1046 (10th Cir. 1994) (noting that a trade secret can include a
system where the elements are in the public domain, but there has been
accomplished an effective, successful, and valuable integration of the
public domain elements and the trade secret gave the claimant a
competitive advantage which is protected from misappropriation).
292
See infra Part III.C.
293
See 5 U.S.C. § 553(a)(2) (Government contracting falls under the
Administrative Procedure Act's exemption of matters relating to "agency
management" or to "public property, loans, grants, benefits, or contracts.").
See generally Wexler, supra note 30, at 1422.
294
SCHWARTZ, supra note 52.
289
290

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

50

facilitate greater transparency. Indeed, the World Economic Forum’s
guidelines for AI procurement encourage consideration of trade secrecy
protections for vendors with possibilities for facilitating transparency,295 and
our proposal could supplement those guidelines with additional specificity on
how to achieve transparency within the confines of U.S. law. In sum, there
are three features of procurement law that support our proposal: competitive
negotiation, qualification, and collateral social and economic policies.
1. Competitive Negotiation
An underlying premise of procurement is that it will be achieved
through full and open competition.296 Accordingly various procedures
have been established in order to effectuate that goal. In addition to sealed
bidding, Congress through the Competition in Contracting Act also added
competitive negotiation as an alternative means of achieving full and
open competition.297 This was seen as an opportunity to introduce more
flexibility in procurement procedures while maintaining the features that
facilitate a competitive process.298 An agency thus has significant
discretion to establish criteria, other than price, to be included in its
solicitation or request for proposals (“RFP”).299
Unlike in a sealed bidding process where submissions by a contractor
are either accepted or rejected based on pre-established criteria, the
competitive negotiation is a more open process that allows for discussions
with the contractor regarding terms before a final decision is made.300
This therefore creates an opportunity for a government agency that values
transparency to set certain disclosure requirements among the minimum
standards in an RFP to acquire AI or other technology. In addition, we
think technical specifications regarding accuracy, and fairness, among
other things, could also be a competitive feature of the procurement
process.
2. Qualification and Responsibility
The term “qualification” in procurement law captures a set of
procedures that provide for assessing whether an entity that seeks to do
business with the government meets the qualification or responsibility

AI Government Procurement Guidelines, WORLD ECONOMIC FORUM,
(Sep. 20, 2019), available at https://www.weforum.org/whitepapers/aigovernment-procurement-guidelines.
296
See 10 U.S.C. s 2304.
297
10 U.S.C. s 2304; see also Federal Property and Administrative Service
Act, 41 U.S.C. s 3301.
298
See SCHWARTZ, supra note 52, at 546.
299
Id. at 591.
300
Id. at 614.
295

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

51

standards to perform the requested work.301 This helps to ensure, for
instance, that bidders are responsible, have the right equipment and
personnel, quality assurance and safety programs, and business ethics. 302
Thus, a law enforcement agency could request that offerors offer an AI
system that would meet certain terms and conditions when sold to law
enforcement even if different from terms to others in the private sector or
even other agencies. In addition, perhaps this may also be used to qualify
the type of organization or contractor with which the agency will do
business for forensic algorithms. Thus, an entity without a positive
reputation for providing technology that is sufficiently accurate in the law
enforcement context, or which has traditionally refused limited disclosure
of proprietary information, negatively affecting prosecutors’ ability to use
evidence against a defendant or obtain convictions might not qualify.

3. Collateral Socio-Economic Policies
Finally, it is worth noting that procurement law is already built to
accommodate what is referred to as collateral socio-economic policies,
including, for instance, policies that favor small businesses.303 Indeed,
sometimes procurement has led the way: our federal procurement laws
made it illegal to discriminate based on race even prior to the enactment
of Title VII of the Civil Rights Act.304 Similarly, we foresee achieving
algorithmic transparency through procurement in much the same way.
Some other collateral socio-economic policies that already operate in
federal procurement include preferences for disadvantaged business
owners,305 equal employment opportunity policies,306 preference for
domestic goods,307 and labor standards.308 Moreover, Presidents routinely
issue executive orders that concern social and environmental issues
through procurement.309 For example, the Clinton, Bush, Obama, Trump,
and Biden administrations have all issued executive orders on

See, e.g., 48 C.F.R. § 9.103 et seq.
See 48 C.F.R. § 9.104-1.
303
See Kingdomware Technologies, Inc. v. United States, 136 S.Ct. 1969
(2016).
304
See Executive Order 11246.
305
See 15 U.S.C. s 637(a).
306
EXEC. ORDER NO. 11246 (barring discrimination on the basis of race,
sex, religion or national origin).
307
See, e.g., Buy American Act, 41 U.S.C. ss 10a-d.
308
See, e.g., Davis-Beacon Act, 40 U.S.C. s276a; Walsh-Healey Act, 41
U.S.C. s6502; Service Contract Act, 41 U.S.C. ss 6701-6703.
309
Statutes could serve as a limit on such orders, and spur additional
legislation.
301
302

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

52

procurement.310 As such, adding algorithmic transparency (or algorithmic
justice and fairness more broadly) as contract terms is consistent with
furthering these kinds of policies through procurement, especially in the
absence of appropriate legislation or other regulations.311
The above three features of procurement reinforce that our transactionby-transaction procurement approach is consistent with existing frameworks
in procurement law, trade secret law, and contract law. Indeed, more broadly,
we think procurement law might also be a pathway for addressing broader
questions having to do with algorithmic justice.312 With respect to the
technological acquisitions that are the subject of this Article, there are
reasonable concerns about the level of expertise available to the range of
government agencies, particularly in local and state offices. A system of
outside technical advisors or experts would likely be needed to work with
government agencies to supplement and improve such a scheme. In fact, a byproduct of using procurement in this context is that it may spur legislation,
particularly as the public and corporate interests realize that the executive
branch could be establishing de facto norms through the procurement process.
Limits that may need to be constrained by statute.
The view that the government may further social and economic
policies or algorithmic governance through procurement policies or contract
terms, while potentially controversial, is nonetheless supported by the market
participant exception to the Commerce Clause.313 When, through procurement,
the government behaves as a market participant rather than a regulator, it has
more discretion and can behave differently in those distinct roles.314 Finally,
there may be potential legal objections by contractors who are denied
contracts for failure to meet algorithmic-related technical or transparency
standards. However, to the extent such objections are based on due process or
equal protection claims, they are unlikely to be successful because, among
other reasons, contractors do not have property rights in prospective
contracts315 and distinctions among vendors would not be based on suspect
See Romeo N. Niyongere, European Style Green Public Procurement
in the American Context, 49 PUB. CONT. L.J. 785 (2020); (Clinton) Exec.
Order 12,873, 58 Fed, Reg, 54911 (Oct. 22, 1993); (Bush) Exec. Order
13,423 (Jan. 26, 2007);(Obama) Exec. Order 13,693 (Mar. 25, 2015);
(Trump)Exec. Order 13,495, (Nov. 5, 2019); (Biden)Exec. Order 14,057,
86 Fed. Reg. 70,935 (Dec. 13, 2021).
311
See generally, MCCRUDDEN, supra note 54.
312
In forthcoming work, Professor Rowe will explore using procurement
beyond transparency, to address issues such as algorithmic standards and
technical expertise in the government’s acquisition of technology, as well
as the interaction with legislative constraints.
313
See, e.g., Reeves, Inc. v. Stake, 447 U.S. 429 (1980).
314
See id.
315
See, e.g., Chamber of Com. v. Napolitano, 648 F. Supp. 2d 726, 736
310

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

53

classifications like sex or race.316

D. Proposed Contract Terms to Protect Trade Secrets
& Permit Limited Disclosure
We propose a transaction-by-transaction procurement approach whereby
those government agencies that value transparency and accountability can
negotiate for and insert the appropriate disclosure provisions into their vendor
contracts. As a baseline, any proposed solution to disclosure is best viewed
not as an all-or-nothing, zero-sum game, but along a spectrum where there is
some sharing while also protecting intellectual property rights. To be sure,
determining how and what to share will be quite challenging and questions
abound. For instance: Should disclosure be context-based? Should the parties
anticipate and negotiate for future court-based challenges? What slices of the
pie are actual trade secrets, and which are merely confidential? Most
importantly, what can be shared without competitive harm? It is important to
be mindful that there are several parts to a computer program, including the
algorithmic models, source code, object code, and related files and
instructions. Each may be protected and layered with intellectual property
rights through patent law, copyright law, or trade secret law. It may also be
useful to distinguish between raw data and the interpretation of that data as
well as models, inputs and outputs.317
There can be tremendous flexibility in crafting the terms of the agreement
based on the parties’ preferences, the nature of the technology in question,
and how it will be used. Key points for consideration and negotiation,
however, should be the nuts and bolts of any disclosure: who, what, when,
and how. The “why” is likely to be the agency’s threshold public policy
position of whether to contract for transparency in the first instance.
We recognize that many or perhaps most agencies may not be motivated
to contract for transparency on their own initiative. However, democratic
governance processes along with procurement discretion could persuade the
reluctant agency to adopt this tool, or make the conscious, intentional choice
not to contract for transparency – a choice which will have to be defended to
voters and the relevant public. As the parties are contracting for transparency,
their respective interests should realistically guide their discussions. For the
developer-trade secret owner, protecting its trade secrets from disclosure
remains paramount in order to avoid losing them.318 Not only are trade secret
rights destroyed if improperly disclosed, but trade secret owners are required
(S.D. Md. 2009) (holding that there is no right to be a government
contractor).
316
See, e.g., Lehnhausen v. Lake Shore Auto Parts Col., 410 U.S. 356, 363
(1973).
317
See Pasquale, supra note 271, at 738.
318
ROWE & SANDEEN, supra note 222, at 199-200.

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

54

to take reasonable efforts to protect information that they deem a trade secret,
and courts typically expect such efforts, at a minimum, will include
nondisclosure agreements.319 Thus, managing the terms of confidentiality and
limits of disclosure are critical.
1. Who?
As a threshold matter, it would seem most agencies may wish to allow for
the agency itself to examine and independently verify accuracy, validity, and
reliability of any software that it seeks to acquire. In the first instance, the
agency-purchaser could reserve the right to inspect and analyze the code
subject to protective orders. This may vary depending on the type of
technology in use by the agency (for instance risk assessment versus facial
recognition) and how heavily the agency weighs its desire to verify validity.
When AI produces the results that governmental officials want, such as
fingerprinting, image identification, or DNA matching, few questions are
likely to be raised about the validity of the methodology.320
With respect to risk assessment algorithms, prominent criminal justice
scholars who support the use of these technologies by law enforcement,
mindful of due process considerations, support the need for independent
assessments of such tools. For instance, Christopher Slobogin notes
“[e]galitarian and retributive justice cannot be evaluated without knowing
whether risk scores are based on race, gender, age, wealth classifications, or
proxies for them, and the extent to which they purport to help the state achieve
its aim in evaluating risk. The accuracy of the probabilities and other results
reached by a risk assessment algorithm cannot be confirmed unless the
underlying data and the empirical analysis using it can be evaluated by
others.”321 As such, consideration and negotiation of terms for evaluation by
experts in the context of criminal trials or other litigation are important.
Consideration of third-party access is also important. For instance, will
the terms provide that if a court orders disclosure in a criminal proceeding or
other proceeding, attorneys may be granted access subject to protective orders
(as occurs in the civil context) or will disclosure be limited only to experts
(and which experts)? Will disclosure be tailored to the purpose for which the
information is used? Will disclosure be made to courts, if so, under seal? Are
researchers included, and how is that defined? It may also be appropriate to
seek agreement on the need to disclose the names of employees who
developed, created, modified, or otherwise worked on the technology. This
way they could be available to attorneys, experts, or courts to testify. In the
ShotSpotter case discussed earlier, the company refused to identify the names
of the employees who altered the algorithm at the request of the Chicago

See ROWE & SANDEEN, supra note 222, at 199-201.
See Murphy, supra note 50, at 745-47.
321
SLOBOGIN, supra note 30, at 109.
319
320

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

55

police. 322
Civil courts routinely use protective orders, and many have models for
appropriate protective orders to be used to safeguard disclosure of trade
secrets.323 When a protective order sets out the terms of confidentiality for
exchange of materials, public access is generally not an issue.324 Even in
litigation involving trade secrets, court records can be sealed to protect trade
secrets, although courts are mindful of parties overclaiming information to be
sealed.325
2. What?
In negotiating what may be disclosed, it is important to note that
transparency does not necessarily require everything, including the secret
sauce. In the mix of potential ingredients such as models, data, codes,
processes, trials, verifications, and testing, decisions can be made by
balancing what are actual trade secrets requiring preservation and what are
merely confidential or proprietary for which a managed sharing arrangement
makes sense.326 Consideration of the standards or goals for transparency may
also provide a useful framework. For instance, some scholars have noted that
to meet its constitutional obligations, the government should disclose input
variables collected about the individual in question, data to support the
accuracy of the algorithm across individuals, and results from verification
procedures.327 Others have also argued that even if source code were made
available, that in itself is not necessarily sufficient transparency given the
realities and complexities of machine learning.328
Depending on the technology at issue, transparency could also pose a risk
of circumvention, which should be considered and avoided. While, for
instance, disclosing source code in a DNA analysis tool poses almost no risk
of helping future criminals evade detection of their DNA, disclosing too much
information about risk assessment tools could, in contrast, conceivably assist
someone in gaming the instrument to the extent that it relies on subject
survey/Q&A input to get results (which some do). The stage at which the tool
is to be used as evidence may also affect the level of transparency negotiated
See State of Illiniois v. Williams, Motion to Exclude Shotspotter
Evidence Pursuant to Frye and Rule 403, 20CR0899601.
323
See, e.g., Northern District of Illinois Model Protective Orders, Northern
District of California Model Protective Orders, and New York Supreme
Court Model Protective Orders.
324
See DePuy Synthes prods., Inc. v. Veterinary Orthopedic Implants, Inc.,
990 F.3d 1364, 1370 (Fed. Cir. 2021).
325
See, e.g., Id. at 1372.
326
See Coglianese and Lehr, supra note 148, at 148-49.
327
See id. at 141-42.
328
See Katyal, supra note 30 at 1251; Frank Pasquale, Beyond Innovation
and Competition: The Need for Qualified Transparency in Internet
Intermediaries, 104 NW. U. L. REV. 105, 162 (2010).
322

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

56

since defendants already have certain rights for evidence used at trial, such as
TrueAllele for DNA, versus risk assessments like COMPAS.
It is also worth noting that while vendors might start from the standpoint
that everything they wish to protect is a trade secret, that approach is unlikely
to survive scrutiny from a trade secret law perspective. Some developers’
agreements currently prevent police departments from disclosing the very
existence of certain technologies in use for surveillance by the departments.329
However, not all confidential information is a trade secret.330 There are many
components that go into the kinds of AI systems, including hardware,
software, the data that is fed into the system as input, and the output that is
generated from the system.331 Depending on the circumstances, some but not
all of these components may be protectible trade secrets. Thus, in drafting
agreements about what may be disclosed, it is important to be precise, rather
than take a one size fits all approach. Of course, as some scholars have noted,
there is also the question of whether the government should, normatively, be
entitled to claim trade secrecy on information related to public functions,332
and query whether in the hybrid private-government contractual context
addressed in this Article, ownership and claims to trade secrecy should remain
with the private developer.333
3. When and How?
The parties may wish to consider the circumstances under which
disclosures may be made. This can range from “upon request of any
individual” to “only by court order.” In trade secret litigation, courts routinely
enter protective orders that set out in detail the terms of access, including what
may be marked, how it should be labeled, use of confidentiality agreements,
access by independent experts, limits on use and disclosure, and how and
whether any materials shared should be destroyed or returned after use.334
Sometimes only experts have access, not the parties.335 Other times, any
inspections are made in camera by a judge.336 Certain information can be
See Wexler, supra note 30, at 1366-67.
ROWE & SANDEEN, supra note 222, at 43 .
331
See APLIN & SANDEEN, supra note 245, at . (last page) [book chapter
draft July 22, 2021]
332
See Graves & Katyal, supra note 30, at 1381-85.
333
Some have noted that the private non-delegation doctrine ought to apply
in these types of situations and that arrangements with private entities
should be structured to preserve constitutional values and accountability.
See, e.g., Andrea Nishi, Privatizing Sentencing: A Delegation Framework
for Recidivism Risk Assessment, 119 COLUM. L. REV. 1671, 1695 (2019)
(quoting Gillian E. Metzger, Privatization as Delegation, 103 COLUM. L.
REV. 1367, 1456 (2003)).
334
ROWE & SANDEEN, supra note 222, at 469-70. See, e.g.,
https://www.cand.uscourts.gov/forms/model-protective-orders/ .
335
See Katyal, supra note 30, at 1277.
336
See id.
329
330

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

57

sealed in the court records, court rooms can be temporarily cleared while
some trade secret information is being discussed, or perhaps access can be
limited for research purposes only.
Mindful of the court’s gatekeeping function, one court has cautioned:
“[h]iding the source code is not the answer. The solution is producing it under
a protective order. Doing so safeguards the company's intellectual property
rights and defendant’s constitutional liberty interest alike. Intellectual
property law aims to prevent business competitors from stealing confidential
commercial information in the marketplace; it was never meant to justify
concealing relevant information from parties to a criminal prosecution in the
context of a Frye hearing.”337 Assuming that a defendant has established a
particularized need for the source code, the court found it reasonable that
source code would be produced, subject to a protective order, in order to
review the software’s reliability.338 Thus, such terms that permit in camera
reviews by a judge or other protective mechanisms would be consistent with
the views of advocates for greater transparency in criminal justice trials.339
Indeed, just as some jurisdictions already have adopted model language
for protective orders in cases involving highly sensitive information and
source code,340 model language for AI procurement contracts as contemplated
in this Article could also be drafted and made available. This would be
especially helpful starting points to under-resourced government agencies,
especially those in smaller cities who may not have ready access to attorneys
or relevant experts. For instance, software license agreements generally
contain nondisclosure provisions, the gist of which allow disclosure when (a)
required by law, (b) pursuant to a court order, (c) prior written notice is
provided from the government agency to the vendor, (d) assistance is
provided by the government agency in obtaining a protective order, (e]
sharing only under “attorneys’ eyes only,” and (f) specific conditions on how
any code or sensitive material is to be handled, reviewed, stored, and
protected are made explicit.
E. Benefits of Contract Approach
General contract law principles require mutual assent for contract
formation.341 Sometimes, however, particularly in situations where a
consumer signs a contract with a manufacturer or developer without the
ability to negotiate its terms, it is referred to as a contract of adhesion or
standardized contract. These contracts are not necessarily invalid, but courts
State v. Pickett, 246 A.3d 279, 311 (N.J. Sup. Ct. App. Div. 2021).
Id.
339
See, e.g., Wexler, supra note 30, at 1403–13; see also Citron &
Pasquale, supra note 30, at 26.
340
See, e.g., those in the Northern District of California, available at
https://www.cand.uscourts.gov/forms/model-protective-orders/ .
341
See WILLISTON ON CONTRACTS § 4:1 (4th ed. 2017).
337
338

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

58

may examine the terms more closely to determine whether they are
unconscionable.342 Naturally, these agreements will often be more favorable
to the party that drafted them, rather than the consumer.343 Thus, as discussed
earlier, this appears to be the status quo, where developers dictate terms that
forbid disclosure and the government agency accepts, probably with little
push back. Those circumstances notwithstanding, it is possible to envision
contract law as a means to further support greater sharing by the government
in the procurement context, while respecting the rights of developers.344
Organizations and agencies that are inclined toward disclosure can
negotiate disclosure provisions in their contracts to suit their particular needs.
For instance, we reviewed the terms of three Arnold Foundation contracts
with courts in three different jurisdictions – Arizona, California, and Florida.
While most of the other contract terms were almost identical among all three,
the non-disclosure provisions varied. The Arizona clause provided in part
“[t]he Arizona Courts agree to refrain from disclosing any information about
the Tool, including information about the development, operation and
presentation of the Tool, to any third parties without prior written approval
from the Foundation.”345 The California provision contained the same
language prohibiting disclosure of any information, and further added that “if,
however, the Court is presented with a request for documents . . . the Court
will immediately give notice to the Foundation of the request and . . . provide
the Foundation with the opportunity to contest such process.”346 Finally, the
Florida contract referred not to information generally, but specifically to trade
secrets. It required specific designation by the Foundation of any information
it considers a trade secret pursuant to Florida law.347 It then provided that “[t]he
Circuit agrees to refrain from disclosing, absent the entry of a court order by
a court of competent jurisdiction, any trade secret about the Tool” and that
the Foundation would receive notice and opportunity to contest requests for
production.348 In addition, “[t]he Circuit will reasonably assist the Foundation
if necessary to defend properly designated trade secrets but will have no
obligation to initiate an action to defend such designation.”349 Thus, this
illustrates the flexibility with which the same vendor can offer the same
technology to different agencies under individually-tailored negotiated terms.
See Jay Kesan et al., supra note 44, at 424.
Id.
344
See Matwyshyn, supra note 47, at 5 (arguing that contract law can be
used as a means to protect consumer privacy).
345
MOU Between the Arnold Foundation and the Administrative Office of
the Courts, Arizona Supreme Court and the Superior Court in Pima
County, at 3 (January 2016) (on file with authors) (emphasis added).
346
MOU Between the Arnold Foundation and Superior Court of California,
Count of San Francisco, at 3 (August 2015) (on file with authors).
347
MOU Between the Arnold Foundation and the Seventh Judicial Circuit
of the State of Florida, at 3 (June 2015) (on file with authors).
348
Id.
349
Id.
342
343

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

59

While the government agencies and developers who are not inclined
toward sharing, might instinctively prefer the status quo’s, absoluteconfidentiality terms, it is worth examining the risks that a court down the
road might not agree with those terms. For instance, as noted earlier, the
Pickett case is illustrative. There, the parties reached an impasse when trying
to negotiate the terms of a protective order for source code.350 The defendant
agreed to a prohibition on disclosure to any individual with “any direct or
indirect commercial or employment interest in competing software products”
and to certain other safeguards.351 However, the prosecution insisted on
additional and more expansive protections including a requirement that the
software be reviewed only at the prosecutor’s office in a supervised
inspection, permitting only handwritten notes on the 170,000 lines of code (a
process which was estimated to take eight years to enable understanding the
code).352 The appeals court remanded, finding that “anything less than full
access contravenes fundamental principles of fairness, which indubitably
compromises a defendant’s right to present a complete defense.”353 Instead,
it directed the trial court to compel disclosure of the source code and related
materials “pursuant to an appropriate protective order.”354
The negotiated contract approach proposed in this Article, helps mitigate
against these uncertainties and brings a more tailored and flexible solution
that meets the parties’ interests. While a one-size-fits-all approach may bring
some efficiencies (for instance an agency could develop a “standard” set of
provisions for all its AI vendors) it may not be ideal for all circumstances.
Additionally, the procurement approach here provides both procedural and
substantive advantages.
1. Better control and accountability
Individually tailored procurement practices could provide more
flexibility for agencies to do what they wish and be in control of their
decisions and choices. As critics have noted, governmental discretionary—
and largely secretive—decision-making processes associated with AI
procurement provide no assurance of accountability or validity of AI use.355
In essence, government agencies are relying on AI developers to self-police
and ensure their own algorithm’s accuracy. The perception is that so long as
the technology provides the respective agency with the results it is hoping for,
the agency has little reason to question its functionality.
The reality is that the government agency is a consumer in a uniquely
advantageous bargaining position compared to individual consumers in the

State v. Pickett, 246 A.3d 279, 309-10 (N.J. Sup. Ct. App. Div. 2021).
Id. at 310.
352
Id. at 309.
353
Id. at 311.
354
Id.
355
See infra Part I.B.
350
351

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

60

private marketplace.356 As such, agencies do not have to accept form
agreements and contracts of adhesion. Instead, agencies can better command
the terms for disclosure and ownership of the technologies that they
acquire.357 For instance, the government can negotiate and preserve
ownership and control rights (if desired). Indeed, just having possession of
testing data and records would get the respective agency closer to being able
to disclose the information in response to FOIA requests.358 It will also allow
agencies to allay concerns that they have not adequately reviewed or
evaluated the technical or functional aspects of the technology.359
2. Better System Integrity
Thinking through and weighing its desired contractual terms will allow
an agency to be more intentional about its algorithmic decisions and processes
not only from a procurement perspective, but ultimately to better comply with
its substantive legal obligations. Thus, in anticipating future court challenges,
agencies will be better able to justify the design and validity of their decisionmaking algorithms.360 This could further help avoid downstream negative
consequences, including preserving convictions on appeal. As the court in
Pickett noted:
If, as Dr. Perlin [the state’s expert] maintains, the source code he
wrote is free of harmful defects, and therefore will not impact the
reliability of TrueAllele, then it is to everyone’s advantage to learn
that at the Frye hearing. If it should turn out that there are source
code errors that might affect TrueAllele’s reliability, the time to
discover that information is now, as part of the judge’s gatekeeping
role. Reliability must be resolved at the Frye hearing rather than in
post-conviction relief proceedings.361
Ultimately, it is more efficient to have a system in place that gets it right the
first time. It reduces the risk that courts will find the technology inadmissible
at Frye hearings if it has not been independently tested and validated and does
not provide access to defense counsel because of overly broad secrecy
provisions. 362 Moreover, it would be an incentive for departments to negotiate
disclosure terms ex ante rather than risk not being able to use evidence to
See Brauneis & Goodman, supra note 30, at 165 (noting that
government agencies have leverage even when they are not paying the
vendor).
357
Id. at 164-65 (discussing the Seventh Judicial Circuit of Florida’s terms
with the Arnold Foundation for its PSA program).
358
Id. at 135-137.
359
See infra Part I.B.3.
360
See Coglianese & Lehr, supra note 148, at 143-44.
361
State v. Pickett, 246 A.3d 279, 301 (N.J. Sup. Ct. App. Div. 2021).
362
See Wexler, supra note 30; SLOBOGIN, supra note 30.
356

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

61

obtain a conviction, as happened with the case involving ShotSpotter in
Chicago.363 The fact that employees could manually change the data and
algorithm – after the fact – to suit law enforcement is not only a bad look
effecting the integrity and credibility of the system as a whole, but the
particular technology as well.
3. Better for Vendors Too
Vendors ought to be able to protect their legitimate trade secret rights.
Nothing in this Article should be read to suggest otherwise, because it is
important to preserve expenditures in research and development and to spur
further innovation that ultimately benefits consumers and the public.
Developers expend tremendous amounts of resources in research and
development in order to build and create AI technologies.364
Accordingly, their desire to protect legitimate intellectual property rights
in transactions with the criminal justice system are reasonable and it is
imperative that they do so. From the developers’ perspective, documents and
data related to their technology should be treated as if they are trade secrets.
Thus, for example, trade secret data must be segregated from other kinds of
data, particularly when they are made available to third parties, and all such
parties should execute confidentiality agreements. Furthermore, the
information should only be shared on a need-to-know basis.
Contracts also continue to be vital to buttressing intellectual property
protections, as the specific agreements between the parties can sometimes
provide extra protection beyond that which is available in each individual area
of intellectual property. A further advantage for developers is that they can
negotiate what works best for their circumstances and the technology. If
subject to broad regulations, requirements tend to be rigid, structured, and of
one-size.
Indeed, transparency does not require an all or nothing approach.
Legitimate trade secrets such as source code, could be protected and withheld
from disclosure, releasing only those records and processes necessary to
support validation and testing.365 Furthermore, beyond trade secrecy, data
security might also weigh against 100 percent disclosure in order to protect
all parties, including the public.366 These negotiated disclosure terms would
See Feathers, supra note 6.
See Justin Jouvenal, A Secret Algorithm is Transforming DNA Evidence.
This Defendant Could be the First to Scrutinize It, WASH. POST (July 13,
2021,
08:00),
https://www.washingtonpost.com/local/legalissues/trueallele-software-dna-courts/2021/07/12/66d27c44-6c9d-11eb9f80-3d7646ce1bc0_story.html (Cybergenetics spent decades and
millions of dollars developing its DNA analysis software, TrueAllele).
365
See Brauneis & Goodman, supra note 30, at 132.
366
See Katyal, supra note 30, at 1185 n. 7; National Security Memorandum
363
364

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

62

provide better certainty and clarity going in to the business relationship and
perhaps better consistency; the contracts could be used for multiple agencies,
rather than relying on the whim of individual state court decisions.
Accordingly, there might be better control over the management and
protection of trade secrets in these technologies licensed to the government,
instead of leaving it to chance. Indeed, sometimes developers could be subject
to out of state subpoenas for algorithmic models, if the parties cannot agree
on terms for protective orders.367 Therefore, negotiating in advance of the
transaction the terms of disclosure through the procurement process could
help avoid surprises and further litigation costs.368
Moreover, recognizing the competitive marketplace in which these
transactions occur, if suitable solutions are not achieved in the governmentprivate vendor relationships for the procurement of AI, other “competitors”
may exist. Thus, for instance, some government agencies may choose to
partner with academic institutions, that presumably would be less likely to
resist disclosure and sharing, to develop their software.369 This in turn could
lead to a significant loss of revenues for private developers who tend to rely
heavily, and often exclusively, on government contracts. ShotSpotter, for
instance, discloses that “[t]o date, substantially all of our revenues have been
derived from contracts with local governments and their agencies, in
particular the police departments of major cities in the United States. We
believe that the success and growth of our business will continue to depend
on our ability to add new police departments and other government agencies,
domestically and internationally, as customers of our public safety
solution….”370
on Improving Cybersecurity for Critical Infrastructure Control Systems,
WHITEHOUSE.GOV (Jul. 28, 2021) https://www.whitehouse.gov/briefingroom/statements-releases/2021/07/28/national-security-memorandum-onimproving-cybersecurity-for-critical-infrastructure-control-systems/
(stating that cybersecurity resiliency is of paramount importance against
the cybersecurity threats towards government operated systems that are
among the most significant and growing issues confronting our Nation).
367
See Whitney Kimball, A Man on Death Row Has Waited Years for
GitHub to Provide Key Evidence. Here's Why It Refuses., GIZMODO (May
27, 2021, 14:10), https://gizmodo.com/a-death-row-inmate-has-waitedyears-for-github-to-provi-1846976389.
368
Conceivably, from the developer’s perspective future product liability
claims might also be limited or avoided if independent verification and
testing were allowed at the outset.
369
See Brauneis & Goodman, supra note 30, at 152 (noting that Allegheny
County contracted with university researchers to develop its predictive
algorithm).
370
ShotSpotter, Inc., Annual Report (Form 10-K) (March 29, 2021),
available at https://ir.shotspotter.com/annual-reports. See also Feathers,
supra note 6 (Shotspotter’s $33 million contract with the city of Chicago
accounted for 13 percent of the company’s first quarter revenue of 2021,

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

63

4. Better for Public Interest in Governmental Transparency
Ultimately, as the elusive search for protection of the “public interest”
continues, ideally through legislative policy developments, a more immediate
measure becomes necessary. As scholars have noted, the public interest in
governmental transparency is critical for democratic governance and the
criminal justice system.371 Given the lack of a robust public interest
framework in trade secrecy,372 the procurement approach proposed in this
Article presents a concrete tool that might allow for a negotiated role for the
public interest where it does not currently exist. Because there is no
monolithic “public interest” we must be mindful of both the public interest in
transparency and the public interest in the protection of IP. Both represent
bedrock principles in their respective spheres, but as they collide in privategovernment ventures, procurement is one way to potentially achieve a
peaceful resolution that serves both sides of the transaction and the courtroom.
Additionally, there is informational value in knowing which vendors and
which government agencies value transparency and which do not. For
instance, by encouraging market negotiations about transparency, our
approach could deliver valuable information about just how much
transparency vendors may be willingly to tolerate. If vendors say “no” to
especially pro-transparency jurisdictions, the willingness to forego seemingly
profitable licensing deals tells public policy makers just how high a price
vendors place on secrecy. Similarly, an agency’s choices and actions in
choosing or not choosing to negotiate for transparency can convey valuable
information to relevant constituents.
Finally, a lack of transparency does not only effect specific criminal
defendants, but the public as a whole, including researchers. To the extent
broad trade secret protections prevent non-profits, university researchers, and
other data scientists from performing critical research and studies on
algorithms to determine whether the algorithmic models perform in an
unbiased manner and what flaws may exist, they prevent algorithmic

making it Shotspotter’s second biggest client after New York City, which
accounted for 34 percent); Garance Burke et al., How tech led to a murder
charge
with
no
evidence,
MIAMI
TIMES
ONLINE,
https://www.miamitimesonline.com/news/world_national/how-tech-ledto-a-murder-charge-with-no-evidence/article_31eafdae-04ef-11ec-954c97438876f262.html (last updated Aug. 27, 2021) (the City of Miami has a
$5 million dollar contract with Shotspotter); see also Burke et al., supra
note 1 (“The U.S. government has spent more than $6.9 million on gunshot
detection systems, including ShotSpotter, in discretionary grants and
earmarked funds…”).
371
See supra Part I note.
372
See supra Part II.C.

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

64

accountability.373 For instance, there is little evidence that algorithm creators
have tested for racial or other bias, and the few third-party studies that have
been performed suggest that developers have failed to conduct such testing.374
Testing and transparency are necessary because it is often difficult to check
even the basic math on some algorithms to verify proper construction of
algorithmic models, and because the algorithms could be redesigned to
function in a manner that would reduce bias and racial disparities.375
Similarly, there is also the general benefit of overall accuracy (that most
citizens might automatically assume exists) when the government chooses to
use technology for decision-making in any context whether it implicates
voting, driving, filing taxes, or just walking down the street.
5. Potential Drawbacks
Despite the benefits of the negotiated contractual approach, there are
potential drawbacks worth considering. As we noted earlier, there are
reasonable concerns about the level of expertise available to the range of
government agencies, particularly in local and state offices to make informed
decisions and undertake negotiations about these technologies. Many
governments agencies have far too little time and expertise to consider the
optimal level of openness for a given predictive tool and to negotiate for it on
an individual basis. A system of outside technical advisors and legal experts
would be useful to consult with interested agencies. Moreover, in cases in
which a contractor is selling a tool to many government agencies (e.g.,
ShotSpotter, COMPAS), the contractor may be in a position to say no to the
few (if only few) agencies that want significant transparency because it still
has most of its business, and it avoids what it, perhaps unreasonably, believes
to be the risks of disclosure.
These concerns could be addressed by further considering two options
(though there could be several others). The first is some form of cooperative
purchasing. There are already several organizations that do cooperative
purchasing for state, tribal and local governments.376 Organizing cooperative
373

Chaney, supra note 150.
See Angwin et al., supra note 100 (finding that even when criminal
history, recidivism, gender, and age were isolated, from the COMPAS
algorithm, Black defendants were still 77% more likely to be flagged for
higher risk of future violent crime); Stevenson, supra note 168 (discussing
the racial inequalities of pre-trial release for Black defendants compared
to White defendants); Simonite, supra note 169 (noting judicial discretion
results in harsher rates of incarceration for defendants of lower socioeconomic status).
375
Chaney, supra note 150.
376
These include private companies like Omnia Partners, nonprofit
membership organizations like the National Cooperative Purchasing
Alliance, National Association of State Purchasing Officials
374

Electronic copy available at: https://ssrn.com/abstract=4044178

Procuring Algorithmic Transparency

65

purchasing of predictive tools, with a level of transparency that would be
above what most local governments would get on an individual basis, could
be very important. The second would be the formation of a standard-setting,
testing, rating organization that could certify predictive tools as meeting
certain standards for openness (possibly with several different levels), and
maybe also for meeting certain statistical standards. Government agencies
could then indicate through the procurement policies we discussed earlier that
they would purchase only tools that met a certified level of openness.
CONCLUSION
As the government increasingly relies on private vendors to supply
its technologies and the attendant algorithms that aid decision-making, the
public’s call for transparency will present significant challenges. Private
vendors’ assertion of trade secret rights in these technologies seemingly
conflict with the public’s need for disclosure. Ideally, legislated exemptions
(both state and federal) could make clear the terms and conditions governing
disclosure of algorithms in the public sphere. Such exemptions, however, are
unlikely to occur on a widescale.
In the meantime, the existing trade secrecy framework supports a
potential solution. We proposed a transaction-by-transaction procurement
approach whereby those agencies that value transparency and accountability
can insert the appropriate disclosure provisions into their vendor contracts.
This is a practice that is consistent not only with trade secret law but with
existing federal and general law and policy on government procurement. Our
proposal allows tremendous flexibility in crafting the terms of the agreement
based on the parties’ preferences, the nature of the technology in question,
and how it will be used. Furthermore, it provides better flexibility and control
for agencies. The approach has benefits for vendors as well, including that the
negotiated disclosure terms would provide better certainty and clarity upon
entering into a business relationship with the government, rather than relying
completely on the whim of individual state court decisions.
Finally, given the lack of a robust public interest framework in trade
secrecy,377 the procurement approach proposed in this Article presents a
concrete tool that might allow for a negotiated role for the public interest in
governmental transparency where it does not currently exist. With respect to
the public interest more generally, our proposal can be generalized and
applied broadly to various contexts. A lack of transparency effects not only
criminal defendants, but non-profits, university researchers, other scientists
and engineers, and members of the press concerned about their lack of access,
as well as the larger problem of algorithmic accountability.
ValuePoint and BuyBoard, state government agencies like Sourcewell (a
Minnesota agency), and the federal General Services Administration,
which offers state and local governments GSA schedule purchasing.
377
See supra Part II.C.

Electronic copy available at: https://ssrn.com/abstract=4044178

