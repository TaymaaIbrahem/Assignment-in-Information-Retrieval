CHI 2018 Honourable Mention

CHI 2018, April 21–26, 2018, Montréal, QC, Canada

Towards Algorithmic Experience: Initial Efforts for Social
Media Contexts
Oscar Alvarado
Universidad de Costa Rica
San José, Costa Rica
oscar.alvaradorodriguez@ucr.ac.cr
ABSTRACT

Algorithms influence most of our daily activities, decisions,
and they guide our behaviors. It has been argued that
algorithms even have a direct impact on democratic
societies. Human - Computer Interaction research needs to
develop analytical tools for describing the interaction with,
and experience of algorithms. Based on user participatory
workshops focused on scrutinizing Facebook’s newsfeed, an
algorithm-influenced social media, we propose the concept
of Algorithmic Experience (AX) as an analytic framing for
making the interaction with and experience of algorithms
explicit. Connecting it to design, we articulate five functional
categories of AX that are particularly important to cater for
in social media: profiling transparency and management,
algorithmic awareness and control, and selective algorithmic
memory.
Author Keywords

Algorithms; algorithmic experience; social media; usercentered design; research through design
ACM Classification Keywords

H.5.2. User Interfaces: User-centered design
INTRODUCTION

Algorithms have permeated our society to the extent that
they have begun to influence our culture and daily practices
[15,30], such as when Facebook’s News Feed has become
the main source of information of government and politics
source for over 60% of the millennials [8:56]. Gillespie
argues that the involvement of algorithms in sorting and
recommending cultural products makes them part of culture
in themselves [15], not only because they manage culturally
related data, but also because they directly influence shifting
opinions. Algorithms carry meaning and values; Bozdag [5]
argues that they cannot merely be considered technical tools
due to the influence of humans in their design and operations.
Geiger argues that those who have the power to decide, write
or design what is executed in code, also have the power to
regulate the behaviors and opinions that algorithms entice
[13:351–352].
© 2018 Association for Computing Machinery. ACM acknowledges that
this contribution was authored or co-authored by an employee, contractor
or affiliate of a national government. As such, the Government retains a
nonexclusive, royalty-free right to publish or reproduce this article, or to
allow others to do so, for Government purposes only.
CHI 2018, April 21–26, 2018, Montreal, QC, Canada
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5620-6/18/04 $15.00
https://doi.org/10.1145/3173574.3173860

Paper 286

Annika Waern
Uppsala University
Uppsala, Sweden
annika.waern@im.uu.se
This means that algorithms have a direct relationship with
the user experience of the systems of which they are part.
Eslami et al. noted how some users who discovered the
algorithm management of their Facebook news feed were
surprised and angered [12], such as when close friends and
family were not shown in their news feed [12:159]. They also
found that a majority of the public does not know that their
Facebook news feed is curated by an algorithm [12:156].
Also “algorithm aversion” has been observed [9], when
people prefer human intervention, even if less accurate and
efficient, over any algorithmic related solution.
The inner working of algorithms remains largely inscrutable
[18:25]. The decisions made by algorithms result from
complex processes, and are influenced by both use and by
continuous changes in both algorithms and interface [18:25–
27].
Hamilton et al [16] propose the “design of algorithmic
interfaces” as an important research topic, striving to balance
the users’ need for transparency with the benefits of
automatic adaptation. Diakopoulos [8] argues that such
research must take the user experience into account. Bucher
states that it is crucial to study how people feel about
algorithms, “…and while algorithms might not speak to
individuals, they might speak through them” [7:42]. We also
need to understand more about how user knowledge about
algorithms affects their interactions with algorithminfluenced systems and services [16] and what attitudes users
develop towards algorithms.
Hamilton et al. [16] further explain that while the invisibility
of these algorithms can be considered successful in that it
produces less effort for the user, a seamful approach could
improve the opportunities for new uses and for addressing
some of the societal effects of algorithms [16:633–634].
Diakopoulos [8] strives for a particular focus on
transparency, also including notifying users when humans
have been involved in aspects of the presentation that users
have come to understand as algorithmically controlled.
This article proposes the new concept of algorithmic
experience (AX) as an analytic tool for approaching a usercentered perspective on algorithms, how users perceive them
and how to design better experiences with them. Using
Facebook’s news feed as an algorithmic influenced case
example, we performed a semiotic analysis and two
participatory design workshops to develop an understanding
of salient features for AX of algorithm-influenced social

Page 1

CHI 2018 Honourable Mention

media. Connecting the results to design, we propose five
different categories to improve AX in social media contexts:
algorithmic profiling transparency, algorithmic profiling
management, algorithmic awareness, algorithmic usercontrol and selective algorithmic remembering. These five
areas suggest an initial framework capable of promoting
requirements and guide the design or evaluation of AX in
social media contexts that look to increase awareness of the
algorithmic influence on use and promote users’
empowerment towards these tools.
BACKGROUND
Algorithms worthy of experience analysis

First, we should determine what characterizes the algorithms
that become agent in this role; what kind of algorithms could
be described as experience worthy. Willson argues that an
important class of such algorithms are those to which we
delegate everyday tasks [30], and further argues that
delegating activities to algorithms is becoming an everyday
practice in itself [30:146]. Relevant characteristics of these
algorithms is that they “operate semi-autonomously, without
the need for interaction with, or knowledge of, human users
or operators” [30:139]. Their level of authority is a crucial
factor in determining their experience worth; Beer argues
that algorithms that are entitled to make decisions without or
with minimal human intervention deserve particular
attention [2:3].
In focusing on the concept of “trending” which typically is
algorithmically determined, Gillespie argues that some
relevant algorithms create specific audiences, as groups that
have been algorithmically identified based on interest profile
[15]. In another article, the same author considers “public
relevance algorithms” and suggests six provisional
dimensions as a way to delimit these [14]:
1.
2.
3.
4.
5.

Patterns of inclusion: algorithms that select information
and exclude other information.
Cycles of anticipation: algorithms that make inferences
about its users.
Evaluation of relevance: algorithms that determine what
is relevant, correct, or legitimate knowledge.
Perceived algorithm objectivity: algorithms that present
themselves as impartial and exempt of human
intervention.
Entanglement with practice: algorithms that impulse
users to reshape their practices.

Gillespie calls for instructing audiences in how such
algorithms work, which indicates that these constitute a
subclass of the experience worthy algorithms. The
characterizations proposed by Willson and Gillespie guided
the choice of methods used in this article as well as the
investigated algorithmic experiences.
To become encountered by end users, algorithms like any
other material need to present a concrete experience. This
experience will be framed by the technical context and
situation of use; Dourish describes how the material

Paper 286

CHI 2018, April 21–26, 2018, Montréal, QC, Canada

manifestations of algorithms are shaped by the “specific
instantiation – as a running system, running in a particular
place, on a particular computer, connected to a particular
network, with a particular hardware configuration” [11:5].
The algorithmic effects on social media

Social media have increasingly come into focus as having
societal impact, such as in the discussion of their role in
creating “filter bubbles” and “echo chambers”
[1,3,4,6,19,20,24]. Algorithms provoke also other effects in
social media through their ability to prioritize, classify,
associate and filter various sources of information [8:57–58].
Rader and Gray discuss how the algorithms employed in e.g.
Facebook create a feedback loop, in which the user’s
behavior (shares or likes) define what he/she also consumes,
creating a loop in which the algorithm always presents items
that the user in turn will almost certainly interact with
[23:173]. Bozdag found that users predominantly trust their
social networks as a source of information. When Facebook
promotes recommendations from the user’s most active
friends and demotes actions from less active friends, this
means that the system controls both the users’ information
and who they can reach [5].
Algorithmic experiences in social media

Using Facebook as a research case, Bucher studied the ways
in which users related to its algorithmic behavior and called
it algorithmic imaginary. She catalogued the following
reactions to the influence of algorithms [7].
Profiling identity: the feeling of being classified or profiled,
not necessarily accurately. An example of this experience is
when a middle age woman is bombarded with losing weight
advertisement, but she is not interested.
Whoa moments: situations in which people sense that they
“have been found” by the system, such as if user is having
coffee and simultaneously Facebook’s ads show a coffee
brand suggestion.
Faulty prediction: when the user sense that the algorithm is
wrong, producing annoying experiences that does not match
the user’s beliefs and interests [7:35–36]. Users that have this
feeling describe the system as broken or malfunctioning.
Popularity game: when users feel that they act to catch the
attention of the algorithm and thereby get increased
visibility, as well as the feeling of not getting enough likes,
comments or shares due to algorithmic performance.
Cruel connections: the feeling that algorithms are insensitive
and unable to relate to human feelings. Being reminded of
the birthday of a recently deceased friend can trigger such
reactions.
Ruined friendships: the feeling that the algorithm curates not
only content but also relationships, and that you lose control
over your friendships due to the way the algorithm shows
content from certain friends and not from others.

Page 2

CHI 2018 Honourable Mention

It should be noted that these are primarily negative
experiences, related to moments when the algorithmic
behavior is foregrounded in the user’s experience due to
unexpected or undesirable results. In this article, we wish to
develop a more value-neutral perspective on the experience
of algorithms. Furthermore, algorithmic imaginary is “the
way people imagine, perceive and experience algorithms”
[7:31], that is, subjective and related to specific encounters.
In this article we instead explore AX as a property of the
service or the interface itself.
Rader and Gray 2015 [23] investigated user beliefs about the
algorithmic influence over Facebook’s news feed. They
found that while many users made no assumptions about this
at all, some common conceptualizations included the feeling
that other Facebook users had made active decisions to
“hide” themselves or specific posts, as well as the belief that
one must actively set preferences to get the news feed sorted
in any way at all.
Re-designing the AX of social media

There have been multiple experiments with re-designing
social media to change their algorithmic experience. These
serve as important inspirations to charter what potentially
could be considered a desirable AX.
Eslami et al. [12] designed FeedVis as a way to illustrate the
effect of algorithmic influence over the Facebook’s news
feed. This tool presents to Facebook users a comparison
between algorithmically curated and not curated News
Feeds. In their study, they found that some participants were
unaware of the algorithmic influence over this function and
upset when they discovered that some of their closest friends
and family did not appear in their news feed.
A visualization tool proposed by Nagulendra and Vassileva
shows which friends are inside or outside their users filter
bubble [22], and allows users to manually include and
exclude friends from their bubble.
Finally, Munson et al. [21] developed a browser plug-in that
highlights the user’s reading tendencies and most common
political biases. While the plug-in was successful in making
readers aware of reading bias, it did not change their
behavior.
METHOD

A user-centered approach towards eliciting desirable
qualities for AX is not straightforward, since the general
awareness of algorithmic influence is low among users [12].
To address this issue, a three-step process was applied. The
first step involved the author sensitizing himself towards the
AX of Facebook through semiotic inspection method
[29:26–33], eliciting an understanding of what use behaviors
the design communicates as intended and encouraged.
Algorithms need not be “black boxes” [17], but can and to
some extent are expressed by the system’s interface. To
create a baseline understanding of the current designer
intentions towards the algorithmic capacity of the system,
Semiotic inspection was applied. Next, two sequences of

Paper 286

CHI 2018, April 21–26, 2018, Montréal, QC, Canada

workshops were conducted in which recurrent users of
Facebook were gathered to discuss its AX and prioritize
among possible features that potentially could enhance it.
Semiotic Inspection

The Semiotic Engineering Process (SEP) is based on
elements of communication theory and semiotics. It was
developed by De Souza and Leitão [29] and presents a way
to expose interfaces as a communication process between the
designer and the user. It differs from ordinary heuristic
inspection in its explicit focus on the designer’s intentions,
rather than the end user experience.
The SEP framework proposes two methods for analyzing an
interface: Semiotic Inspection Method (SIM) and
Communicability Inspection Method (CIM) [29:23–25]. It is
the first of these that was employed in this work. SIM
provides tools to elicit the system’s message for the user
through inspecting an interface.
The method consists of five stages of inspection. The first,
second and third stages are related to understanding signs,
and relate to Metalinguistic Signs, Static Signs and Dynamic
Signs, respectively. These stages need to be iterated in order
to deconstruct the designer message [29:27]. The task is to
look for graphic elements and their distribution, signs and
space, specific terms and common words, the interaction
opportunities and the interface elements related to
interaction.
The final step in the SIM method is to compare the different
aspects of meta-communication with the aim to detect
consistent and inconsistent patterns and relationships
between the uncovered elements [29:32]. This results in an
overarching analysis, in which the researcher judges the
system’s communication strategy and identifies elements
that define the designer’s intended message to the user.
Semiotic inspection was applied to the Facebook app, as it
appeared at the time of analysis (between January and March
2017) on less of 5 inches Android phones and delimited to
the Newsfeed, including other post related features as
Newsfeed configurations and user’s system profiling.
Results

The most common metalinguistic signs found in Facebook’s
interface are texts used to explain each feature in the news
feed. The profile picture and the user name are reiterated
signs to associate posts with their creator. Features related to
post creation are supported with text to specify creation of
text content, and text & icons for types of contents and
moods.
Metalinguistic signs for reading the news feed are related to
features such as “Like” and other mood signs, and the option
to comment on or share a specific post. There are both static
and dynamic signs for these functions.
There was a distinct lack of signs for scrolling the news feed,
for downwards (to read more) or for scrolling up (to refresh).
Some other opportunities that were only weakly

Page 3

CHI 2018 Honourable Mention

communicated included signaling when a post had been
created, and the navigation to an explanation as to why a
suggested or sponsored post was shown, and to tools that
allowed the user to un-follow a post.
While the app offered some ways to manipulate the news
feed, these were hard to find as shown in Figure 1. Other hard
to find options included the management of which friends are
shown first, who the user wants to un-follow, the opportunity
to reconnect to those previously un-followed, and
recommended pages to follow.

CHI 2018, April 21–26, 2018, Montréal, QC, Canada

and through various FB groups for students such as student
unions and student housing organizations. Participants
demonstrated a strong interest in discussing issues related to
AX and the algorithmic influence over Facebook. Due to the
lack of knowledge that users in general exhibit related to
experience-worthy algorithms (as delimited in the
background section above), the workshop started with a
priming tutorial on how algorithms are used in several
common apps with a focus on the Facebook case.
After this priming, the participants were invited to discuss
their perception of algorithmic influence. The discussion was
guided by a semi-structured set of questions developed from
Bucher’s and Rader and Gray’s categorization for
algorithmic experiences in Facebook [7,23] as well as
Diakopoulos invitation for algorithmic accountability [8],
but gave room for users to volunteer additional perspectives
and experiences.
In total, eleven (11) participants attended three sessions for
the first sequence of workshops. The first session had five
(5) participants; the second four (4) and the third one had two
(2) participants.
Results

The sensitizing workshop confirmed most of the experiences
reported by Bucher [7]. Almost all the participants had felt
that Facebook had on occasion classified them in ways that
they were not comfortable with.
Another common experience was that specific contacts were
not visible in the news feed, and many of the participant felt
that the app was making them lose contact with friends and
even family. One participant suggested that the interface
could display a low-interaction friend list, to promote
awareness and encourage more interaction with such friends.

Figure 1. Ad preferences configuration is possible to be found,
but only through several tabs.

To conclude, the SIM uncovers a main goal of the news feed
interface: to highlight selected posts and promote
commenting and sharing, as well as to promote the creation
of new posts. Understanding and managing the feed is
demoted and only partially possible. For example, there is no
way to determine if a post was presented in the news feed
through human involvement or due to an algorithmic
decision. Furthermore, there is no way to understand the
inner workings of the news feed algorithm, or when or how
any personalization is being made.
Sensitizing Workshops

The first sequence of workshops focused on collecting user’s
opinions on the current algorithmic experience of
Facebook’s news feed. The participants were all active
Facebook users, recruited through email and Facebook’s
messenger (within and outside the recruiter’s list of friends),

Paper 286

‘Whoa moments’ [7:35] were mainly connected to the
experience that Facebook was using information from other
sources such as browser activity or third party brands. This
was reported as surprising, scary and annoying. Participants
expressed a desire to know if these suspicions were accurate.
One user expressed strong negative feelings towards the
Facebook algorithm due to the way the algorithm bases its
recommendations equally on every action. He argued that
just because he clicked somewhere, this does not mean that
he wants that kind of content in the future.
Faulty predictions had also been noted: for example,
participants had been annoyed by faulty friend suggestions
and or faulty places predictions where they had not visited.
Closely related, they felt annoyed when the system reminded
them of deceased friends and would like a function to stop
such reminders.
To summarize, the expressed feelings towards Facebook’s
algorithmic behavior were ambivalent.

Page 4

CHI 2018 Honourable Mention

CHI 2018, April 21–26, 2018, Montréal, QC, Canada

Figure 3. A suggested redesign to improve algorithmic
awareness. Suggested re-design used in the second workshop.

This strategy made it possible to evaluate the feedback from
participants in relation to previous comments and
suggestions. Former participants were treated confidentially
and presented just as “…a previous user suggested …” to
maintain their privacy.

Figure 2. The ad preferences could be made available from the
profile page. Suggested re-design used in the second workshop.

Some participants considered the algorithmic influence
needed as they otherwise would experience information
overflow, and they also found it largely successful in
providing the expected and desired information. Others felt
that they would prefer to receive every post without any kind
of filtering. Concerning Facebook’s use of profiling for
commercial purposes some participants argued that since this
is a freely available service, Facebook needs to track and
make inferences about its users to be able to make profit.
Others rejected this reasoning and feared the results of such
tracking.

Figure 3 shows one of the proposed designs related to the
news feed structure. This suggested redesign explains why
the post appears in the newsfeed, with the purpose of
increasing user awareness of selection criteria. Figure 4
presents another suggestion, aiming to address the need to be
able to stop specific memories from re-appearing in the news
feed. Both suggestions were appreciated by workshop
participants.

Redesign workshop

In addition to volunteering their experiences, the workshop
participants had also provided suggestions for how the AX
of Facebook could potentially be improved. Before the
second round of workshops, we designed a set of potential
interventions in the Facebook news feed interface as
suggestions towards of improving its algorithmic experience.
In this step, eight (8) of the participants from the first
workshop were individually consulted and asked for their
opinions and feedback on the suggested designs. The
workshop setup was incremental, so that previous opinions
and suggestions from participants were incorporated into the
material before consulting the next person for feedback.
Figure 4. A suggested redesign to make the system forget
previous memories.

Paper 286

Page 5

CHI 2018 Honourable Mention

The results from the SIM inspection, and the opinions and
suggestions from the two workshops were merged into a list
of design considerations that subsequently were clustered to
form the following framework.
A FRAMEWORK FOR ALGORITHMIC EXPERIENCE

The design opportunities elicited to improve AX in
Facebook’s news feed can be clustered into five groups of
features. Figure 5 presents a graphical representation of the
five groups of features and contexts.
Algorithmic profiling transparency

This aspect relates to how the system makes visible what the
algorithm knows about a user and explains why the
algorithm present results based on that profiling. Making
profiling transparent could be considered to improve the
algorithmic experience. Ideally it should also be easily
accessible, and the transparency related to the filtering,
trending or profiling of results that is produced by the
algorithm.
Transparency can further be divided into the dimension of
internal versus external sources. Internal sources are those
mechanisms inside the social media system itself that
influence user profiling. External sources may include e.g.
Google search information, cookies stored by other services,

CHI 2018, April 21–26, 2018, Montréal, QC, Canada

or adjacent tabs open in the web browser. Facebook’s
browser tracking [28] is an example of this external profiling.
In both workshops, participants considered algorithmic
profiling transparency important. They emphasized the need
to show why particular posts appeared in the news feed.
Some filtering effects were discussed in more depth. During
the second workshop, participants expressed a wish to
understand which friends had little or no influence in the
algorithmic results for the news feed, so that they could make
a conscious choice about whether to remove them from the
friends list or actively increase their interaction with such
friends.
Algorithmic profiling management

This aspect relates to how users can corroborate and manage
the profiling made by the algorithm. It can improve the
algorithmic experience in social media services through
making users feel empowered and capable of managing what
the system thinks about them and how this affects
algorithmic interventions.
Throughout the workshops, it was the user’s profile page that
was indicated as the best context for profile management.
Participants tended to regard profiling not as Facebook’s

Figure 5. Design areas for Algorithmic Experience in Social Media.

Paper 286

Page 6

CHI 2018 Honourable Mention

property but as their own. Users considered algorithmic
profiling as a representation of them, and desired it to be
tuned, modified or adjusted as a “public” aspect of their
identity.
Algorithmic user-control

This aspect of AX is related to the capabilities that the user
is given towards directly controlling the algorithm. We
identified five specific ways in which users found it
meaningful to exert control over the algorithm as such.
Firstly, there is an option to sometimes turn off the
algorithmic influence, e.g. over the news feed. Facebook
offers such an option for handling the news feed through the
“Top news” and “Most Recent” options, where “Most
Recent” represents a way to turn off the algorithmic
influence over the newsfeed.
While participants appreciated this function, they also
expressed a need for order and coherence. For example, after
reactivating the algorithm the users may expect see some
new posts at the top but maintaining the order of posts from
the previous presentation further down the news feed list.
This can be understood in terms of coherence, as the users
expect the algorithm to be selecting and curating posts from
the same pool of posts and with very similar criteria before
and after the algorithm was turned off and on.
A second option for algorithmic user-control is to let users
selectively turn off some or all of the data sources that are
influencing the profiling algorithm. Specifically, this
includes turning off and on information from sensors, such
as position tracking. Some such options currently exist in
Facebook, although not for all types of user data.
Finally, participants expressed a desire to be able to present
the algorithm with explicit negative feedback when a faulty
prediction was made.
Selective algorithmic memory

Relates to the user’s influence over which data the algorithm
bases its decisions on. Users may wish to exclude specific
data from the profiling mechanisms, both when gathered but
also in in retrospect, if the user’s situation has changed.
In the context of Facebook, workshop participants brought
this up as the need to be able to un-follow memories if the
system e.g. recommended shared memories with deceased
friends or an old relationship after a bad breakup. In this
context, selective algorithmic memory would require a way
to tell the system to forget specific memories or people.
Algorithmic awareness

Algorithmic awareness relates to the overall awareness that
users have of the algorithmic influence, how user profiling is
done, and what influence the user can have on its results.
While all of the aspects above contribute to a higher level of
algorithmic awareness, direct tips and recommendations on
how to understand the results of algorithmic influence may
also be desirable. Initial posts telling the users how the news
feed works, what kind of behavior influence the algorithms,

Paper 286

CHI 2018, April 21–26, 2018, Montréal, QC, Canada

and what information is being tracked could be steps towards
encouraging users to acquire a more thorough understanding
of the function.
A particular aspect of this general awareness is also desirable
to communicate clearly when there is human intervention to
the functions that users have come to consider as
algorithmically controlled. The workshops brought about an
interesting discussion of algorithmic spaces; graphically
delimited spaces of the service interface in which the user
expects the algorithm to present its results in terms of
filtering, personalizing or trending. For these spaces, there is
an implicit contract between the users and the service
providers: only algorithms curate results in this space. In
such spaces, the workshop participants considered important
that human intervention or content moderation [25] would be
clearly marked, making it clear if post was eliminated or
added due to a human decision. Therefore, we can add the
need to signal when an algorithm has been changed due to
human intervention.
CONCLUSION

This article has proposed the concept of algorithmic
experience (AX) as a way to conceptualize the ways in which
users experience systems and interfaces that are heavily
influenced by algorithmic behavior. Building on Bucher [7]
we can note that in current social media, AX is largely
negative, as the algorithmic influence is foregrounded only
when it behaves erroneously or unpredictably. Hence, we
suggest that AX can be deliberately designed to foreground
algorithmic behavior and increase user awareness of
algorithmic influence.
It is important to realize that AX and the socioeconomic
dynamics of Facebook’s business are only partly aligned
[10,27]. While there may be economic and commercial
reasons behind the current algorithmic obscureness towards
the user, we have here chosen to frame AX from a usercentered perspective. From a commercial perspective, AX
may contribute towards a more joyful and faithful
relationship with a service, avoiding the possible bad
experiences [7,9]. By placing the user in focus, HCI can
serve to support an important debate related to the political
and legal issues involved around algorithms, privacy, and
users’ rights.
During the workshops carried out within this project, users
consistently expressed that the explicit awareness of and
discussion of AX gave them conceptual tools to analyze the
technologies they use and to understand how they work.
Participants were not previously aware of how their behavior
was tracked, and the ways this influenced the Facebook
service. After getting familiar with the concept of
algorithmic experience, participants reported that they
became more aware of their use of digital platforms and also
changed their way of using Facebook in order to improve its
service in accordance to their interests. The awareness of AX
thus works as a “new lens” for users, for understanding and
using digital technologies.

Page 7

CHI 2018 Honourable Mention

The concept also can become useful in design. In this article,
we suggest five particular ways in which the design of social
media can cater for increasing algorithmic awareness and
improving AX.
Algorithmic profiling transparency provides users ways to
understand what the system knows about them and how the
offered results are related to that profiling. In this it is
important to display both internal profiling, the implicit
profiling mechanism based on the user’s behavior within the
system, and to make transparent what the system is gathering
from external sources such as browser tracking or third
party/allied companies’ services.
Algorithmic profiling management allows users to refine the
profile. Explicit profile management helps users correct
profiling behavior, and is an important resource also for
designers as it allows for implicit and explicit feedback to
personalization [5:213].
Algorithmic user-control could empower users to give them
a level of control over their social media services. It includes
turning off and on algorithmic interventions as well as
tracking.
Selective algorithmic memory offers the user a possibility to
make the system forget their previous interactions and
delimit their influence over the future algorithmic results.
Finally, Algorithmic awareness can be directly fostered
through informing users about how the algorithms work and
how the user’s behavior affects its behavior. In this lies a
didactical challenge to present the user with sufficient
information, but at the same time not expose the social media
platform to user behavior that can compromise its function
or commercial viability.
FUTURE RESEARCH

The present research opens several questions to be elaborated
in future research. It would be valuable to apply similar
strategies to analyze other social media platforms, to
understand to what extent the same issues apply and how the
suggested design strategies would affect their algorithmic
experience. Furthermore, it is possible and needed to develop
methods to assess AX.
Referring to algorithms, Schou and Farkas invite to think
about “how to make visible that which is invisible by
design”[26:44]. As algorithms become an integral part of
most everyday services, we must look for ways to develop
AX in areas outside of social media. Some examples of
algorithms worthy of experience include map services and
services that mix human and algorithmic intervention such
as Amazon Mechanical Turk, Über, or Airbnb. It is critical
to develop AX perspectives in relation to health and self-care
systems, where algorithms may have a direct influence over
health and well-being of humans, and in the domain of
Human-Robot Interaction where the focus on
anthropomorphism may interfere with the algorithmic
transparency.

Paper 286

CHI 2018, April 21–26, 2018, Montréal, QC, Canada

ACKNOWLEDGMENTS

The authors are indebted to Else Nygren and Paulina
Rajkowska for feedback on earlier versions of the text. This
research made possible thanks to the economic support of the
Sciences, Technology and Telecommunications Ministry of
Costa Rica (Ministerio de Ciencias, Tecnología y
Telecomunicaciones de Costa Rica) and the University of
Costa Rica (Universidad de Costa Rica).
REFERENCES

1.

Pablo Barberá, John T Jost, Jonathan Nagler, Joshua A
Tucker, and Richard Bonneau. 2015. Tweeting From
Left to Right: Is Online Political Communication More
Than an Echo Chamber? Psychological science 26, 10:
1531–42. https://doi.org/10.1177/0956797615594620

2.

David Beer. 2017. The social power of algorithms.
Information, Communication & Society 20, 1: 1–13.
https://doi.org/10.1080/1369118X.2016.1216147

3.

Alessandro Bessi. 2016. Personality traits and echo
chambers on facebook. Computers in Human Behavior
65: 319–324. https://doi.org/10.1016/j.chb.2016.08.016

4.

Andrei Boutyline and Robb Willer. 2016. The Social
Structure of Political Echo Chambers: Variation in
Ideological Homophily in Online Networks. Political
Psychology xx, xx. https://doi.org/10.1111/pops.12337

5.

Engin Bozdag. 2013. Bias in algorithmic filtering and
personalization. Ethics and Information Technology
15, 3: 209–227. https://doi.org/10.1007/s10676-0139321-6

6.

Engin Bozdag and Jeroen van den Hoven. 2015.
Breaking the filter bubble: democracy and design.
Ethics and Information Technology 17, 4: 249–265.
https://doi.org/10.1007/s10676-015-9380-y

7.

Taina Bucher. 2016. The algorithmic imaginary:
exploring the ordinary affects of Facebook algorithms.
Information, Communication & Society 4462, April:
30–44.
https://doi.org/10.1080/1369118X.2016.1154086

8.

Nicholas Diakopoulos. 2016. Accountability in
algorithmic decision making. Communications of the
ACM 59, 2: 56–62. https://doi.org/10.1145/2844110

9.

Berkeley J Dietvorst, Joseph P Simmons, and Cade
Massey. 2015. Algorithm aversion: People erroneously
avoid algorithms after seeing them err. Journal of
Experimental Psychology: General 144, 1: 114–126.
https://doi.org/10.1037/xge0000033

10. Jose van Dijck. 2013. The Culture of Connectivity.
Oxford University Press. Retrieved December 29, 2017
from https://global.oup.com/academic/product/theculture-of-connectivity9780199970780?cc=us&lang=en&#
11. P. Dourish. 2016. Algorithms and their others:
Algorithmic culture in context. Big Data & Society 3,
2: 1–11. https://doi.org/10.1177/2053951716665128

Page 8

CHI 2018 Honourable Mention

12. Motahhare Eslami, Aimee Rickman, Kristen Vaccaro,
Amirhossein Aleyasen, Andy Vuong, Karrie
Karahalios, Kevin Hamilton, and Christian Sandvig.
2015. “I always assumed that I wasn’t really that close
to [her].” Proceedings of the 33rd Annual ACM
Conference on Human Factors in Computing Systems CHI ’15, APRIL: 153–162.
https://doi.org/10.1145/2702123.2702556
13. R. Stuart Geiger. 2014. Bots, bespoke, code and the
materiality of software platforms. Information,
Communication & Society 17, 3: 342–356.
https://doi.org/10.1080/1369118X.2013.873069
14. Tarleton Gillespie. 2013. The relevance of algorithms.
Media Technologies: Essays on Communication,
Materiality, and Society, Light 1999: 167–194.
https://doi.org/10.7551/mitpress/9780262525374.003.0
009
15. Tarleton Gillespie. 2016. #Trendingistrending: When
Algorithms Become Culture. Algorithmic Cultures:
Essays on Meaning, Performance and New
Technologies 189: 1–23.
16. Kevin Hamilton, Karrie Karahalios, Christian Sandvig,
and Motahhare Eslami. 2014. A path to understanding
the effects of algorithm awareness. Proceedings of the
extended abstracts of the 32nd annual ACM conference
on Human factors in computing systems - CHI EA ’14:
631–642. https://doi.org/10.1145/2559206.2578883
17. Kristina Höök, Jussi Karlgren, Annika Waern, Nils
Dahlbäck, Carl-Gustaf Jansson, Klas Karlgren, and
Benoit Lemaire. 1998. A Glass Box Approach to
Adaptive Hypermedia. In Adaptive hypertext and
hypermedia. Kluwer Academic Publishers, 143–170.
18. L. D. Introna. 2015. Algorithms, Governance, and
Governmentality: On Governing Academic Writing.
Science, Technology & Human Values 41, 1.
https://doi.org/10.1177/0162243915587360
19. Q. Vera Liao and Wai-Tat Fu. 2014. Can you hear me
now? Proceedings of the 17th ACM conference on
Computer supported cooperative work & social
computing - CSCW ’14: 184–196.
https://doi.org/10.1145/2531602.2531711
20. Qv Liao and Wt Fu. 2013. Beyond the filter bubble:
interactive effects of perceived threat and topic
involvement on selective exposure to information.
Proceedings of the SIGCHI Conference on Human
Factors in Computing Systems: 2359–2368.
https://doi.org/10.1145/2470654.2481326

CHI 2018, April 21–26, 2018, Montréal, QC, Canada

22. Sayooran Nagulendra and Julita Vassileva. 2014.
Understanding and controlling the filter bubble through
interactive visualization: A user study Understanding
and Controlling the Filter Bubble through Interactive
Visualization : A User Study. 107–115.
https://doi.org/10.1145/2631775.2631811
23. Emilee Rader and Rebecca Gray. 2015. Understanding
User Beliefs About Algorithmic Curation in the
Facebook News Feed. Proceedings of the 33rd Annual
ACM Conference on Human Factors in Computing
Systems - CHI ’15: 173–182.
https://doi.org/10.1145/2702123.2702174
24. Paul Resnick, R. Kelly Garrett, Travis Kriplean, Sean
a. Munson, and Natalie Jomini Stroud. 2013. Bursting
your (filter) bubble. Proceedings of the 2013
conference on Computer supported cooperative work
companion - CSCW ’13: 95.
https://doi.org/10.1145/2441955.2441981
25. Sarah T Roberts. 2016. Commercial content
moderation: Digital laborers’ dirty work. The
intersectional internet: Race, sex, class and culture
online: 147–160. https://doi.org/10.1007/s13398-0140173-7.2
26. Jannick Schou and Johan Farkas. 2016. Algorithms,
interfaces, and the circulation of information:
Interrogating the epistemological challenges of
Facebook. Kome 4, 1: 36–49.
https://doi.org/10.17646/KOME.2016.13
27. Beverley Skeggs and Simon Yuill. 2016. The
methodology of a multi-model project examining how
facebook infrastructures social relations. Information
Communication and Society 19, 10: 1356–1372.
https://doi.org/10.1080/1369118X.2015.1091026
28. Beverley Skeggs and Simon Yuill. 2016. Capital
experimentation with person/a formation: how
Facebook’s monetization refigures the relationship
between property, personhood and protest. Information
Communication and Society 19, 3: 380–396.
https://doi.org/10.1080/1369118X.2015.1111403
29. Clarisse Sieckenius De Souza and Carla Faria Leitão.
2009. Semiotic Engineering Methods for Scientific
Research in HCI.
https://doi.org/10.2200/S00173ED1V01Y200901HCI0
02
30. Michele Willson. 2017. Algorithms (and the) everyday.
Information, Communication & Society 20, 1: 137–
150. https://doi.org/10.1080/1369118X.2016.1200645

21. Sean A. Munson and Paul Resnick. 2010. Presenting
diverse political opinions: how and how much. Proc.
CHI 2010: 1457–1466.
https://doi.org/10.1145/1753326.1753543

Paper 286

Page 9

