Think About the Stakeholders First! Towards an Algorithmic
Transparency Playbook for Regulatory Compliance
Andrew Bell

New York University
New York, United States
alb9742@nyu.edu

Oded Nov

New York University
New York, United States
onov@nyu.edu

arXiv:2207.01482v1 [cs.CY] 10 Jun 2022

ABSTRACT
Increasingly, laws are being proposed and passed by governments
around the world to regulate Artificial Intelligence (AI) systems
implemented into the public and private sectors. Many of these
regulations address the transparency of AI systems, and related
citizen-aware issues like allowing individuals to have the right
to an explanation about how an AI system makes a decision that
impacts them. Yet, almost all AI governance documents to date
have a significant drawback: they have focused on what to do (or
what not to do) with respect to making AI systems transparent, but
have left the brunt of the work to technologists to figure out how
to build transparent systems. We fill this gap by proposing a novel
stakeholder-first approach that assists technologists in designing
transparent, regulatory compliant systems. We also describe a realworld case-study that illustrates how this approach can be used in
practice.

CCS CONCEPTS
• Human-centered computing → Interaction design theory,
concepts and paradigms.

KEYWORDS
human-centered computing, transparency, explainability, regulation, regulatory compliance

1

INTRODUCTION

In the past decade, there has been widespread proliferation of artificial intelligence (AI) systems into the private and public sectors.
These systems have been implemented in a broad range of contexts,
including employment, healthcare, lending, criminal justice, and
more. The rapid development and implementation of AI technologies has greatly outpaced public oversight, creating a “wild-west”style regulatory environment. As policy makers struggle to catch
up, the issues of unregulated AI have become glaringly obvious,
especially for underprivileged and marginalized communities. Famously, ProPublica revealed that the AI-driven system COMPAS
used to assess the likelihood of a prisoner recidivating was highly
discriminatory against black individuals [5]. In another example,
Amazon built and implemented an automated resume screening
and hiring AI system–only to later find out that the system was
biased against hiring women [53]. In an effort to address these
issues, countries around the world have begun regulating the use of
AI systems. Over 50 nations and intergovernmental organizations
have published AI strategies, actions plans, policy papers or directives [72]. A survey of existing and proposed regulation around AI
transparency is given in Section 2.

Julia Stoyanovich

New York University
New York, United States
stoyanovich@nyu.edu

Unfortunately, most strategies, directives and laws to date lack
specificity on how AI regulation should be carried out in practice by technologists. Where there is specificity, there is a lack of
mechanisms for enforcing laws and holding institutions using AI
accountable. Documents on AI governance have focused on what
to do (or what not to do) with respect to AI, but leave the brunt
of the work to practitioners to figure out how things should be
done [32]. This tension plays out heavily in regulations governing the transparency of AI systems (called “explainability” by AI
practitioners). The most prominent example of this is the “right
to explanation” of data use that is included in the EU’s General
Data Protection Regulation (GDPR). Despite being passed into law
in 2016, the meaning and scope of the right is still being debated
by legal scholars, with little of the discussion resulting in concrete
benefits for citizens [64].
While regulation can help weigh the benefits of new technology
against the risks, developing effective regulation is difficult, as is establishing effective mechanisms to comply with existing regulation.
This paper aims to fill a gap in the existing literature by writing to
technologists and AI practitioners about the existing AI regulatory
landscape, and speaks to their role in designing complaint systems.
We make a case for why AI practitioners should be leading efforts
to ensure the transparency of AI systems, and to this end, we propose a novel framework for implementing regulatory-compliant
explanations for stakeholders. We also consider an instantiation
of our stakeholder-first approach in the context of a real-world
example using work done by a national employment agency.
We make the following three contributions: (1) provide a survey of existing and proposed regulations on the transparency and
explainability of AI systems; (2) propose a novel framework for a
stakeholder-first approach to designing transparent AI systems; and
(3) present a case-study that illustrates how this stakeholder-first
approach could be used in practice.

2

EXISTING AND EMERGING REGULATIONS

In recent years, countries around the world have increasingly been
drafting strategies, action plans, and policy directives to govern the
use of AI systems. To some extent, regulatory approaches vary by
country and region. For example, policy strategies in the US and
the EU reflect their respective strengths: free-market ideas for the
former, and citizen voice for the latter [23]. Yet, despite countrylevel variation, many AI policies contain similar themes and ideas.
A meta-analysis of over 80 AI ethics guidelines and soft-laws found
that 87% mention transparency, and include an effort to increase
the explainability of AI systems [32]. Unfortunately, all documents
to date have one major limitation: they are filled with uncertainty

Andrew Bell, Oded Nov, and Julia Stoyanovich

on how transparency and explainability should actually be implemented in a way that is compliant with the evolving regulatory
landscape [22, 32, 32, 38]. This limitation has 3 main causes: (1)
it is difficult to design transparency regulations that can easily
be standardized across different fields of AI, such as self-driving
cars, robotics, and predictive modeling [75]; (2) when it comes to
transparency, there is a strong information asymmetry between
technologists and policymakers, and, ultimately, the individuals
who are impacted by AI systems [35]; (3) there is no normative
consensus around AI transparency, and most policy debates are
focused on the risks of AI rather than the opportunities [22]. For the
purposes of scope, we will focus on regulations in the United States
and Europe. However, its important noting that there is meaningful
AI regulation emerging in Latin and South America, Asia, Africa,
and beyond, and summarizing those regulations is an avenue for
future work. For example, in 2021, Chile presented it’s first national
action plan on AI policy 1 .

2.1

United States

In 2019 the US took two major steps in the direction of AI regulation. First, Executive Order 13859 was issued with the purpose of
establishing federal principles for AI systems, and to promote AI
research, economic competitiveness, and national security. Importantly, the order mandates that AI algorithms implemented for use
by public bodies must be “understandable”, “transparent”, “responsible”, and “accountable.” Second, the Algorithmic Accountability Act
of 2019 was introduced to the House of Representatives, and more
recently reintroduced under the name Algorithmic Accountability
Act of 2022. If passed into law, the Algorithmic Accountability Act
would be a landmark legalisation for AI regulation in the US. The
purpose of the bill is to create transparency and prevent disparate
outcomes for AI systems, and it would require companies to assess
the impacts of the AI systems they use and sell. The bill describes
the impact assessment in detail — which must be submitted to an
oversight committee— and states that the assessment must address
“the transparency and explainability of [an AI system] and the degree to which a consumer may contest, correct, or appeal a decision
or opt out of such system or process”, which speaks directly to what
AI practitioners refer to as “recourse”, or the ability of an individual
to understand the outcome of an AI system and what they could
do to change that outcome [73, 76].
In 2019 the OPEN Government Data Act was passed into law,
requiring that federal agencies maintain and publish their information online as open data. The data also must be cataloged on
Data.gov, a public data repository created by the the US government. While this law only applies to public data, it demonstrates
how policy can address transparency within the whole pipeline of
an AI system, from the data to the algorithm to the system outcome.
There are also some industry-specific standards for transparency
that could act as a model for future cross-industry regulations. Under the Equal Credit Opportunity Act, creditors who deny loan
applicants must provide a specific reason for the denial. This includes denials made by AI systems. The explanations for a denial
1 https://www.gob.cl/en/news/chile-presents-first-national-policy-artificial-

intelligence/

come from a standardized list of numeric reason codes, such as:
“U4: Too many recently opened accounts with balances2 .”

2.2

European Union

In 2019 the EU published a white paper titled “Ethics Guidelines
for Trustworthy AI,” containing a legal framework that outlines
ethical principles and legal obligations for EU member states to
follow when deploying AI3 . While the white paper is non-binding,
it lays out expectations on how member-states should regulate the
transparency of AI systems: “... data, system and AI business models
should be transparent. Traceability mechanisms can help achieving
this. Moreover, AI systems and their decisions should be explained
in a manner adapted to the stakeholder concerned. Humans need
to be aware that they are interacting with an AI system, and must
be informed of the system’s capabilities and limitations.”
Currently, the European Commission is reviewing the Artificial
Intelligence Act4 , which would create a common legal framework
for governing all types of AI used in all non-military sectors in
Europe. The directive takes the position that AI systems pose a significant risk to the health, safety and fundamental rights of persons,
and governs from that perspective. With respect to transparency,
the directive delineates between non-high-risk and high-risk AI systems (neither of which are rigorously defined at this time). It states
that for “non-high-risk AI systems, only very limited transparency
obligations are imposed, for example in terms of the provision
of information to flag the use of an AI system when interacting
with humans.” Yet, for high-risk systems, “the requirements of high
quality data, documentation and traceability, transparency, human
oversight, accuracy and robustness, are strictly necessary to mitigate the risks to fundamental rights and safety posed by AI and
that are not covered by other existing legal frameworks.” Notably,
as in the Algorithmic Accountability Act in the United States, the
document contains explicit text mentioning recourse (referred to
as “redress”) for persons affected by AI systems.
The EU has also passed Regulation (EU) 2019/1150 that sets
guidelines for the transparency of rankings for online search.5 In
practice, this means that online stores and search engines should
be required to disclose the algorithmic parameters used to rank
goods and services on their site. The regulation also states that
explanations about rankings should contain redress mechanisms
for individuals and businesses affected by the rankings.
2.2.1 Right to Explanation. The Right to Explanation is a proposed
fundamental human right that would guarantee individuals access
to an explanation for any AI system decision that affects them.
The Right to Explanation was written into the EU’s 2016 GDPR
regulations, and reads as follows: “[the data subject should have]
the right ... to obtain an explanation of the decision reached.”6 The
legal meaning and obligation of the text has been debated heavily by
legal scholars, who are unsure under which circumstances it applies,
what constitutes an explanation [64], and how the right is applicable
2 https://www.fico.com/en/latest-thinking/solution-sheet/us-fico-score-reason-

codes
3 https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthyai
4 https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206
5 https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A32019R1150
6 https://www.privacy-regulation.eu/en/r71.htm

Think About the Stakeholders First! Towards an Algorithmic Transparency Playbook for Regulatory Compliance

to different AI systems [19]. The Right to Explanation is an example
of how emerging AI technologies may “reveal” additional rights
that need to be considered by lawmakers and legal experts [52].
The EU’s recently proposed Artificial Intelligence Act simultaneously reinforces the idea that explanations about AI systems are
a human right, while slightly rolling back the Right to Explanation
by acknowledging that there are both non-high-risk and high-risk
AI systems. Discussions about the Right are likely to continue, and
will be a central part of debates on regulating AI transparency. In
fact, some local governing bodies have already taken steps to adopt
the Right to Explanation. France passed the Digital Republic Act in
2016, which gives the Right to Explanation for individuals affected
by an AI system in the public sector [20]. Hungary also has a similar
law [42].

2.3

Local

There has been significant movement on the regulation of specific
forms of AI systems at local levels of government. In response to
the well-documented biases of facial recognition software when
identifying people of different races and ethnicities [11], Washington State signed Senate Bill 6820 into law in 2020, which prohibits
the use of facial recognition software in surveillance and limits its
use in criminal investigation.7 Detroit has also reacted to concerns
about facial recognition, and its City Council approved legislation
that mandates transparency and accountability for the procurement process of video and camera surveillance contracts used in
the city.8 The New York City Council recently regulated the use
of AI systems in relation to employment decisions (Local Law 144
of 2021).9 The bill requires that AI tools for hiring employees be
subject to yearly bias audits. An additional requirement is to notify
job seekers that they were screened by a tool, and to disclose to
them what “qualifications or characteristics” were used by the tool
as basis of decisions. Finally, in the Netherlands, the municipality
of Rotterdam has created a Data-Driven Working program which
has been critical of transparency surrounding the algorithms used
for fraud detection.10

3

THE ROLE OF TECHNOLOGISTS

The continuously evolving regulatory landscape of AI, combined
with the limitations of existing regulation in providing clarity on
how transparency should be implemented into AI systems, has
left open questions concerning responsibilities for AI design and
implementation. We argue that (1) practitioners should bear the bulk
of the responsibility for designing and implementing compliant,
transparent AI systems (2) it is in the best interest of practitioners
to bear this responsibility. Researchers have also shown that there
may be risks of only partially complying with AI regulations, and
that fusll compliance is the best way forward [15]. Technologists

include AI practitioners, researchers, designers, programmers, and
developers.
Practitioners have the right technical expertise. Transparency
has been a central topic of AI research for the past decade, and is
motivated beyond just regulatory compliance by ideas like making
systems more efficient, debugging systems, and giving decision making agency to the data subjects (i.e., those affected by AI-assisted
decisions) or to the users of AI systems (i.e., those making decisions
with the help of AI). New technologies in transparent AI are being
created at a fast pace, and there is no indication that the rapid innovation of explainable AI will slow any time soon [14, 14, 16, 41, 57],
meaning that of all the stakeholders involved in the socio-technical
environment of AI systems, technologists are the most likely to
be aware of available tools for creating transparent AI systems.
Furthermore, there are currently no objective measures for the
quality of transparency in AI systems [1, 25, 29, 40, 79], and so
technologists are necessary to discern the difference between a
“good explanation” and a “bad explanation” about a system.
Practitioners are the least-cost avoiders. This idea is based
on the principle of the least-cost avoider, which states that obligations and liabilities should be allocated entirely to the party with the
lowest cost of care [68]. AI practitioners are the least-cost avoiders
because they are already equipped with the technical know-how
for building and implementing transparency tools into AI systems,
especially when compared to policymakers and the individuals affected by the outcome of the system. Notably, given the wide range
of existing transparency tools, implementing the “bare minimum”
is trivially easy for most technologists.
One argument practitioners give against building transparent
systems is that they may be less accurate than highly complex,
black-box systems [30]. However, there has been a growing amount
of evidence suggesting that building transparent systems actually
results into little to no trade-off in the accuracy of AI systems [9,
17, 61, 67]. In other words: building transparent systems is not a
Pareto-reducing constraint for practitioners.
Practitioners already bear the responsibility for implementing transparency into AI systems. A study interviewing AI practitioners found that using AI responsibly in their work is viewed as
the practitioner’s burden, not the institutions for which they work.
Practitioners noted that existing structures within institutions are
often antithetical to the goals of responsible AI, and that it is up to
them to push for structural change within that institution [55]. Section 2 shows that AI regulation is converging on requiring transparent AI systems that offer meaningful explanations to stakeholders.
Therefore, it is in the best interest of practitioners to continue the
bottom-up approach of building transparent AI systems in the face
of looming regulations.

4
7 https://app.leg.wa.gov/billsummary?BillNumber=6280&Initiative=false&Year=2019
8 https://www.detroitnews.com/story/news/local/detroit-city/2021/05/25/detroit-

council-approves-ordinance-boost-transparency-surveillance-camera-contracts/
7433185002/
9 https://legistar.council.nyc.gov/LegislationDetail.aspx?ID=4344524&GUID=
B051915D-A9AC-451E-81F8-6596032FA3F9&Options=Advanced&Search
10 https://nos.nl/artikel/2376810-rekenkamer-rotterdam-risico-op-vooringenomenuitkomsten-door-gebruik-algoritmes

A STAKEHOLDER-FIRST APPROACH TO
DESIGNING TRANSPARENT ADS
4.1 Definitions
Technologists and AI researchers have not agreed on a definition of
transparency for AI systems. Instead, a number of terms have been
used, including explainability, interpretability, intelligibility, understandability, and comprehensibility [43]. There is no consensus

Andrew Bell, Oded Nov, and Julia Stoyanovich

on the meaning of these terms and they are often defined differently by different authors or used interchangeably. Furthermore,
transparency and its related terms cannot trivially be quantified or
measured, and transparency for one stakeholder does not automatically imply the same for different stakeholders [27, 37].
While having multiple definitions of transparency has been useful for distinguishing nuance in a research setting, it also poses
a challenge for policy making. In contrast to technologists, policymakers favor definitions of transparency that are about human
thought and behavior such as accountability or legibility [34]. Table 4.1 outlines terms related to transparency commonly used by
policymakers versus those used by technologists.
Transparency. For the purposes of this paper, we choose to
use only the term “transparency,” in the broadest possible sense, so
that it encompasses all the definitions above. This is most similar
to the way “explainability” is used by technologists. Here we use
the definition adapted from work by Christoph Molnar and define
transparency as “the degree to which a human can understand an
AI system [48].”
Explanation. We use the term “explanation” to refer to an instantiation of transparency. For example, to ensure transparency
for a system, a technologist may create an explanation about the
data it uses.

Terms used by policymakers

Terms used by technologists

Transparency
Accountability
Understandable
Legibility
Traceability
Redress

Explainability
Transparency
Interpretablity
Intellegibility
Understandability
Comprehensibility
Recourse

Table 1: Discrepancies in the way policymakers and AI practitioners communicate about the transparency of AI systems.

Automated Decision Systems. The approach described in this
paper applies to all Automated Decision Systems (ADS), which is
any system that processes data to make decisions about people.
This means that AI systems are a subset of ADS, but there are two
key distinctions: (1) an ADS is underpinned by any algorithm and
not just AI or machine learning, and (2) an ADS implies a context
of use and some kind of impact. For a formal definition of ADS,
see [69]. Henceforth, we will use the term ADS.
Notably, while many regulations are written to specifically mention “AI systems”, all the ideas they contain about transparency
could be applied to all ADS. It is likely that future regulations will
focus broadly on ADS, as seen in NYC Local Law 144 of 2021 and
France’s Digital Republic Act.

4.2

Running Example: Predicting
Unemployment in Portugal

To make the discussion concrete, we use a running example of an
ADS implemented in Portugal to try and prevent long-term unemployment (being unemployed for 12 months or more). The longterm unemployed are particularly vulnerable persons, and tend to
earn less once they find new jobs, have poorer health and have children with worse academic performance as compared to those who
had continuous employment [49]. The Portuguese national employment agency, the Institute for Employment and Vocational Training
(IEFP), uses an ADS to allocate unemployment resources to at-risk
unemployed persons. The system is based on demographic data
about the individual, including their age, unemployment length,
and profession, along with other data on macroeconomic trends in
Portugal.
The ADS is used by job counselors who work at the IEFP unemployment centers spread across Portugal. This interaction model,
where an ML system makes a prediction and a human ultimately
makes a final determination informed by the system’s predictions, is
referred to as having a “human-in-the-loop” (HITL). Having a HITL
is an increasingly common practice for implementing ADS [24, 56,
77]. The ADS assigns unemployed persons as low, medium, or high
risk for remaining unemployed, and then job counselors have the
responsibility of assigning them to interventions such as re-skilling,
resume building, or job search training [80].
This is a useful case study for three reasons: (1) people’s access
to economic opportunity is at stake, and as a result, systems for
predicting long-term unemployment are used widely around the
world [12, 39, 44, 59, 63, 70]; (2) the ADS exists in a dynamic setting which includes several stakeholders, like unemployed persons,
job counselors who act as the human-in-the-loop, policymakers
who oversee the implementation of the tool, and the technologists
who developed the tool; (3) lessons from this case about designing stakeholder-first transparent systems generalize well to other
real-world uses cases of ADS.

4.3

The Approach

There are many purposes, goals, use-cases and methods for the
transparency of ADS, which have been categorized in a number
of taxonomies and frameworks [6, 36, 43, 45, 48, 58, 60, 66, 74].
The approach we propose here has three subtle — yet important
— differences from much of the existing work in this area: (1) our
approach is stakeholder-first, capturing an emerging trend among
researchers in this field to reject existing method or use-case driven
approaches; (2) our approach is focused on improving the design of
transparent ADS, rather than attempting to categorize the entire
field of transparency; (3) our approach is aimed at designing ADS
that comply with transparency regulations.
Our approach can be seen in Figure 1 and is made up of the
following components: stakeholders, goals, purpose, and methods.
We describe each component in the remainder of this section, and
explain how they apply to the running example.
4.3.1 Stakeholders. Much of ADS transparency research is focused
on creating novel and innovative transparency methods for algorithms, and then later trying to understand how these methods

Think About the Stakeholders First! Towards an Algorithmic Transparency Playbook for Regulatory Compliance

Goal

Definition

Validity

Making sure that an ADS is constructed correctly and
is reasonable; encompasses ideas like making sure the
ADS is reliable and robust [18]
Knowing “how often an ADS is right” and “for which
examples it is right” [37]; influences the adoption of an
ADS [60]
Ensuring that an ADS is fair

Trust
Fairness

Privacy
Learning
Support
Recourse

and

Example

An practitioner may use a transparency method to debug an ADS; An auditor may gain intuition about how
an ADS is making decisions through transparency
A policymaker may use transparency to gain trust in
the ADS; an affected individual may find through transparency that they do not trust a particular ADS [62]
An auditor may use an explanation about an ADS to
make sure it is fair to all groups of individuals; a practitioner may use transparency tools to find bias in their
modeling pipeline
Ensuring that an ADS respects the data privacy of an An auditor individual may use an explanation of the
individual
data used in an ADS to evaluate privacy concerns
To satisfy human curiosity, or increase understanding A doctor may use an explanation to understand an ADS
about how an ADS is supporting a real-world recom- recommendation of a certain treatment
mendation [48, 60]
Allowing a stakeholder to take some action against the An individual may use an explanation to appeal a loan
outcome of an ADS [10, 60]
rejection; An individual may request to see an explanation of an ADS output to understand why it was
made

Table 2: Definitions and examples of stakeholder goals for the 6 categories of ADS transparency goals.

can be used to meet stakeholders needs [10, 54]. Counter to this
rationale, we propose a starting point that focuses on ADS stakeholders: assuming algorithmic transparency is intended to improve
the understanding of a human stakeholder, technologists designing
transparent ADS must first consider the stakeholders of the system,
before thinking about the system’s goals or the technical methods
for creating transparency.
The existing literature and taxonomies on ADS transparency
have identified a number of important stakeholders, which include technologists, policymakers, auditors, regulators, humansin-the-loop, and those individuals affected by the output of the
ADS [4, 45, 46]. While there is some overlap in how these stakeholders may think about transparency, in general, there is no single approach to designing transparent systems for these disparate
stakeholder groups, and each of them has their own goals and purposes for wanting to understand an ADS [66]. In fact, even within
a stakeholder group there may be variations on how they define
meaningful transparency [28]. Designers of ADS may also want
to weight the needs of separate stakeholders differently. For example, it may be more meaningful to meet the transparency needs of
affected individuals over AI managers or auditors.
Importantly, by staking transparency on the needs of stakeholders, technologists will be compliant with citizen-aware regulations
like the Right to Explanation, and those that require audits of ADS.
Running example. In the ADS used by IEFP in Portugal, there
are four main stakeholders: the technologists who developed the
ADS, the policymakers who reviewed the ADS and passed laws for
its implementation, the job counselors who use the system, and
the affected individuals who are assessed for long-term unemployment. In the development of the AI, explanations were created to
meet the varying goals of many of these stakeholders including
practitioners, policymakers, and the job counselors. Unfortunately,

and significantly, affected individuals were not considered. Had
the practitioners adopted a robust stakeholder-first approach to
designing transparent systems they could have better considered
how to meet the goals of this key stakeholder group. For example,
a person may want to appeal being predicted low risk because they
feel they are high risk for long-term unemployment and need access
to better interventions.
4.3.2 Goals. There has been little consensus in the literature on
how ADS goals should be classified. Some researchers have focused broadly, classifying the goals of ADS as evaluating, justifying, managing, improving, or learning about the outcome of an
ADS [45]. Others have defined goals more closely to what can be
accomplished by known transparency methods, including building
trust, establishing causality, and achieving reliability, fairness, and
privacy [43]. Amarasinghe et al. identified five main goals (designated as use-cases) of transparency specifically in a policy setting:
model-debugging, trust and adoption, whether or not to intervene,
improving intervention assignments, and for recourse. In this context, the term intervention refers to a policy action associated with
the outcome of an ADS.
Notably, the goals of transparency are distinct from the purpose.
The purpose addresses a context-specific aim of the ADS. For example, if an explanation is created for an ADS with the purpose
of explaining to an individual why their loan was rejected, the
goal may be to offer individual recourse against the rejection. This
distinction is made clear in 4.3.3.
For our stakeholder-first approach we make two changes to the
existing body of research work. First, we require that the goal of
transparent design must start with a stakeholder. Since all transparency elements of an ADS are intended for a human audience,
defining a stakeholder is implicit in defining goals. Second, we have

Andrew Bell, Oded Nov, and Julia Stoyanovich

Figure 1: A stakeholder-first approach for creating transparent ADS. The framework is made up of four components:
stakeholders, goals, purpose, and methods. We recommend
that transparency be thought of first by stakeholders, second by goals, before thirdly defining the purpose, and lastly
choosing an appropriate method to serve said purpose. Using the framework is simple: starting at the top, one should
consider each bubble in a component before moving onto
the next component.

established 6 goal categories, which encompass those found in literature. These categories are validity, trust, learning and support,
recourse, fairness and privacy, and are defined in Table 2 alongside
concrete examples of how these goals may be implemented.
An important discussion surrounding goals are the justifications
for pursuing them. For example, fairness and privacy goals may
be justified for humanitarian reasons (they are perceived by the
stakeholders as the “right thing to do”). Other justifications may
be to prevent harm, like offering recourse to stakeholders against
an outcome of an ADS, or for a reward, like an explanation that
supports a doctor’s correct diagnosis. For reasons of scope we will
not delve into the issue of goal justification in this paper.
Running example. In our case study, transparency is built into the
ADS with the goal of offering learning and support to job counselors.
The ADS generates explanations about what factors contribute to
an individual being classified as low, medium, or high risk for longterm unemployment, which job counselors use to help make better
treatment decision. Furthermore, the job counselor may also use
the explanation to offer recommendations for recourse against a
high risk score.

4.3.3 Purpose. Miller proposed that the purpose of transparency
is to answer a “why” question [47], and gives the following example: In the context where a system is predicting if a credit loan
is accepted or rejected, one may ask, “why was a particular loan
rejected?” Liao et al. expanded on this significantly by creating a
“question bank” which is a mapping from a taxonomy of technical transparency methodology to different types of user questions.
Instead of just answering why questions, the works shows that
transparency can be used to answer 10 categories of questions:
questions about the input, output, and performance of the system,
how, why, why not, what if, how to be that, how to still be this,
and others [36]. These questions have two important characteristics. First, they are context-specific and should address a direct
transparency goal of the stakeholder. Second, and importantly for
technologists, these questions can be mapped onto known methods
for creating explanations, meaning that a well-defined purpose for
transparency acts a bridge between the goals and methods.
Thoughtfully defining the goals and purpose of transparency in
ADS is critical for technologists to be compliant with regulators. It
is not sufficient to try and apply general, one-size-fits-all design
like simply showing the features that were used by an ADS. For
instance, both the proposed Algorithmic Accountability Act in the
United States and the Artificial Intelligence Act in the European
Union specifically mention that ADS should have transparency
mechanisms that allow individuals to have recourse against a system outcome. Researchers have noted that feature-highlighting
transparency lacks utility when there is a disconnect between the
explanation and real-world actions [7]. For instance, if someone is
rejected for a loan and the reason for that decision is the person’s
age, there is no action that they can effectively take for recourse
against that decision.
Running example. In the long-term unemployment use case, there
were two main purposes of transparency: to understand why an
individual was assigned to a particular risk category, and to understand what could be done to help high risk individuals lower their
chances of remaining long-term unemployed.
4.3.4 Methods. Once the stakeholders, goals, and purposes for algorithmic transparency have been established, it is time for the technologist to pick the appropriate transparency method (somtimes
called explainablity method). Over the past decade there has been
significant work in transparent ADS research (sometimes called
“explainable AI” research or XAI) on developing new methods for
understanding opaque ADS. There are several existing taxonomies
of these methods, which show that explanations can be classified
on a number of attributes like the scope (local or global), intrinsic or post-hoc, data or model, model-agnostic or model-specific,
surrogate or model behavior, and static or interactive [6, 43, 48].
Furthermore, researchers have created a number of different tools
to accomplish transparency in ADS [14, 14, 16, 41, 57].
In contrast to the complex classification of transparency methods
by technologists, regulations have focused on two elements of ADS:
(1) what aspect of the ADS pipeline is being explained (the data,
algorithm, or outcome)?, and (2) what is the scope of the explanation (for one individual or the entire system)? Table 3 shows how
different regulations speak to different combinations of pipeline
and scope. In our stakeholder first-approach to transparency, we

Think About the Stakeholders First! Towards an Algorithmic Transparency Playbook for Regulatory Compliance

focus on these two main attributes. We will not discuss specific
methods in detail, but for the convenience of technologists we have
underlined them throughout this discussion.
Data, algorithm, or outcome. Transparency methods have focused on generating explanations for three different “points in time”
in an ADS pipeline: the data (pre-processing), the model/algorithm
(in-processing, intrinsic), or the outcome (post-processing, posthoc) [6, 74]. Importantly, transparency is relevant for each part of
the machine learning pipeline because issues likes bias can arise
within each component [78].
Transparency techniques that focus on the pre-processing component of the pipeline, that is, on the data used to create an ADS,
typically include descriptive statistics or data visualizations.
Data visualizations have proved useful for informing users and
making complex information more accessible and digestible, and
have even been found to have a powerful persuasive effect [50, 71].
Therefore, it is advisable to use data visualization if it can easily address the purpose of an explanation. However, visualizations should
be deployed thoughtfully, as they have the ability to be abused and
can successfully misrepresent a message through techniques like
exaggeration or understatement [51].
Techniques for creating in-processing or post-processing explanations call into question the important consideration of using explainable versus black-box algorithms when designing AI.
The machine learning community accepts two classifications of
models: those that are intrinsically transparent by their nature
(sometimes called directly interpretable or white-box models), and
those that are not (called black box models) [43]. Interpretable
models, like linear regression, decision trees, or rules-based models, have intrinsic transparency mechanisms that offer algorithmic
transparency, like the linear formula, the tree diagram, and the set of
rules, respectively. There are also methods like select-regress-round
that simplify black-box models into interpretable models that use a
similar set of features [33].
As an important design consideration for technologists, researchers
have studied the effect of the complexity of a model and how it
impacts its ability to be understood by a stakeholder. A user study
found that the understanding of a machine learning model is negatively correlated with it’s complexity, and found decision trees to
be among the model types most understood by users [3]. An additional, lower-level design consideration is that model complexity is
not fixed to a particular model type, but rather to the way that the
model is constructed. For example, a decision tree with 1,000 nodes
will be understood far less well than a tree with only 3 or 5 nodes.
In contrast to in-process transparency, which is intrinsically
built into a model or algorithm, post-hoc transparency aims to
answer questions about a model or algorithm after is has already
been created. Some of the most popular post-hoc methods are
LIME, SHAP, SAGE, and QII [14, 16, 41, 57]. These methods are
considered model-agnostic because they can be used to create explanations for any model, from linear models to random forests
to neural networks. Some methods create a transparent surrogate
model that mimics the behavior of a black-box model. For example, LIME creates a linear regression to approximate an underlying
black-box model [41]. More work needs to be done in this direction,
but one promising study has shown that post-hoc explanations can

actually improve the perceived trust in the outcome of an algorithm [8].
However, post-hoc transparency methods have been shown to
have two weaknesses that technologists should be aware of: (1) in
many cases, these methods are at-best approximations of the blackbox they are trying to explain [81], and (2) these methods may
be vulnerable to adversarial attacks and exploitation [65]. Some
researchers have also called into question the utility of black-box
models and post-hoc explanation methods altogether, and have
cautioned against their use in real-world contexts like clinical settings [61].
Scope. There are two levels at which a transparent explanation
about an ADS can operate: it either explains its underlying algorithm fully, called a “global” explanation; or it explains how the
algorithm operates on one specific instance, called a “local” explanation. Molnar further subdivides each of these levels into two
sub-levels: global explanations can either be holistic (applying to
an entire algorithm, which includes all of its features, and in the
case of an ensemble algorithm, all of the component algorithms) or
modular, meaning they explain on part of the holistic explanation
and local explanations can either be applied to a single individual,
or aggregated to provide local explanations for an entire group [48].
The scope of an explanation is highly relevant to the stakeholder
and goals of an explanation, and is related to whether the stakeholder operates at a system or individual level. Researchers found
that the scope of explanation can influence whether or not an individual thinks a model is fair [31, 36]. Policymakers and ADS
compliance officers are more apt to be concerned with system level
goals, like ensuring that the ADS is fair, respects privacy, and is valid
overall, while humans-in-the-loop and those individuals affected
by the outcome of an ADS are likely more interested in seeing local
explanations to pertain to their specific cases. Technologists should
consider both.
Naturally, there is considerable overlap between stakeholders’
scope needs (for example, an auditor may want to inspect a model
globally and look at local cases), but generally, it is important which
scope an explanation has. Therefore designers of ADS explanations
should be thoughtful of how they select the scope of an explanation
based on a stakeholder and their goals.
Running-example. In the IEFP use case, SHAP factors were given
to job counselors to show the top factors influencing the score of
a candidate both positively and negatively [80]. The transparency
provided by SHAP provided a local explanation about the outcome
of the model. A bias audit was also conducted on the entire algorithm, and presented to policy officials within IEFP.
Overall, researchers found that the explanations improved the
confidence of the decisions, but counter-intuitively, had a somewhat
negative effect on the quality of those decisions [80].

4.4

Putting the Approach into Practice

The stakeholder-first approach describe in Section 4.3 is meant
to act as a guide for technologists creating regulatory-compliant
ADS. Putting this approach into practice is simple: starting at the
first component in Figure 1 (stakeholders), one should consider
each bubble, before moving onto the next component and again
considering each bubble. By the time one has finished worked

Andrew Bell, Oded Nov, and Julia Stoyanovich

Data

Local

Global

Algorithm

Outcome

Right to Explanation gives individuals the right to know how an
algorithm made a decision about
them
EU Regulation 2019/115 requires
OPEN Government Data Act (US) that online stores and search enmandates the government pub- gines to disclose the algorithmic
lishes public data
parameters used to rank goods
and services on their site
GDPR (EU) gives individuals the
right to request a copy of any of
their personal data

Both the proposed Algorithmic
Accountability Act (US) and Artificial Intelligence Act (AI) give
individuals the right to recourse
NYC Int 1894-2020 requires hiring
algorithms be audited for biased
outcomes

Table 3: How different laws regulate the aspects the ADS pipeline (the data, algorithm or outcome), and within what scope
(local or global).

their way through the figure, they should have considered all the
possible stakeholders, goals, purposes, and methods of an ADS. An
instantiation of the approach can be found throughout Section 4.3
in the running example of building an ADS that predicts the risk of
long-term unemployment in Portugal.
It’s important to note that our proposed stakeholder-first approach is only a high-level tool for thinking about ADS transparency through the perspective of stakeholders and their needs.
Beyond this approach, there are meaningful low-level steps that
can be taken by technologists when it comes to actually implement
transparency into ADS. One such step is the use of participatory
design, where stakeholders are included directly in design conversations [2, 13, 21, 26]. In one promising study researchers used
participatory design to successfully create better algorithmic explanations for users in the field of communal energy accounting [13].

5

CONCLUDING REMARKS

If there is to be a positive, ethical future for the use of AI systems, there needs to be stakeholder-driven design for creating
transparency algorithms — and who better to lead this effort than
technologists. Here we proposed a stakeholder-first approach that
technologists can use to guide their design of transparent AI systems that are compliant with existing and proposed AI regulations.
While there is still significant research that needs to be done in
understanding how the transparency of AI systems can be most
useful for stakeholders, and in the policy design of AI regulation,
this paper aims to be a step in the right direction.
There are several important research steps that could be taken
to extend this work. First, the stakeholder-first approach described
here lays the foundation for creating a complete playbook to designing transparent systems. This playbook would be useful to a
number of audiences including technologists, humans-in-the-loop,
and policymakers. Second, a repository of examples and use cases
of regulatory-compliant systems derived from this approach could
be created, to act as a reference to technologists.

REFERENCES
[1] Ashraf Abdul, Christian von der Weth, Mohan Kankanhalli, and Brian Y Lim.
Cogam: Measuring and moderating cognitive load in machine learning model
explanations. In Proceedings of the 2020 CHI Conference on Human Factors in
Computing Systems, pages 1–14, 2020.

[2] Evgeni Aizenberg and Jeroen Van Den Hoven. Designing for human rights in ai.
Big Data & Society, 7(2):2053951720949566, 2020.
[3] Hiva Allahyari and Niklas Lavesson. User-oriented assessment of classification
model understandability. In Anders Kofod-Petersen, Fredrik Heintz, and Helge
Langseth, editors, Eleventh Scandinavian Conference on Artificial Intelligence,
SCAI 2011, Trondheim, Norway, May 24th - 26th, 2011, volume 227 of Frontiers in
Artificial Intelligence and Applications, pages 11–19. IOS Press, 2011.
[4] Kasun Amarasinghe, Kit T. Rodolfa, Hemank Lamba, and Rayid Ghani. Explainable machine learning for public policy: Use cases, gaps, and research directions.
CoRR, abs/2010.14374, 2020.
[5] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias.
propublica. See https://www. propublica. org/article/machine-bias-risk-assessmentsin-criminal-sentencing, 2016.
[6] Vijay Arya, Rachel K. E. Bellamy, Pin-Yu Chen, Amit Dhurandhar, Michael Hind,
Samuel C. Hoffman, Stephanie Houde, Q. Vera Liao, Ronny Luss, Aleksandra Mojsilovic, Sami Mourad, Pablo Pedemonte, Ramya Raghavendra, John T. Richards,
Prasanna Sattigeri, Karthikeyan Shanmugam, Moninder Singh, Kush R. Varshney,
Dennis Wei, and Yunfeng Zhang. AI explainability 360: An extensible toolkit for
understanding data and machine learning models. J. Mach. Learn. Res., 21:130:1–
130:6, 2020.
[7] Solon Barocas, Andrew D Selbst, and Manish Raghavan. The hidden assumptions
behind counterfactual explanations and principal reasons. In Proceedings of the
2020 Conference on Fairness, Accountability, and Transparency, pages 80–89, 2020.
[8] Nadia El Bekri, Jasmin Kling, and Marco F. Huber. A study on trust in black box
models and post-hoc explanations. In Francisco Martínez-Álvarez, Alicia Troncoso Lora, José António Sáez Muñoz, Héctor Quintián, and Emilio Corchado,
editors, 14th International Conference on Soft Computing Models in Industrial
and Environmental Applications (SOCO 2019) - Seville, Spain, May 13-15, 2019,
Proceedings, volume 950 of Advances in Intelligent Systems and Computing, pages
35–46. Springer, 2019.
[9] Andrew Bell, Alexander Rich, Melisande Teng, Tin Orešković, Nuno B Bras,
Lénia Mestrinho, Srdan Golubovic, Ivan Pristas, and Leid Zejnilovic. Proactive
advising: a machine learning driven approach to vaccine hesitancy. In 2019 IEEE
International Conference on Healthcare Informatics (ICHI), pages 1–6. IEEE, 2019.
[10] Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh, Ruchir Puri, José M. F. Moura, and Peter Eckersley.
Explainable machine learning in deployment, 2020.
[11] Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy
disparities in commercial gender classification. In Sorelle A. Friedler and Christo
Wilson, editors, Conference on Fairness, Accountability and Transparency, FAT
2018, 23-24 February 2018, New York, NY, USA, volume 81 of Proceedings of Machine
Learning Research, pages 77–91. PMLR, 2018.
[12] Dorte Caswell, Greg Marston, and Jørgen Elm Larsen. Unemployed citizen or
‘at risk’client? classification systems and employment services in denmark and
australia. Critical Social Policy, 30(3):384–404, 2010.
[13] Florian Cech. Tackling algorithmic transparency in communal energy accounting
through participatory design. In C&T’21: Proceedings of the 10th International
Conference on Communities & Technologies-Wicked Problems in the Age of Tech,
pages 258–268, 2021.
[14] Ian Covert, Scott M. Lundberg, and Su-In Lee. Dblp:journals/corr/abs-200400668 feature contributions through additive importance measures. CoRR,
abs/2004.00668, 2020.
[15] Jessica Dai, Sina Fazelpour, and Zachary Lipton. Fair machine learning under
partial compliance. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics,
and Society, pages 55–65, 2021.

Think About the Stakeholders First! Towards an Algorithmic Transparency Playbook for Regulatory Compliance

[16] Anupam Datta, Shayak Sen, and Yair Zick. Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems. In 2016
IEEE symposium on security and privacy (SP), pages 598–617. IEEE, 2016.
[17] Inigo Martinez de Troya, Ruqian Chen, Laura O Moraes, Pranjal Bajaj, Jordan
Kupersmith, Rayid Ghani, Nuno B Brás, and Leid Zejnilovic. Predicting, explaining, and understanding risk of long-term unemployment. In NeurIPS Workshop
on AI for Social Good, 2018.
[18] Finale Doshi-Velez and Been Kim. Towards a rigorous science of interpretable
machine learning. arXiv preprint arXiv:1702.08608, 2017.
[19] Finale Doshi-Velez, Mason Kortz, Ryan Budish, Chris Bavitz, Sam Gershman,
David O’Brien, Kate Scott, Stuart Schieber, James Waldo, David Weinberger,
et al. Accountability of ai under the law: The role of explanation. arXiv preprint
arXiv:1711.01134, 2017.
[20] Lilian Edwards and Michael Veale. Enslaving the algorithm: From a “right to an
explanation” to a “right to better decisions”? IEEE Security & Privacy, 16(3):46–54,
2018.
[21] Malin Eiband, Hanna Schneider, Mark Bilandzic, Julian Fazekas-Con, Mareike
Haug, and Heinrich Hussmann. Bringing transparency design into practice. In
23rd international conference on intelligent user interfaces, pages 211–223, 2018.
[22] Urs Gasser and Virgílio A. F. Almeida. A layered model for AI governance. IEEE
Internet Comput., 21(6):58–62, 2017.
[23] Indermit S Gill. Policy approaches to artificial intelligence based technologies in
china, european union and the united states. 2020.
[24] Philip Gillingham. Can predictive algorithms assist decision-making in social
work with children and families? Child abuse review, 28(2):114–126, 2019.
[25] David Gunning, Mark Stefik, Jaesik Choi, Timothy Miller, Simone Stumpf, and
Guang-Zhong Yang. Xai—explainable artificial intelligence. Science Robotics,
4(37), 2019.
[26] Abhishek Gupta and Tania De Gasperis. Participatory design to build better
contact-and proximity-tracing apps. arXiv preprint arXiv:2006.00432, 2020.
[27] Michael Hind. Explaining explainable ai. XRDS: Crossroads, The ACM Magazine
for Students, 25(3):16–19, 2019.
[28] Fred Hohman, Andrew Head, Rich Caruana, Robert DeLine, and Steven Mark
Drucker. Gamut: A design probe to understand how data scientists understand
machine learning models. In Stephen A. Brewster, Geraldine Fitzpatrick, Anna L.
Cox, and Vassilis Kostakos, editors, Proceedings of the 2019 CHI Conference on
Human Factors in Computing Systems, CHI 2019, Glasgow, Scotland, UK, May 04-09,
2019, page 579. ACM, 2019.
[29] Andreas Holzinger, André Carrington, and Heimo Müller. Measuring the quality
of explanations: the system causability scale (scs). KI-Künstliche Intelligenz, pages
1–6, 2020.
[30] Johan Huysmans, Bart Baesens, and Jan Vanthienen. Using rule extraction to
improve the comprehensibility of predictive models. 2006.
[31] Sheikh Rabiul Islam, William Eberle, Sheikh Khaled Ghafoor, and Mohiuddin Ahmed. Explainable artificial intelligence approaches: A survey. CoRR,
abs/2101.09429, 2021.
[32] Anna Jobin, Marcello Ienca, and Effy Vayena. Artificial intelligence: the global
landscape of ethics guidelines. CoRR, abs/1906.11668, 2019.
[33] Jongbin Jung, Connor Concannon, Ravi Shroff, Sharad Goel, and Daniel G Goldstein. Simple rules for complex decisions. arXiv preprint arXiv:1702.04690, 2017.
[34] P. M. Krafft, Meg Young, Michael A. Katell, Karen Huang, and Ghislain Bugingo.
Defining AI in policy versus practice. In Annette N. Markham, Julia Powles, Toby
Walsh, and Anne L. Washington, editors, AIES ’20: AAAI/ACM Conference on AI,
Ethics, and Society, New York, NY, USA, February 7-8, 2020, pages 72–78. ACM,
2020.
[35] Maciej Kuziemski and Gianluca Misuraca. Ai governance in the public sector: Three tales from the frontiers of automated decision-making in democratic
settings. Telecommunications Policy, 44(6):101976, 2020. Artificial intelligence,
economy and society.
[36] Q. Vera Liao, Daniel M. Gruen, and Sarah Miller. Questioning the AI: informing design practices for explainable AI user experiences. In Regina Bernhaupt,
Florian ’Floyd’ Mueller, David Verweij, Josh Andres, Joanna McGrenere, Andy
Cockburn, Ignacio Avellino, Alix Goguey, Pernille Bjøn, Shengdong Zhao, Briane Paul Samson, and Rafal Kocielnik, editors, CHI ’20: CHI Conference on Human
Factors in Computing Systems, Honolulu, HI, USA, April 25-30, 2020, pages 1–15.
ACM, 2020.
[37] Zachary C Lipton. The mythos of model interpretability: In machine learning,
the concept of interpretability is both important and slippery. Queue, 16(3):31–57,
2018.
[38] Michele Loi and Matthias Spielkamp. Towards accountability in the use of artificial intelligence for public administrations. In Proceedings of the 2021 AAAI/ACM
Conference on AI, Ethics, and Society, pages 757–766, 2021.
[39] Artan Loxha and Matteo Morgandi. Profiling the unemployed: a review of oecd
experiences and implications for emerging economies. 2014.
[40] Joy Lu, Dokyun Lee, Tae Wan Kim, and David Danks. Good explanation for
algorithmic transparency. Available at SSRN 3503603, 2019.
[41] Scott M. Lundberg and Su-In Lee. A unified approach to interpreting model
predictions. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M.

Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA,
pages 4765–4774, 2017.
[42] Gianclaudio Malgieri. Automated decision-making in the eu member states: The
right to explanation and other “suitable safeguards” in the national legislations.
Computer law & security review, 35(5):105327, 2019.
[43] Ricards Marcinkevics and Julia E. Vogt. Interpretability and explainability: A
machine learning zoo mini-tour. CoRR, abs/2012.01805, 2020.
[44] Simon Matty. Predicting Likelihood of Long-term Unemployment: The Development
of a UK Jobseekers’ Classification Instrument. Corporate Document Services, 2013.
[45] Christian Meske, Enrico Bunde, Johannes Schneider, and Martin Gersch. Explainable artificial intelligence: Objectives, stakeholders and future research
opportunities. Information Systems Management, 12 2020.
[46] Marcia K Meyers, Susan Vorsanger, B Guy Peters, and Jon Pierre. Street-level
bureaucrats and the implementation of public policy. The handbook of public
administration, pages 153–163, 2007.
[47] Tim Miller. Explanation in artificial intelligence: Insights from the social sciences.
CoRR, abs/1706.07269, 2017.
[48] Christoph Molnar. Interpretable Machine Learning. 2019. https://christophm.
github.io/interpretable-ml-book/.
[49] Austin Nichols, Josh Mitchell, and Stephan Lindner. Consequences of long-term
unemployment. Washington, DC: The Urban Institute, 2013.
[50] Anshul Vikram Pandey, Anjali Manivannan, Oded Nov, Margaret Satterthwaite,
and Enrico Bertini. The persuasive power of data visualization. IEEE Trans. Vis.
Comput. Graph., 20(12):2211–2220, 2014.
[51] Anshul Vikram Pandey, Katharina Rall, Margaret L Satterthwaite, Oded Nov,
and Enrico Bertini. How deceptive are deceptive visualizations? an empirical
analysis of common distortion techniques. In Proceedings of the 33rd annual acm
conference on human factors in computing systems, pages 1469–1478, 2015.
[52] Jack Parker and David Danks. How technological advances can reveal rights. In
Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES ’19,
page 201, New York, NY, USA, 2019. Association for Computing Machinery.
[53] Andi Peng, Besmira Nushi, Emre Kiciman, Kori Inkpen, Siddharth Suri, and Ece
Kamar. What you see is what you get? the impact of representation criteria on
human bias in hiring. CoRR, abs/1909.03567, 2019.
[54] Alun Preece, Dan Harborne, Dave Braines, Richard Tomsett, and Supriyo
Chakraborty. Stakeholders in explainable ai. arXiv preprint arXiv:1810.00184,
2018.
[55] Bogdana Rakova, Jingying Yang, Henriette Cramer, and Rumman Chowdhury.
Where responsible ai meets reality: Practitioner perspectives on enablers for
shifting organizational practices. arXiv preprint arXiv:2006.12358, 2020.
[56] Jennifer Raso. Displacement as regulation: New regulatory technologies and
front-line decision-making in ontario works. Canadian Journal of Law and Society,
32(1):75–95, 2017.
[57] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. " why should i trust
you?" explaining the predictions of any classifier. In Proceedings of the 22nd ACM
SIGKDD international conference on knowledge discovery and data mining, pages
1135–1144, 2016.
[58] John T Richards, David Piorkowski, Michael Hind, Stephanie Houde, Aleksandra
Mojsilovic, and Kush R Varshney. A human-centered methodology for creating
ai factsheets. IEEE Data Eng. Bull., 44(4):47–58, 2021.
[59] T Riipinen. Risk profiling of long-term unemployment in finland. In Power Point
presentation at the European Commission’s “PES to PES Dialogue Dissemination
Conference,” Brussels, September, pages 8–9, 2011.
[60] Kit T. Rodolfa, Hemank Lamba, and Rayid Ghani. Machine learning for public
policy: Do we need to sacrifice accuracy to make models fair?, 2020.
[61] Cynthia Rudin. Stop explaining black box machine learning models for high
stakes decisions and use interpretable models instead. Nature Machine Intelligence,
1(5):206–215, 2019.
[62] Philipp Schmidt, Felix Biessmann, and Timm Teubner. Transparency and trust in
artificial intelligence systems. Journal of Decision Systems, 29(4):260–278, 2020.
[63] Anette Scoppetta and Arthur Buckenleib. Tackling long-term unemployment
through risk profiling and outreach. 2018.
[64] Andrew Selbst and Julia Powles. "meaningful information" and the right to
explanation. In Sorelle A. Friedler and Christo Wilson, editors, Conference on
Fairness, Accountability and Transparency, FAT 2018, 23-24 February 2018, New
York, NY, USA, volume 81 of Proceedings of Machine Learning Research, page 48.
PMLR, 2018.
[65] Dylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju.
Fooling LIME and SHAP: adversarial attacks on post hoc explanation methods.
In Annette N. Markham, Julia Powles, Toby Walsh, and Anne L. Washington,
editors, AIES ’20: AAAI/ACM Conference on AI, Ethics, and Society, New York, NY,
USA, February 7-8, 2020, pages 180–186. ACM, 2020.
[66] Kacper Sokol and Peter A. Flach. One explanation does not fit all: The
promise of interactive explanations for machine learning transparency. CoRR,
abs/2001.09734, 2020.

Andrew Bell, Oded Nov, and Julia Stoyanovich

[67] Gregor Stiglic, Petra Povalej Brzan, Nino Fijacko, Fei Wang, Boris Delibasic,
Alexandros Kalousis, and Zoran Obradovic. Comprehensible predictive modeling
using regularized logistic regression and comorbidity based features. PloS one,
10(12):e0144439, 2015.
[68] Julia Stoyanovich and Ellen P Goodman. Revealing algorithmic rankers. Freedom
to Tinker (August 5 2016), 2016.
[69] Julia Stoyanovich, Bill Howe, and H.V. Jagadish. Responsible data management.
PVLDB, 13(12):3474–3489, 2020.
[70] Karolina Sztandar-Sztanderska and Marianna Zielenska. Changing social citizenship through information technology. Social Work & Society, 16(2), 2018.
[71] Aner Tal and Brian Wansink. Blinded with science: Trivial graphs and formulas
increase ad persuasiveness and belief in product efficacy. Public Understanding
of Science, 25(1):117–125, 2016.
[72] UNICRI. Towards responsible artificial intelligence innovation. 2020.
[73] Berk Ustun, Alexander Spangher, and Yang Liu. Actionable recourse in linear
classification. In Proceedings of the conference on fairness, accountability, and
transparency, pages 10–19, 2019.
[74] Elio Ventocilla, Tove Helldin, Maria Riveiro, Juhee Bae, Veselka Boeva, Göran
Falkman, and Niklas Lavesson. Towards a taxonomy for interpretable and interactive machine learning. In XAI Workshop on Explainable Artificial Intelligence,

pages 151–157, 2018.
[75] Sandra Wachter, Brent Mittelstadt, and Luciano Floridi. Transparent, explainable,
and accountable ai for robotics. 2017.
[76] Sandra Wachter, Brent Mittelstadt, and Chris Russell. Counterfactual explanations
without opening the black box: Automated decisions and the gdpr. Harv. JL &
Tech., 31:841, 2017.
[77] Ben Wagner. Liable, but not in control? ensuring meaningful human agency in
automated decision-making systems. Policy & Internet, 11(1):104–122, 2019.
[78] Ke Yang, Biao Huang, Julia Stoyanovich, and Sebastian Schelter. Fairness-aware
instrumentation of preprocessing pipelines for machine learning. In HILDA
workshop at SIGMOD, 2020.
[79] Yiwei Yang, Eser Kandogan, Yunyao Li, Prithviraj Sen, and Walter S Lasecki. A
study on interaction in human-in-the-loop machine learning for text analytics.
In IUI Workshops, 2019.
[80] Leid Zejnilović, Susana Lavado, Íñigo Martínez de Rituerto de Troya, Samantha
Sim, and Andrew Bell. Algorithmic long-term unemployment risk assessment in
use: Counselors’ perceptions and use practices. Global Perspectives, 1(1), 2020.
[81] Yujia Zhang, Kuangyan Song, Yiming Sun, Sarah Tan, and Madeleine Udell.
" why should you trust my explanation?" understanding uncertainty in lime
explanations. arXiv preprint arXiv:1904.12991, 2019.

